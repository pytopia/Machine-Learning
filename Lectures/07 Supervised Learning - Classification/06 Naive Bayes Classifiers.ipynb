{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a popular machine learning algorithm used for classification tasks. It's based on Bayes' theorem and makes a strong (naive) assumption about the independence of features. Despite its simplicity, Naive Bayes often performs surprisingly well in many real-world scenarios, particularly in text classification and spam filtering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic classifier that makes predictions based on the probability of an object belonging to a particular class. The algorithm gets its name from two key aspects:\n",
    "\n",
    "1. **Naive**: It assumes that features are independent of each other, which is often not true in real-world scenarios. This \"naive\" assumption simplifies the computation and is what makes the algorithm efficient.\n",
    "\n",
    "2. **Bayes**: It's based on Bayes' theorem, a fundamental theorem in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bayes.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nb-dist.avif\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifies by calculating the probability of an instance belonging to each class and selecting the class with the highest probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand Naive Bayes, let's consider a simple example:\n",
    "\n",
    "Imagine you're trying to classify fruits as either apples or oranges based on their color and shape. You have observed the following:\n",
    "\n",
    "- 70% of the fruits in your dataset are apples, 30% are oranges.\n",
    "- 80% of apples are red, 20% are green.\n",
    "- 70% of oranges are orange-colored, 30% are green.\n",
    "- 90% of apples are round, 10% are slightly elongated.\n",
    "- 80% of oranges are round, 20% are slightly elongated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you encounter a new fruit that is red and round, Naive Bayes would calculate:\n",
    "\n",
    "1. The probability of it being an apple given it's red and round.\n",
    "2. The probability of it being an orange given it's red and round.\n",
    "\n",
    "It would then compare these probabilities and classify the fruit as the class with the higher probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes has several advantages that make it a popular choice in machine learning:\n",
    "\n",
    "1. **Simplicity**: It's easy to implement and understand.\n",
    "2. **Efficiency**: It's computationally fast and requires less training data than many other algorithms.\n",
    "3. **Performance**: Despite its simplicity, it often performs well, especially in text classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is particularly effective when the dimensionality of the input is high, making it a go-to algorithm for text classification and spam filtering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, Naive Bayes applies Bayes' theorem to make predictions. The theorem is expressed as:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "Where:\n",
    "- $P(A|B)$ is the probability of A given B is true.\n",
    "- $P(B|A)$ is the probability of B given A is true.\n",
    "- $P(A)$ and $P(B)$ are the probabilities of A and B independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of classification, we're interested in finding the probability of a class given certain features. This can be written as:\n",
    "\n",
    "$P(Class|Features) = \\frac{P(Features|Class) \\cdot P(Class)}{P(Features)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** The \"naive\" assumption comes into play when calculating $P(Features|Class)$. We assume that features are independent, allowing us to multiply their individual probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we'll dive deeper into Bayes' theorem, explore different types of Naive Bayes classifiers, and learn how to implement this algorithm in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Bayes' Theorem and Its Application in Classification](#toc1_)    \n",
    "  - [Understanding Bayes' Theorem](#toc1_1_)    \n",
    "  - [Applying Bayes' Theorem to Classification](#toc1_2_)    \n",
    "  - [The Classification Rule](#toc1_3_)    \n",
    "  - [An Illustrative Example](#toc1_4_)    \n",
    "  - [The Naive Assumption](#toc1_5_)    \n",
    "- [Types of Naive Bayes Classifiers](#toc2_)    \n",
    "  - [Gaussian Naive Bayes](#toc2_1_)    \n",
    "  - [Multinomial Naive Bayes](#toc2_2_)    \n",
    "  - [Bernoulli Naive Bayes](#toc2_3_)    \n",
    "  - [Complement Naive Bayes](#toc2_4_)    \n",
    "  - [Choosing the Right Naive Bayes Variant](#toc2_5_)    \n",
    "- [The 'Naive' Assumption and Its Implications](#toc3_)    \n",
    "  - [Understanding the Naive Assumption](#toc3_1_)    \n",
    "  - [Implications of the Naive Assumption](#toc3_2_)    \n",
    "  - [When the Naive Assumption Fails](#toc3_3_)    \n",
    "  - [Addressing the Limitations](#toc3_4_)    \n",
    "  - [Interpreting Naive Bayes Results](#toc3_5_)    \n",
    "- [Implementing Naive Bayes](#toc4_)    \n",
    "  - [Implementation from Scratch](#toc4_1_)    \n",
    "  - [Implementation using scikit-learn](#toc4_2_)    \n",
    "  - [Handling Text Data](#toc4_3_)    \n",
    "- [Advantages and Disadvantages of Naive Bayes](#toc5_)    \n",
    "  - [Advantages of Naive Bayes](#toc5_1_)    \n",
    "  - [Disadvantages of Naive Bayes](#toc5_2_)    \n",
    "- [Real-world Applications of Naive Bayes](#toc6_)    \n",
    "  - [Text Classification and Spam Filtering](#toc6_1_)    \n",
    "  - [Sentiment Analysis](#toc6_2_)    \n",
    "  - [Medical Diagnosis](#toc6_3_)    \n",
    "  - [Recommendation Systems](#toc6_4_)    \n",
    "  - [Weather Prediction](#toc6_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Bayes' Theorem and Its Application in Classification](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Theorem is the cornerstone of Naive Bayes classification. It provides a way to calculate the probability of a hypothesis given observed evidence, which is exactly what we need for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Understanding Bayes' Theorem](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Theorem, named after Reverend Thomas Bayes, is a fundamental principle in probability theory. It describes the probability of an event based on prior knowledge of conditions that might be related to the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bayes-2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theorem is expressed mathematically as:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "Where:\n",
    "- $P(A|B)$ is the posterior probability: the probability of event A occurring given that B is true.\n",
    "- $P(B|A)$ is the likelihood: the probability of B occurring given that A is true.\n",
    "- $P(A)$ is the prior probability: the probability of A occurring regardless of any other information.\n",
    "- $P(B)$ is the marginal likelihood: the probability of B occurring regardless of any other information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Bayes' Theorem allows us to update our beliefs about the probability of an event as we gather more evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Applying Bayes' Theorem to Classification](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of classification, we can rewrite Bayes' Theorem as:\n",
    "\n",
    "$P(Class|Features) = \\frac{P(Features|Class) \\cdot P(Class)}{P(Features)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/naive-bayes-classifier.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what each term means in a classification context:\n",
    "\n",
    "1. $P(Class|Features)$: The probability of a class given the observed features. This is what we're trying to calculate to make a classification.\n",
    "\n",
    "2. $P(Features|Class)$: The probability of observing these features given the class. This is calculated from the training data.\n",
    "\n",
    "3. $P(Class)$: The prior probability of the class, regardless of the features. This is also calculated from the training data.\n",
    "\n",
    "4. $P(Features)$: The probability of observing these features across all classes. This acts as a normalizing constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[The Classification Rule](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify an instance, we calculate $P(Class|Features)$ for each possible class and choose the class with the highest probability. This is known as the Maximum A Posteriori (MAP) decision rule:\n",
    "\n",
    "$Class_{predicted} = \\arg\\max_{Class} P(Class|Features) = \\arg\\max_{Class} P(Features|Class) \\cdot P(Class)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we often don't need to calculate $P(Features)$ because it's constant for all classes. We can simply compare $P(Features|Class) \\cdot P(Class)$ across classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[An Illustrative Example](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit our fruit classification example:\n",
    "\n",
    "Suppose we want to classify a fruit that is red and round. We have two classes: Apple and Orange.\n",
    "\n",
    "Given:\n",
    "- $P(Apple) = 0.7$, $P(Orange) = 0.3$\n",
    "- $P(Red|Apple) = 0.8$, $P(Red|Orange) = 0.2$\n",
    "- $P(Round|Apple) = 0.9$, $P(Round|Orange) = 0.8$\n",
    "\n",
    "We can calculate:\n",
    "\n",
    "- $P(Apple|Red,Round) \\propto P(Red|Apple) \\cdot P(Round|Apple) \\cdot P(Apple) = 0.8 \\cdot 0.9 \\cdot 0.7 = 0.504$\n",
    "\n",
    "- $P(Orange|Red,Round) \\propto P(Red|Orange) \\cdot P(Round|Orange) \\cdot P(Orange) = 0.2 \\cdot 0.8 \\cdot 0.3 = 0.048$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 0.504 > 0.048, we would classify this fruit as an Apple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_'></a>[The Naive Assumption](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"naive\" in Naive Bayes comes from the assumption that features are conditionally independent given the class. This means:\n",
    "\n",
    "$P(Features|Class) = P(Feature_1|Class) \\cdot P(Feature_2|Class) \\cdot ... \\cdot P(Feature_n|Class)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption greatly simplifies the computation, but it's often violated in real-world scenarios. Despite this, Naive Bayes often performs well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** The naive assumption allows us to easily compute probabilities even with many features, making Naive Bayes computationally efficient and effective for high-dimensional problems like text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we'll explore different types of Naive Bayes classifiers and see how to implement this algorithm in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Types of Naive Bayes Classifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers come in several variants, each designed to handle different types of data and distributions. The main difference between these variants lies in the assumptions they make about the distribution of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nb-types.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Gaussian Naive Bayes](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes assumes that the continuous values associated with each class are distributed according to a Gaussian (normal) distribution. Here are the key characteristics:\n",
    "- Suitable for continuous data\n",
    "- Assumes features follow a normal distribution for each class\n",
    "- Calculates mean and standard deviation of features for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a feature given a class is calculated as:\n",
    "\n",
    "$P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)$\n",
    "\n",
    "Where $\\mu_y$ is the mean and $\\sigma_y^2$ is the variance of feature $i$ for class $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Gaussian Naive Bayes is often used when dealing with continuous data, such as in many scientific and engineering applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Multinomial Naive Bayes](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes is typically used for discrete data, and it's particularly popular for text classification tasks. Here are the key characteristics:\n",
    "- Suitable for discrete data (e.g., word counts for text classification)\n",
    "- Assumes features follow a multinomial distribution\n",
    "- Often used with term frequency features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a feature given a class is calculated as:\n",
    "\n",
    "$P(x_i | y) = \\frac{N_{yi} + \\alpha}{N_y + \\alpha n}$\n",
    "\n",
    "Where $N_{yi}$ is the count of feature $i$ in class $y$, $N_y$ is the total count of all features in class $y$, $n$ is the number of features, and $\\alpha$ is a smoothing parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** Multinomial Naive Bayes often works well for text classification, even with a small amount of training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Bernoulli Naive Bayes](#toc0_)\n",
    "\n",
    "Bernoulli Naive Bayes is used for binary/boolean features. It's similar to Multinomial Naive Bayes but penalizes the non-occurrence of a feature that's indicative of a class. Here are the key characteristics:\n",
    "- Suitable for binary/boolean features\n",
    "- Assumes features are binary-valued (e.g., word presence/absence)\n",
    "- Penalizes non-occurrence of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a feature given a class is calculated as:\n",
    "\n",
    "$P(x_i | y) = P(i | y)x_i + (1 - P(i | y))(1 - x_i)$\n",
    "\n",
    "Where $P(i | y)$ is the probability of feature $i$ appearing in class $y$, and $x_i$ is either 1 or 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Bernoulli Naive Bayes considers both the presence and absence of features, making it particularly effective for short texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Choosing the Right Naive Bayes Variant](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of Naive Bayes variant depends on the nature of your data:\n",
    "\n",
    "1. For continuous data, use Gaussian Naive Bayes.\n",
    "2. For discrete data, especially in text classification, use Multinomial Naive Bayes.\n",
    "3. For binary features or short text classification, consider Bernoulli Naive Bayes.\n",
    "4. For imbalanced text datasets, try Complement Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Understanding the different types of Naive Bayes classifiers allows you to choose the most appropriate variant for your specific problem, potentially improving your model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we'll explore the implications of the naive assumption and learn how to implement Naive Bayes classifiers in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[The 'Naive' Assumption and Its Implications](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'naive' assumption is a fundamental aspect of Naive Bayes classifiers that greatly simplifies the model but also has important implications for its performance and interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Understanding the Naive Assumption](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive assumption, also known as the conditional independence assumption, states that all features are independent of each other given the class label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, this can be expressed as:\n",
    "\n",
    "$P(X_1, X_2, ..., X_n | Y) = P(X_1 | Y) \\cdot P(X_2 | Y) \\cdot ... \\cdot P(X_n | Y)$\n",
    "\n",
    "Where $X_1, X_2, ..., X_n$ are features and $Y$ is the class label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** This assumption allows us to simplify the computation of the joint probability distribution over features, making Naive Bayes computationally efficient and scalable to high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Implications of the Naive Assumption](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive assumption dramatically **reduces the computational complexity of the model**. Instead of having to learn the parameters of a full joint probability distribution, we only need to learn the parameters for each feature independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** This efficiency makes Naive Bayes particularly useful for high-dimensional problems like text classification, where the number of features (words) can be very large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its simplicity, Naive Bayes often **performs surprisingly well in practice**, even when the independence assumption is violated. This is partly because:\n",
    "\n",
    "1. The classification doesn't require precise probability estimates, only that the correct class has the highest probability.\n",
    "2. The errors in the probability estimates often cancel out when we multiply many small probabilities together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive assumption **introduces bias into the model**, as it simplifies the true relationship between features. However, this bias often leads to lower variance, which can be beneficial when dealing with limited training data.\n",
    "\n",
    "$Bias^2 + Variance + Irreducible\\,Error = Expected\\,Prediction\\,Error$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** The bias-variance tradeoff is crucial in machine learning. Naive Bayes often achieves a good balance, making it resistant to overfitting, especially with small datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[When the Naive Assumption Fails](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Naive Bayes is surprisingly robust, there are situations where the naive assumption can lead to poor performance:\n",
    "\n",
    "- When features are strongly correlated, Naive Bayes can overemphasize their importance, leading to skewed probability estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in a text classification task, the words \"machine\" and \"learning\" might frequently appear together. Naive Bayes would treat these as independent pieces of evidence, potentially overestimating their combined importance.\n",
    "\n",
    "- If a categorical feature has a category in the test data that was not observed in the training data, the model will assign a zero probability to this class. This is known as the \"zero frequency\" problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** This issue is typically addressed through smoothing techniques, such as Laplace smoothing, which adds a small count to all feature/class combinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Addressing the Limitations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several techniques can help mitigate the limitations of the naive assumption:\n",
    "\n",
    "1. **Feature Selection**: Removing redundant or highly correlated features can help reduce the impact of the independence assumption.\n",
    "\n",
    "2. **Smoothing**: Techniques like Laplace smoothing help address the zero frequency problem.\n",
    "\n",
    "3. **Ensemble Methods**: Combining Naive Bayes with other models can help compensate for its limitations.\n",
    "\n",
    "4. **Kernel Density Estimation**: For continuous features, using kernel density estimation instead of assuming a specific distribution (like Gaussian) can capture more complex relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_'></a>[Interpreting Naive Bayes Results](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting Naive Bayes results, it's important to remember that:\n",
    "\n",
    "1. The raw probability estimates may not be well-calibrated due to the independence assumption.\n",
    "2. The relative rankings of probabilities are often more reliable than their absolute values.\n",
    "3. Feature importance can be assessed by examining the conditional probabilities, but be cautious of overinterpreting when features are correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, while the naive assumption is a simplification of reality, it often leads to a good balance between model complexity and performance. Understanding its implications is crucial for effectively applying and interpreting Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Implementing Naive Bayes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll explore how to implement Naive Bayes classifiers both from scratch and using the scikit-learn library. This dual approach will give you a deeper understanding of the algorithm's mechanics while also introducing you to practical tools for real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Implementation from Scratch](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a simple Gaussian Naive Bayes classifier from scratch. This will help us understand the inner workings of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.parameters[c] = {\n",
    "                'mean': X_c.mean(axis=0),\n",
    "                'var': X_c.var(axis=0),\n",
    "                'prior': X_c.shape[0] / X.shape[0]\n",
    "            }\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x) for x in X])\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        for c in self.classes:\n",
    "            prior = np.log(self.parameters[c]['prior'])\n",
    "            likelihood = np.sum(np.log(norm.pdf(x, self.parameters[c]['mean'], np.sqrt(self.parameters[c]['var']))))\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation on a simple dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Generate a random dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and test our model\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)\n",
    "predictions = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** This implementation assumes Gaussian distribution for features. For other types of data, you'd need to modify the likelihood calculation accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Implementation using scikit-learn](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how to implement Naive Bayes using scikit-learn, which provides optimized implementations of various Naive Bayes classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a random dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Scikit-learn provides implementations for different types of Naive Bayes classifiers. Use `GaussianNB` for continuous data, `MultinomialNB` for discrete data (like text classification), and `BernoulliNB` for binary features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Handling Text Data](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is particularly popular for text classification. Here's how you can use it for a simple text classification task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love this movie\", \"This movie is awful\", \"Great acting\", \"Terrible plot\", \"I enjoyed it\"]\n",
    "labels = [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Create a pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "text_clf.fit(texts, labels)\n",
    "\n",
    "# Make a prediction\n",
    "new_texts = [\"This film is amazing\", \"I hated every minute of it\"]\n",
    "predictions = text_clf.predict(new_texts)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** For text data, we typically use `MultinomialNB` instead of `GaussianNB`, as it's better suited for discrete counts (like word frequencies).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By implementing Naive Bayes both from scratch and using scikit-learn, we gain a deeper understanding of the algorithm while also learning practical tools for real-world applications. The scikit-learn implementation is optimized and provides additional features, making it the preferred choice for most practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Advantages and Disadvantages of Naive Bayes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the strengths and limitations of Naive Bayes classifiers is crucial for effectively applying them to real-world problems. In this section, we'll explore the key advantages and disadvantages of this popular algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Advantages of Naive Bayes](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplicity and Efficiency**\n",
    "\n",
    "Naive Bayes is remarkably simple to implement and computationally efficient. Its training and prediction processes are typically much faster than more complex algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.1630 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a large dataset\n",
    "X, y = make_classification(n_samples=1000000, n_features=20, random_state=42)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {training_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** This efficiency makes Naive Bayes particularly suitable for real-time prediction tasks and large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance with Small Datasets**\n",
    "\n",
    "Naive Bayes can perform well even with limited training data. It doesn't require large amounts of data to estimate the necessary parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handles High-Dimensional Data**\n",
    "\n",
    "The algorithm performs well with high-dimensional data, such as text classification problems where the number of features (words) can be very large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiclass Classification**\n",
    "\n",
    "Naive Bayes naturally extends to multiclass classification problems, making it versatile for various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on multiclass problem: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset (a classic multiclass problem)\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and evaluate\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on multiclass problem: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insensitive to Irrelevant Features**\n",
    "\n",
    "Naive Bayes is relatively robust to irrelevant features. It can handle situations where some features are not informative for the classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[Disadvantages of Naive Bayes](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Independence Assumption**\n",
    "\n",
    "The \"naive\" assumption of feature independence is often violated in real-world scenarios, which can lead to suboptimal performance in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** While this assumption simplifies the model, it can sometimes lead to oversimplified predictions, especially when features are strongly correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limited Expressiveness**\n",
    "\n",
    "Naive Bayes cannot learn interactions between features. This limitation can make it less suitable for complex relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on interaction-dependent data: 0.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate data where the class depends on the interaction of two features\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(1000, 2)\n",
    "y = (X[:, 0] * X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Train and evaluate\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "y_pred = gnb.predict(X)\n",
    "\n",
    "print(f\"Accuracy on interaction-dependent data: {accuracy_score(y, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous Data Assumptions**\n",
    "\n",
    "For continuous features, Naive Bayes often assumes a specific distribution (e.g., Gaussian), which may not always hold true for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero Frequency Problem**\n",
    "\n",
    "When a categorical variable has a category in the test data that was not observed in the training data, the model will assign a zero probability and be unable to make a prediction. This is known as the \"zero frequency\" problem.\n",
    "\n",
    "‚ùóÔ∏è **Important Note:** This issue is typically addressed through smoothing techniques, but it's important to be aware of its potential impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Understanding these advantages and disadvantages helps in deciding when to use Naive Bayes and how to interpret its results. It's often a good baseline model and can be particularly effective for text classification and spam filtering tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's important to compare Naive Bayes with other models and consider ensemble methods to leverage its strengths while mitigating its weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Real-world Applications of Naive Bayes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers, despite their simplicity, have found widespread use in various real-world applications. Their efficiency, ability to handle high-dimensional data, and surprisingly good performance make them suitable for many practical scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Text Classification and Spam Filtering](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common and successful applications of Naive Bayes is in text classification, particularly spam filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample email data\n",
    "emails = [\n",
    "    \"Get rich quick! Buy now!\",\n",
    "    \"Meeting scheduled for tomorrow\",\n",
    "    \"Claim your prize money now\",\n",
    "    \"Project report due next week\",\n",
    "    \"You've won a free iPhone\",\n",
    "    \"Congratulations! You've won a free iPhone\",\n",
    "    \"You've won a free iPhone\",\n",
    "    \"You've won a huge amount of money\",\n",
    "    \"Hey, I'm looking for a new job\",\n",
    "    \"Hello, how are you?\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 1, 1, 1, 0, 0]  # 1 for spam, 0 for ham\n",
    "\n",
    "# Create a pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and evaluate\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Naive Bayes works well for text classification because it can handle the high dimensionality of text data efficiently and is less prone to overfitting on small datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_2_'></a>[Sentiment Analysis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is often used for sentiment analysis, determining whether a piece of text expresses positive, negative, or neutral sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample reviews\n",
    "reviews = [\n",
    "    \"This product is amazing!\",\n",
    "    \"Terrible service, would not recommend\",\n",
    "    \"Average experience, nothing special\",\n",
    "    \"Absolutely love it, best purchase ever\",\n",
    "    \"Disappointing quality, not worth the price\"\n",
    "]\n",
    "sentiments = [1, -1, 0, 1, -1]  # 1 for positive, -1 for negative, 0 for neutral\n",
    "\n",
    "# Create and train the model\n",
    "sentiment_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "sentiment_clf.fit(reviews, sentiments)\n",
    "\n",
    "predictions = sentiment_clf.predict(reviews)\n",
    "print(f\"Accuracy: {accuracy_score(sentiments, predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment for a new review\n",
    "new_review = [\"The product exceeded my expectations\"]\n",
    "prediction = sentiment_clf.predict(new_review)\n",
    "sentiment_map = {1: \"Positive\", 0: \"Neutral\", -1: \"Negative\"}\n",
    "print(f\"Predicted sentiment: {sentiment_map[prediction[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_3_'></a>[Medical Diagnosis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be used in medical diagnosis to predict the likelihood of a disease based on symptoms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19 prediction: Negative\n",
      "Probability of being positive: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Sample medical data (simplified)\n",
    "# Features: [fever, cough, fatigue, difficulty breathing]\n",
    "X = np.array([\n",
    "    [1, 1, 1, 1],\n",
    "    [0, 0, 1, 0],\n",
    "    [1, 0, 1, 1],\n",
    "    [1, 1, 0, 0],\n",
    "    [0, 1, 1, 0]\n",
    "])\n",
    "y = np.array([1, 0, 1, 0, 0])  # 1 for COVID-19 positive, 0 for negative\n",
    "\n",
    "# Train the model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "\n",
    "# Predict for a new patient\n",
    "new_patient = np.array([[1, 1, 1, 0]])\n",
    "prediction = gnb.predict(new_patient)\n",
    "probability = gnb.predict_proba(new_patient)\n",
    "\n",
    "print(f\"COVID-19 prediction: {'Positive' if prediction[0] == 1 else 'Negative'}\")\n",
    "print(f\"Probability of being positive: {probability[0][1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** In real medical applications, much more comprehensive data and expert knowledge would be required. This example is highly simplified for illustration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_4_'></a>[Recommendation Systems](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be used in recommendation systems, particularly for content-based filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Like\n",
      "Probability of liking: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "# Sample user preferences data\n",
    "# Features: [Action, Comedy, Romance, Sci-Fi]\n",
    "X = np.array([\n",
    "    [1, 0, 0, 1],\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1]\n",
    "])\n",
    "y = np.array([1, 0, 1, 0])  # 1 for like, 0 for dislike\n",
    "\n",
    "# Train the model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X, y)\n",
    "\n",
    "# Predict preference for a new movie\n",
    "new_movie = np.array([[1, 0, 0, 1]])  # An action sci-fi movie\n",
    "prediction = mnb.predict(new_movie)\n",
    "probability = mnb.predict_proba(new_movie)\n",
    "\n",
    "print(f\"Prediction: {'Like' if prediction[0] == 1 else 'Dislike'}\")\n",
    "print(f\"Probability of liking: {probability[0][1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_5_'></a>[Weather Prediction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be applied to simple weather prediction tasks based on observed conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: No\n",
      "Probability of playing tennis: 0.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample weather data\n",
    "X = [\n",
    "    ['Sunny', 'Hot', 'High', 'Weak'],\n",
    "    ['Sunny', 'Hot', 'High', 'Strong'],\n",
    "    ['Overcast', 'Hot', 'High', 'Weak'],\n",
    "    ['Rainy', 'Mild', 'High', 'Weak'],\n",
    "    ['Rainy', 'Cool', 'Normal', 'Weak']\n",
    "]\n",
    "y = ['No', 'No', 'Yes', 'Yes', 'Yes']  # Play tennis or not\n",
    "\n",
    "# Encode categorical features\n",
    "enc = OrdinalEncoder()\n",
    "X_encoded = enc.fit_transform(X)\n",
    "\n",
    "# Train the model\n",
    "cnb = CategoricalNB()\n",
    "cnb.fit(X_encoded, y)\n",
    "\n",
    "# Predict for new weather conditions\n",
    "new_day = enc.transform([['Sunny', 'Cool', 'High', 'Strong']])\n",
    "prediction = cnb.predict(new_day)\n",
    "probability = cnb.predict_proba(new_day)\n",
    "\n",
    "print(f\"Prediction: {prediction[0]}\")\n",
    "print(f\"Probability of playing tennis: {probability[0][1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** These real-world applications demonstrate the versatility of Naive Bayes. Its simplicity, efficiency, and effectiveness in various domains make it a valuable tool in a data scientist's toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Naive Bayes might not always be the best performing model for these tasks, it often serves as an excellent baseline and can be particularly useful in scenarios with limited computational resources or when quick results are needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
