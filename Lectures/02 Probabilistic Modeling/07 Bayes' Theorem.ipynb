{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem: A Fundamental Concept in Probability Theory and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem, named after the 18th-century mathematician Thomas Bayes, is a fundamental concept in probability theory and statistics. It provides a way to update the probability of an event occurring based on new information or evidence. Bayes' theorem is a powerful tool for reasoning under uncertainty and making informed decisions in various domains, including machine learning and data science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, Bayes' theorem describes the relationship between conditional probabilities. It allows us to calculate the probability of an event A given that event B has occurred, denoted as $P(A|B)$, using the prior probability of A, the likelihood of B given A, and the marginal probability of B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of Bayes' theorem in machine learning and statistics cannot be overstated:\n",
    "\n",
    "1. **Probabilistic modeling**: Bayes' theorem forms the foundation of probabilistic modeling, which is a key approach in machine learning. Probabilistic models allow us to quantify uncertainty, make predictions, and update our beliefs based on observed data. By incorporating prior knowledge and likelihood functions, Bayesian methods provide a principled way to learn from data and make informed decisions.\n",
    "\n",
    "2. **Classification tasks**: Bayes' theorem is at the heart of many classification algorithms, such as the Naive Bayes classifier. These algorithms use Bayes' theorem to calculate the posterior probability of a class given the observed features, enabling them to make predictions and classify new instances based on their probability distributions.\n",
    "\n",
    "3. **Bayesian inference**: Bayes' theorem is the cornerstone of Bayesian inference, a powerful framework for reasoning under uncertainty. Bayesian inference allows us to update our beliefs about model parameters or hypotheses based on observed data, providing a principled way to incorporate prior knowledge and quantify uncertainty in parameter estimates.\n",
    "\n",
    "4. **Decision-making**: Bayes' theorem plays a crucial role in decision theory and decision-making under uncertainty. By combining prior probabilities with the likelihood of observed data, Bayes' theorem helps us make optimal decisions based on expected utility or cost, taking into account the uncertainty associated with different outcomes.\n",
    "\n",
    "5. **Robustness and regularization**: Bayesian methods, which rely on Bayes' theorem, offer a natural way to incorporate prior knowledge and regularization into machine learning models. By specifying appropriate prior distributions, we can prevent overfitting, handle limited data scenarios, and obtain more robust and interpretable models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lecture, we will dive deeper into the concepts and applications of Bayes' theorem, exploring its mathematical formulation, examples, and its impact on various machine learning techniques. By understanding and applying Bayes' theorem, you will gain a powerful tool for reasoning under uncertainty and making informed decisions in your machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Bayes' Theorem Formula](#toc1_)    \n",
    "  - [Derivation of Bayes' theorem](#toc1_1_)    \n",
    "  - [Explanation of terms: prior, likelihood, posterior, and evidence](#toc1_2_)    \n",
    "- [Examples of Bayes' Theorem](#toc2_)    \n",
    "  - [Simple coin toss example](#toc2_1_)    \n",
    "  - [Medical diagnosis example](#toc2_2_)    \n",
    "- [Bayesian Inference](#toc3_)    \n",
    "  - [Introduction to Bayesian inference](#toc3_1_)    \n",
    "  - [Updating beliefs based on new evidence](#toc3_2_)    \n",
    "  - [Comparison with frequentist inference](#toc3_3_)    \n",
    "- [Summary](#toc4_)    \n",
    "  - [Recap of key points](#toc4_1_)    \n",
    "  - [Importance of understanding Bayes' theorem in machine learning](#toc4_2_)    \n",
    "  - [Additional Resources](#toc4_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Bayes' Theorem Formula](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem is a mathematical formula that describes the relationship between conditional probabilities. It allows us to calculate the probability of an event A given that event B has occurred, denoted as $P(A|B)$, using the prior probability of A, the likelihood of B given A, and the marginal probability of B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Derivation of Bayes' theorem](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive Bayes' theorem, we start with the definition of conditional probability:\n",
    "\n",
    "$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "where $P(A \\cap B)$ is the joint probability of events A and B occurring together, and $P(B)$ is the marginal probability of event B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can write the conditional probability of B given A as:\n",
    "\n",
    "$P(B|A) = \\frac{P(A \\cap B)}{P(A)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rearranging the terms, we get:\n",
    "\n",
    "$P(A \\cap B) = P(B|A) \\cdot P(A)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting this expression into the original equation for $P(A|B)$, we obtain Bayes' theorem:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fundamental equation of Bayes' theorem, which relates the conditional probability of A given B to the conditional probability of B given A, the prior probability of A, and the marginal probability of B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Explanation of terms: prior, likelihood, posterior, and evidence](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand Bayes' theorem, let's explore the meaning of each term in the equation:\n",
    "\n",
    "1. **Posterior probability** ($P(A|B)$): The posterior probability is the updated probability of event A occurring after observing evidence B. It represents our revised belief about the probability of A given the new information provided by B. The posterior probability is what we are ultimately interested in calculating using Bayes' theorem.\n",
    "\n",
    "2. **Prior probability** ($P(A)$): The prior probability represents our initial belief or knowledge about the probability of event A occurring before observing any evidence. It is based on our previous experience, domain knowledge, or assumptions.\n",
    "\n",
    "3. **Likelihood** ($P(B|A)$): The likelihood is the probability of observing evidence B given that event A has occurred. It quantifies how likely we are to observe the evidence if the event A is true. The likelihood function is a key component in updating our beliefs based on new data.\n",
    "\n",
    "4. **Evidence** or **Marginal likelihood** ($P(B)$): The evidence, also known as the marginal likelihood, is the probability of observing evidence B regardless of whether event A occurred or not. It serves as a normalizing constant in Bayes' theorem and ensures that the posterior probabilities sum up to 1. The evidence can be calculated using the law of total probability: $P(B) = P(B|A) \\cdot P(A) + P(B|not A) \\cdot P(not A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding these terms and their roles in Bayes' theorem, we can effectively apply the theorem to update our beliefs based on new evidence and make informed decisions in various machine learning and statistical problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will explore some concrete examples to illustrate the application of Bayes' theorem and solidify our understanding of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Examples of Bayes' Theorem](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how Bayes' theorem works in practice, let's explore a couple of examples that demonstrate its application in different scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Simple coin toss example](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a fair coin that has a 50% probability of landing on heads (H) and a 50% probability of landing on tails (T). Now, suppose we toss the coin three times and observe the following sequence: H, T, H. We want to calculate the probability that the next toss will result in heads, given the observed sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the events:\n",
    "- A: The next toss results in heads (H)\n",
    "- B: The observed sequence is H, T, H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Bayes' theorem to calculate $P(A|B)$:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "- $P(A)$ is the prior probability of getting heads, which is 0.5 for a fair coin.\n",
    "- $P(B|A)$ is the likelihood of observing the sequence H, T, H given that the next toss is heads. Since the tosses are independent, $P(B|A) = 0.5 \\cdot 0.5 \\cdot 0.5 = 0.125$.\n",
    "- $P(B)$ is the marginal probability of observing the sequence H, T, H, which can be calculated as: $P(B) = P(B|A) \\cdot P(A) + P(B|not A) \\cdot P(not A) = 0.125 \\cdot 0.5 + 0.125 \\cdot 0.5 = 0.125$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the values:\n",
    "\n",
    "$P(A|B) = \\frac{0.125 \\cdot 0.5}{0.125} = 0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the probability that the next toss will result in heads, given the observed sequence, is still 0.5. This makes sense because the coin is fair, and the previous tosses do not affect the probability of the next toss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Medical diagnosis example](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a medical diagnosis scenario where a patient undergoes a test for a rare disease. The test has a 95% accuracy rate, meaning that it correctly identifies 95% of patients who have the disease and 95% of patients who do not have the disease. The prevalence of the disease in the population is known to be 1 in 1000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the probability that a patient has the disease given that the test result is positive.\n",
    "\n",
    "Let's define the events:\n",
    "- A: The patient has the disease\n",
    "- B: The test result is positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Bayes' theorem to calculate $P(A|B)$:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "- $P(A)$ is the prior probability of having the disease, which is 0.001 (1 in 1000).\n",
    "- $P(B|A)$ is the likelihood of testing positive given that the patient has the disease, which is 0.95 (the test's accuracy rate).\n",
    "- $P(B)$ can be calculated using the law of total probability: $P(B) = P(B|A) \\cdot P(A) + P(B|not A) \\cdot P(not A) = 0.95 \\cdot 0.001 + 0.05 \\cdot 0.999 \\approx 0.0508$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the values:\n",
    "\n",
    "$P(A|B) = \\frac{0.95 \\cdot 0.001}{0.0508} \\approx 0.0187$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the probability that a patient has the disease given a positive test result is approximately 0.0187 or 1.87%. This result might seem counterintuitive, but it highlights the importance of considering the prior probability (prevalence) of the disease in the population. Even with a highly accurate test, the posterior probability of having the disease can be relatively low when the disease is rare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples demonstrate how Bayes' theorem allows us to update our beliefs based on new evidence and calculate the posterior probabilities in different scenarios. By understanding and applying Bayes' theorem, we can make more informed decisions and reason effectively under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Bayesian Inference](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian inference is a powerful framework for updating our beliefs about unknown quantities or hypotheses based on observed data. It combines prior knowledge with the likelihood of the data to obtain updated posterior beliefs. Bayesian inference is fundamentally based on Bayes' theorem and provides a principled way to reason under uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Introduction to Bayesian inference](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian inference, we start with an initial belief or hypothesis about an unknown quantity, represented by a prior probability distribution. The prior distribution encapsulates our knowledge or assumptions about the quantity before observing any data. It can be based on domain expertise, previous studies, or theoretical considerations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we observe data that are related to the unknown quantity. The likelihood function quantifies the probability of observing the data given different values of the unknown quantity. It describes how well each possible value of the quantity explains the observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem then allows us to combine the prior distribution with the likelihood function to obtain the posterior distribution. The posterior distribution represents our updated beliefs about the unknown quantity after taking into account the observed data. It provides a complete description of the uncertainty associated with the quantity, including point estimates, credible intervals, and probability statements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Updating beliefs based on new evidence](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key strengths of Bayesian inference is its ability to update beliefs sequentially as new evidence becomes available. Each time we observe new data, we can use Bayes' theorem to update our posterior distribution, incorporating the new information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updating process follows these steps:\n",
    "1. Start with the current prior distribution (which may be the posterior distribution from a previous update).\n",
    "2. Observe new data and calculate the likelihood of the data given different values of the unknown quantity.\n",
    "3. Use Bayes' theorem to combine the prior distribution with the likelihood, obtaining the updated posterior distribution.\n",
    "4. The updated posterior distribution becomes the new prior distribution for the next iteration, and the process repeats as more data are observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iterative updating allows Bayesian inference to adapt and refine beliefs as new evidence accumulates, making it particularly useful in scenarios where data arrive sequentially or when we need to make decisions based on evolving knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Comparison with frequentist inference](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian inference differs from the traditional frequentist approach to statistical inference in several key aspects:\n",
    "\n",
    "1. **Treatment of probability**: In Bayesian inference, probability is interpreted as a measure of belief or uncertainty about an unknown quantity. In contrast, frequentist inference interprets probability as the long-run frequency of an event in repeated experiments.\n",
    "\n",
    "2. **Incorporation of prior knowledge**: Bayesian inference explicitly incorporates prior knowledge or beliefs through the prior distribution. Frequentist inference does not typically incorporate prior information and relies solely on the observed data.\n",
    "\n",
    "3. **Interpretation of results**: Bayesian inference provides posterior probability distributions that directly quantify the uncertainty about unknown quantities. Frequentist inference focuses on point estimates, confidence intervals, and hypothesis tests based on sampling distributions.\n",
    "\n",
    "4. **Sequential updating**: Bayesian inference naturally allows for sequential updating of beliefs as new data arrive. Frequentist inference typically treats each analysis as a separate entity and does not provide a formal mechanism for updating beliefs.\n",
    "\n",
    "5. **Handling of uncertainty**: Bayesian inference provides a coherent framework for quantifying and propagating uncertainty throughout the analysis. Frequentist inference often relies on asymptotic approximations and may struggle with small sample sizes or complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both approaches have their merits and are used in different contexts, Bayesian inference offers a principled and flexible framework for reasoning under uncertainty, incorporating prior knowledge, and updating beliefs based on observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we explored Bayes' theorem, a fundamental concept in probability theory and statistics that plays a crucial role in machine learning and data science. We delved into the mathematical formulation of Bayes' theorem, its interpretation, and its applications in various domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Recap of key points](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bayes' theorem describes the relationship between conditional probabilities and provides a way to update our beliefs about an event based on new evidence.\n",
    "\n",
    "2. The key components of Bayes' theorem are:\n",
    "   - Prior probability: Our initial belief about an event before observing any evidence.\n",
    "   - Likelihood: The probability of observing the evidence given that the event occurred.\n",
    "   - Posterior probability: The updated probability of the event after taking into account the evidence.\n",
    "   - Evidence (marginal likelihood): The probability of observing the evidence regardless of the event.\n",
    "\n",
    "3. Bayes' theorem is derived from the definition of conditional probability and can be expressed as:\n",
    "\n",
    "   $P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "4. We explored examples of Bayes' theorem in action, including a simple coin toss example and a medical diagnosis example, to illustrate how it allows us to update our beliefs based on new information.\n",
    "\n",
    "5. Bayesian inference is a powerful framework built upon Bayes' theorem, enabling us to update our beliefs about unknown quantities or hypotheses based on observed data. It combines prior knowledge with the likelihood of the data to obtain posterior distributions.\n",
    "\n",
    "6. Bayesian inference differs from the frequentist approach in its treatment of probability, incorporation of prior knowledge, interpretation of results, sequential updating, and handling of uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Importance of understanding Bayes' theorem in machine learning](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Bayes' theorem is essential for anyone working in machine learning or data science. Here's why:\n",
    "\n",
    "1. **Probabilistic modeling**: Bayes' theorem forms the foundation of probabilistic modeling, which is a key approach in machine learning. It allows us to build models that quantify uncertainty, make predictions, and update our beliefs based on observed data.\n",
    "\n",
    "2. **Machine learning algorithms**: Many machine learning algorithms, such as Naive Bayes classifiers and Bayesian networks, are based on Bayes' theorem. Understanding the underlying principles of Bayes' theorem enables us to effectively apply and interpret these algorithms.\n",
    "\n",
    "3. **Handling uncertainty**: Bayes' theorem provides a principled way to reason under uncertainty and make decisions based on incomplete or noisy data. It allows us to quantify and propagate uncertainty throughout the analysis, leading to more robust and reliable models.\n",
    "\n",
    "4. **Incorporating prior knowledge**: Bayes' theorem enables us to incorporate prior knowledge or domain expertise into our models through the prior distribution. This is particularly useful when dealing with limited or imbalanced data, as it helps regularize the model and prevent overfitting.\n",
    "\n",
    "5. **Sequential learning**: Bayesian inference, which is based on Bayes' theorem, naturally supports sequential learning and updating of beliefs as new data arrive. This is crucial in scenarios where data is streaming or when we need to adapt our models to changing environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding Bayes' theorem and its applications in machine learning, practitioners can make informed decisions, build more accurate and interpretable models, and effectively reason under uncertainty. It provides a solid foundation for tackling complex real-world problems and is a valuable tool in the machine learning and data science toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Additional Resources](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helpful resources to further your understanding of Bayes' theorem and its applications in machine learning:\n",
    "\n",
    "- üìπ [Bayes' Theorem - The Simplest Case](https://www.youtube.com/watch?v=XQoLVl31ZfQ) by 3Blue1Brown\n",
    "- üìù [An Intuitive Explanation of Bayes' Theorem](https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/) by BetterExplained\n",
    "- üìπ [Naive Bayes Classifier - Fun and Easy Machine Learning](https://www.youtube.com/watch?v=CPqOCI0ahss) by Statquest with Josh Starmer\n",
    "- üìπ [Bayesian Inference in Python: A Beginner's Guide](https://www.youtube.com/watch?v=3OJEae7Qb_o) by Data Science Dojo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These resources provide visual explanations, intuitive examples, and practical implementations of Bayes' theorem and its applications in machine learning. They will help solidify your understanding and give you hands-on experience with Bayesian techniques.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
