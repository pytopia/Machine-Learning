{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule Mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining is a powerful technique in data mining and machine learning that aims to discover interesting relationships or patterns in large datasets. It's particularly useful for uncovering associations between different items or events that occur together frequently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining is an unsupervised learning method that identifies strong rules discovered in databases using different measures of interestingness. These rules can reveal hidden patterns and relationships within data, providing valuable insights for decision-making processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic example of association rule mining is the \"market basket analysis\" in retail. For instance, a rule might state: \"Customers who buy bread are likely to buy milk as well.\" This information can be invaluable for strategic product placement, promotional offers, and inventory management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/basket.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/transactions.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of association rule mining was first introduced by Rakesh Agrawal, Tomasz Imieli≈Ñski, and Arun Swami in 1993. Their work on mining association rules between sets of items in large databases laid the foundation for this field of study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** Understanding the historical context can provide valuable insights into the development and evolution of association rule mining techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining finds applications in various domains:\n",
    "\n",
    "1. **Retail and E-commerce**: Analyzing customer purchasing patterns to optimize product placement and recommendations.\n",
    "\n",
    "2. **Healthcare**: Identifying relationships between symptoms, diseases, and treatments.\n",
    "\n",
    "3. **Bioinformatics**: Discovering associations in genetic data.\n",
    "\n",
    "4. **Web Usage Mining**: Analyzing web logs to improve website design and user experience.\n",
    "\n",
    "5. **Fraud Detection**: Identifying unusual patterns in financial transactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of association rule mining typically involves the following steps:\n",
    "\n",
    "1. **Data Preparation**: Cleaning and transforming the data into a suitable format.\n",
    "\n",
    "2. **Frequent Itemset Generation**: Identifying sets of items that frequently appear together.\n",
    "\n",
    "3. **Rule Generation**: Creating rules based on the frequent itemsets.\n",
    "\n",
    "4. **Rule Evaluation**: Assessing the quality of the generated rules using various metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, association rule mining is based on set theory and probability. A rule is typically represented as an implication:\n",
    "\n",
    "$$X \\Rightarrow Y$$\n",
    "\n",
    "Where $X$ and $Y$ are sets of items (or itemsets), and $X \\cap Y = \\emptyset$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** The empty intersection between $X$ and $Y$ ensures that we don't derive trivial rules where an item implies itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining is a versatile and powerful technique in the data scientist's toolkit. By uncovering hidden patterns in large datasets, it provides actionable insights that can drive business decisions, improve customer experiences, and optimize various processes across different industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/frequent-itemset-mining-algorithms.webp\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we'll delve deeper into the fundamental concepts, algorithms, and practical applications of association rule mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Fundamental Concepts and Terminology](#toc1_)    \n",
    "  - [Itemsets and Transactions](#toc1_1_)    \n",
    "  - [Support and Frequency](#toc1_2_)    \n",
    "  - [Confidence](#toc1_3_)    \n",
    "  - [Lift](#toc1_4_)    \n",
    "  - [Frequent Itemsets](#toc1_5_)    \n",
    "  - [Strong Association Rules](#toc1_6_)    \n",
    "  - [Pruning and Anti-monotonicity Property](#toc1_7_)    \n",
    "  - [Practical Example: Market Basket Analysis](#toc1_8_)    \n",
    "- [The Apriori Algorithm](#toc2_)    \n",
    "  - [Basic Principle](#toc2_1_)    \n",
    "  - [Algorithm Overview](#toc2_2_)    \n",
    "  - [Detailed Algorithm Steps](#toc2_3_)    \n",
    "  - [Example Implementation](#toc2_4_)    \n",
    "  - [Advantages and Limitations of Apriori](#toc2_5_)    \n",
    "- [FP-Growth Algorithm (Optional)](#toc3_)    \n",
    "  - [Algorithm Overview](#toc3_1_)    \n",
    "  - [Step 1: FP-Tree Construction](#toc3_2_)    \n",
    "  - [Step 2: Frequent Itemset Extraction](#toc3_3_)    \n",
    "  - [Example Implementation](#toc3_4_)    \n",
    "  - [Advantages and Limitations of FP-Growth](#toc3_5_)    \n",
    "  - [Practical Considerations](#toc3_6_)    \n",
    "- [Measures of Interestingness](#toc4_)    \n",
    "  - [Support](#toc4_1_)    \n",
    "  - [Confidence](#toc4_2_)    \n",
    "  - [Lift](#toc4_3_)    \n",
    "  - [Conviction](#toc4_4_)    \n",
    "  - [Leverage](#toc4_5_)    \n",
    "  - [Jaccard Index](#toc4_6_)    \n",
    "  - [Kulczynski Measure](#toc4_7_)    \n",
    "  - [Example Implementation](#toc4_8_)    \n",
    "  - [Choosing the Right Measures](#toc4_9_)    \n",
    "- [Practical Applications and Case Studies](#toc5_)    \n",
    "  - [Retail and E-commerce](#toc5_1_)    \n",
    "  - [Healthcare and Pharmaceuticals](#toc5_2_)    \n",
    "  - [Financial Services](#toc5_3_)    \n",
    "  - [Web Usage Mining](#toc5_4_)    \n",
    "  - [Manufacturing and Quality Control](#toc5_5_)    \n",
    "  - [Implementation Challenges and Best Practices](#toc5_6_)    \n",
    "- [Summary](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Fundamental Concepts and Terminology](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the basic concepts and terminology is crucial for mastering Association Rule Mining. Let's break down these concepts with more detailed explanations and examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Itemsets and Transactions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of Association Rule Mining are two fundamental concepts: items and transactions.\n",
    "\n",
    "- **Item**: An item is a single object or entity in your dataset. \n",
    "  - Example: In a grocery store, items could be \"milk\", \"bread\", \"eggs\", etc.\n",
    "\n",
    "- **Itemset**: An itemset is a collection of one or more items. \n",
    "  - Example: {milk, bread} or {coffee, sugar, cream}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/itemset.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transaction is a set of items that are grouped together in some meaningful way.\n",
    "\n",
    "- In retail: A transaction is typically a single customer purchase.\n",
    "- In web analytics: A transaction might be the set of pages a user visits in a single session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of transactions in a grocery store:\n",
    "\n",
    "```\n",
    "Transaction 1: {milk, bread, eggs}\n",
    "Transaction 2: {bread, butter, jam}\n",
    "Transaction 3: {milk, cereal, banana}\n",
    "Transaction 4: {bread, milk, eggs, cheese}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** When working with transaction data, it's often represented as a list of lists or a binary matrix, where each row is a transaction and each column represents an item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Support and Frequency](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/support-confidence-lift.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support and frequency are measures of how common an itemset is in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support** is the proportion of transactions in the dataset that contain a particular itemset.\n",
    "\n",
    "$$Support(X) = \\frac{\\text{Number of transactions containing X}}{\\text{Total number of transactions}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "If {milk, bread} appears in 3 out of 10 transactions:\n",
    "$Support(\\text{\\{milk, bread\\}}) = \\frac{3}{10} = 0.3$ or 30%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Count** is simply the number of transactions that contain the itemset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the Support Count for {milk, bread} would be 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Support helps identify itemsets that are frequent enough to be interesting. Items with very low support might be discarded as they're not representative of common patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Confidence](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence measures how often the rule is true. It's the probability of finding the consequent of the rule in transactions that contain the antecedent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a rule $X \\Rightarrow Y$ (read as \"X implies Y\"):\n",
    "\n",
    "$$Confidence(X \\Rightarrow Y) = \\frac{Support(X \\cup Y)}{Support(X)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Let's say we have the rule {bread} => {milk}\n",
    "- Support({bread, milk}) = 0.4 (they appear together in 40% of transactions)\n",
    "- Support({bread}) = 0.6 (bread appears in 60% of transactions)\n",
    "\n",
    "$Confidence(\\text{\\{bread\\}} \\Rightarrow \\text{\\{milk\\}}) = \\frac{0.4}{0.6} = 0.67$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that when bread is purchased, milk is also purchased 67% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Lift](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lift measures how much more often X and Y occur together than expected if they were statistically independent.\n",
    "\n",
    "$$Lift(X \\Rightarrow Y) = \\frac{Support(X \\cup Y)}{Support(X) \\times Support(Y)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- Lift > 1: X and Y appear together more often than expected (positive correlation)\n",
    "- Lift < 1: X and Y appear together less often than expected (negative correlation)\n",
    "- Lift = 1: X and Y appear together as often as expected (independent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Continuing with {bread} => {milk}\n",
    "- Support({bread, milk}) = 0.4\n",
    "- Support({bread}) = 0.6\n",
    "- Support({milk}) = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Lift(\\text{\\{bread\\}} \\Rightarrow \\text{\\{milk\\}}) = \\frac{0.4}{0.6 \\times 0.5} = 1.33$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lift of 1.33 suggests that bread and milk are bought together 33% more often than if their purchases were completely independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_'></a>[Frequent Itemsets](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequent itemset is an itemset whose support is greater than or equal to a user-specified minimum support threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "If we set a minimum support threshold of 30%:\n",
    "- `{milk, bread}` with support 40% would be a frequent itemset\n",
    "- `{caviar, truffle}` with support 1% would not be a frequent itemset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_'></a>[Strong Association Rules](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strong association rule is a rule that meets both a minimum support threshold and a minimum confidence threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "With min_support = 30% and min_confidence = 60%:\n",
    "- `{bread} => {milk}` with support 40% and confidence 67% would be a strong rule\n",
    "- `{eggs} => {bacon}` with support 20% and confidence 75% would not be a strong rule (fails support threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_'></a>[Pruning and Anti-monotonicity Property](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning is a technique used to reduce the number of candidate itemsets to check, making algorithms more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anti-monotonicity property (also known as the downward-closure property) states:\n",
    "\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- Conversely, if an itemset is infrequent, then all of its supersets must be infrequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "If `{A, B, C}` is frequent, then `{A, B}`, `{B, C}`, `{A, C}`, `{A}`, `{B}`, and `{C}` must all be frequent.\n",
    "If `{X, Y}` is infrequent, then `{X, Y, Z}` must be infrequent for any item `Z`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** This property allows algorithms to \"prune\" the search space, significantly reducing computation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_8_'></a>[Practical Example: Market Basket Analysis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through a small example to tie these concepts together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support({bread, milk}) = 0.60\n",
      "Confidence(bread => milk) = 0.75\n",
      "Lift(bread => milk) = 0.94\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    ['bread', 'milk'],\n",
    "    ['bread', 'diapers', 'beer', 'eggs'],\n",
    "    ['milk', 'diapers', 'beer', 'cola'],\n",
    "    ['bread', 'milk', 'diapers', 'beer'],\n",
    "    ['bread', 'milk', 'diapers', 'cola']\n",
    "]\n",
    "\n",
    "total_transactions = len(transactions)\n",
    "\n",
    "# Calculate support for {bread, milk}\n",
    "bread_milk_support = sum(1 for t in transactions if 'bread' in t and 'milk' in t) / total_transactions\n",
    "print(f\"Support({{bread, milk}}) = {bread_milk_support:.2f}\")\n",
    "\n",
    "# Calculate confidence for the rule {bread} => {milk}\n",
    "bread_support = sum(1 for t in transactions if 'bread' in t) / total_transactions\n",
    "confidence_bread_milk = bread_milk_support / bread_support\n",
    "print(f\"Confidence(bread => milk) = {confidence_bread_milk:.2f}\")\n",
    "\n",
    "# Calculate lift for {bread} => {milk}\n",
    "milk_support = sum(1 for t in transactions if 'milk' in t) / total_transactions\n",
    "lift_bread_milk = bread_milk_support / (bread_support * milk_support)\n",
    "print(f\"Lift(bread => milk) = {lift_bread_milk:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to calculate support, confidence, and lift for a simple rule in a market basket scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These fundamental concepts form the backbone of Association Rule Mining. Understanding them thoroughly will help you grasp the mechanics of algorithms like Apriori and FP-Growth, which we'll explore in subsequent sections. Remember, the key is to find rules that are both frequent (high support) and strong (high confidence), while also considering the lift to ensure the associations are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[The Apriori Algorithm](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is one of the most popular and well-known algorithms for mining frequent itemsets and generating association rules. Developed by R. Agrawal and R. Srikant in 1994, it has become a fundamental technique in data mining and association rule learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Basic Principle](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is based on the \"apriori principle\" which states:\n",
    "\n",
    "> All subsets of a frequent itemset must also be frequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, if an itemset is infrequent, all its supersets will also be infrequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** This principle allows the algorithm to significantly reduce the search space, making it computationally efficient for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Algorithm Overview](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm works in a step-wise fashion:\n",
    "\n",
    "1. Generate candidate itemsets of size k\n",
    "2. Scan the database to count the support of each candidate\n",
    "3. Determine frequent itemsets based on the minimum support threshold\n",
    "4. Repeat steps 1-3, incrementing k each time, until no new frequent itemsets are found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/apriori.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Detailed Algorithm Steps](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Generate Candidate 1-Itemsets\n",
    "- Scan the database and count the occurrences of each item\n",
    "- Create C‚ÇÅ (candidate 1-itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Generate Frequent 1-Itemsets\n",
    "- Apply minimum support threshold to C‚ÇÅ\n",
    "- Create L‚ÇÅ (frequent 1-itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Generate Candidate k-Itemsets (k > 1)\n",
    "- Join L‚Çñ‚Çã‚ÇÅ with itself to create C‚Çñ\n",
    "- Prune C‚Çñ using the apriori principle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Generate Frequent k-Itemsets\n",
    "- Scan the database to count support for each candidate in C‚Çñ\n",
    "- Apply minimum support threshold to create L‚Çñ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Repeat Steps 3-4\n",
    "- Continue until no new frequent itemsets are found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Generate Association Rules\n",
    "- For each frequent itemset l, generate all non-empty subsets\n",
    "- For each non-empty subset s of l, output the rule s ‚áí (l - s) if confidence ‚â• minimum confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Example Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simplified Python implementation of the Apriori algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "{'C'}: 4\n",
      "{'B'}: 5\n",
      "{'D'}: 3\n",
      "{'A'}: 3\n",
      "{'E'}: 3\n",
      "{'D', 'B'}: 3\n",
      "{'A', 'B'}: 3\n",
      "{'C', 'B'}: 4\n",
      "{'C', 'D'}: 2\n",
      "{'D', 'A'}: 2\n",
      "{'C', 'A'}: 3\n",
      "{'C', 'E'}: 2\n",
      "{'B', 'E'}: 3\n",
      "{'C', 'D', 'A'}: 2\n",
      "{'A', 'D', 'B'}: 2\n",
      "{'A', 'B', 'C'}: 3\n",
      "{'C', 'D', 'B'}: 2\n",
      "{'C', 'B', 'E'}: 2\n",
      "{'C', 'B', 'D', 'A'}: 2\n",
      "\n",
      "Association Rules:\n",
      "{'D'} -> {'B'} (Confidence: 1.00)\n",
      "{'B'} -> {'D'} (Confidence: 0.60)\n",
      "{'A'} -> {'B'} (Confidence: 1.00)\n",
      "{'B'} -> {'A'} (Confidence: 0.60)\n",
      "{'C'} -> {'B'} (Confidence: 1.00)\n",
      "{'B'} -> {'C'} (Confidence: 0.80)\n",
      "{'D'} -> {'C'} (Confidence: 0.67)\n",
      "{'D'} -> {'A'} (Confidence: 0.67)\n",
      "{'A'} -> {'D'} (Confidence: 0.67)\n",
      "{'C'} -> {'A'} (Confidence: 0.75)\n",
      "{'A'} -> {'C'} (Confidence: 1.00)\n",
      "{'E'} -> {'C'} (Confidence: 0.67)\n",
      "{'B'} -> {'E'} (Confidence: 0.60)\n",
      "{'E'} -> {'B'} (Confidence: 1.00)\n",
      "{'D'} -> {'C', 'A'} (Confidence: 0.67)\n",
      "{'A'} -> {'C', 'D'} (Confidence: 0.67)\n",
      "{'C', 'D'} -> {'A'} (Confidence: 1.00)\n",
      "{'C', 'A'} -> {'D'} (Confidence: 0.67)\n",
      "{'D', 'A'} -> {'C'} (Confidence: 1.00)\n",
      "{'A'} -> {'D', 'B'} (Confidence: 0.67)\n",
      "{'D'} -> {'B', 'A'} (Confidence: 0.67)\n",
      "{'D', 'A'} -> {'B'} (Confidence: 1.00)\n",
      "{'B', 'A'} -> {'D'} (Confidence: 0.67)\n",
      "{'D', 'B'} -> {'A'} (Confidence: 0.67)\n",
      "{'A'} -> {'C', 'B'} (Confidence: 1.00)\n",
      "{'B'} -> {'C', 'A'} (Confidence: 0.60)\n",
      "{'C'} -> {'B', 'A'} (Confidence: 0.75)\n",
      "{'B', 'A'} -> {'C'} (Confidence: 1.00)\n",
      "{'C', 'A'} -> {'B'} (Confidence: 1.00)\n",
      "{'C', 'B'} -> {'A'} (Confidence: 0.75)\n",
      "{'D'} -> {'C', 'B'} (Confidence: 0.67)\n",
      "{'C', 'D'} -> {'B'} (Confidence: 1.00)\n",
      "{'D', 'B'} -> {'C'} (Confidence: 0.67)\n",
      "{'E'} -> {'C', 'B'} (Confidence: 0.67)\n",
      "{'C', 'E'} -> {'B'} (Confidence: 1.00)\n",
      "{'B', 'E'} -> {'C'} (Confidence: 0.67)\n",
      "{'A'} -> {'C', 'D', 'B'} (Confidence: 0.67)\n",
      "{'D'} -> {'C', 'B', 'A'} (Confidence: 0.67)\n",
      "{'C', 'A'} -> {'D', 'B'} (Confidence: 0.67)\n",
      "{'C', 'D'} -> {'B', 'A'} (Confidence: 1.00)\n",
      "{'B', 'A'} -> {'C', 'D'} (Confidence: 0.67)\n",
      "{'D', 'A'} -> {'C', 'B'} (Confidence: 1.00)\n",
      "{'D', 'B'} -> {'C', 'A'} (Confidence: 0.67)\n",
      "{'C', 'B', 'A'} -> {'D'} (Confidence: 0.67)\n",
      "{'C', 'D', 'A'} -> {'B'} (Confidence: 1.00)\n",
      "{'C', 'D', 'B'} -> {'A'} (Confidence: 1.00)\n",
      "{'B', 'D', 'A'} -> {'C'} (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_candidates(itemsets, k):\n",
    "    return set([i.union(j) for i in itemsets for j in itemsets if len(i.union(j)) == k])\n",
    "\n",
    "def apriori(transactions, min_support, min_confidence):\n",
    "    # Convert transactions to sets if they aren't already\n",
    "    transactions = [set(t) for t in transactions]\n",
    "\n",
    "    # Count individual items\n",
    "    itemsets = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            itemsets[frozenset([item])] += 1\n",
    "\n",
    "    # Filter itemsets by minimum support\n",
    "    itemsets = {k: v for k, v in itemsets.items() if v >= min_support}\n",
    "\n",
    "    k = 2\n",
    "    all_itemsets = dict(itemsets)\n",
    "\n",
    "    while itemsets:\n",
    "        # Generate candidate itemsets of size k\n",
    "        candidates = generate_candidates(set(itemsets.keys()), k)\n",
    "\n",
    "        # Count candidate itemsets\n",
    "        temp_itemsets = defaultdict(int)\n",
    "        for transaction in transactions:\n",
    "            for candidate in candidates:\n",
    "                if candidate.issubset(transaction):\n",
    "                    temp_itemsets[candidate] += 1\n",
    "\n",
    "        # Filter itemsets by minimum support\n",
    "        itemsets = {k: v for k, v in temp_itemsets.items() if v >= min_support}\n",
    "        all_itemsets.update(itemsets)\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Generate association rules\n",
    "    rules = []\n",
    "    for itemset in all_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    antecedent = frozenset(antecedent)\n",
    "                    consequent = itemset - antecedent\n",
    "                    confidence = all_itemsets[itemset] / all_itemsets[antecedent]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence))\n",
    "\n",
    "    return all_itemsets, rules\n",
    "\n",
    "# Example usage\n",
    "transactions = [\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "    ['B', 'C', 'E'],\n",
    "    ['A', 'B', 'C', 'E'],\n",
    "    ['B', 'D', 'E'],\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "]\n",
    "\n",
    "min_support = 2\n",
    "min_confidence = 0.6\n",
    "\n",
    "frequent_itemsets, association_rules = apriori(transactions, min_support, min_confidence)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset, support in frequent_itemsets.items():\n",
    "    print(f\"{set(itemset)}: {support}\")\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "for antecedent, consequent, confidence in association_rules:\n",
    "    print(f\"{set(antecedent)} -> {set(consequent)} (Confidence: {confidence:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use third-party libraries to implement the Apriori algorithm, such as `pip install efficient-apriori`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficient-apriori\n",
      "  Downloading efficient_apriori-2.0.5-py3-none-any.whl.metadata (6.7 kB)\n",
      "Downloading efficient_apriori-2.0.5-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: efficient-apriori\n",
      "Successfully installed efficient-apriori-2.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install efficient-apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{eggs} -> {bacon}, {soup} -> {bacon}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from efficient_apriori import apriori\n",
    "transactions = [('eggs', 'bacon', 'soup'),\n",
    "                ('eggs', 'bacon', 'apple'),\n",
    "                ('soup', 'bacon', 'banana')]\n",
    "itemsets, rules = apriori(transactions, min_support=0.5, min_confidence=1)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 1.0], [0.6666666666666666, 0.6666666666666666], [1.0, 1.0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rule.lift for rule in rules], [rule.support for rule in rules], [rule.confidence for rule in rules]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Advantages and Limitations of Apriori](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, apriori has many applications and can be used to discover interesting patterns in large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/apriori-application.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main advantages and three main limitations of the Apriori algorithm:\n",
    "1. **Simple and Easy to Implement**: The algorithm is straightforward and can be easily coded.\n",
    "2. **Uses Large Itemset Property**: It leverages the apriori principle to reduce the search space.\n",
    "3. **Easily Parallelizable**: The algorithm can be parallelized to handle large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, nothing comes without drawbacks:\n",
    "\n",
    "1. **Multiple Scans of Database**: It requires multiple passes over the database, which can be time-consuming for large datasets.\n",
    "2. **Generation of Candidate Sets**: For large numbers of frequent itemsets, it can generate a huge number of candidate sets.\n",
    "3. **Difficulty with Long Patterns**: It struggles with very long frequent patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** While Apriori is foundational, more efficient algorithms like FP-Growth have been developed to address some of its limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is a cornerstone in association rule mining. Its simplicity and effectiveness make it a great starting point for understanding frequent itemset mining. However, for large-scale or complex datasets, more advanced algorithms might be preferred. In the next section, we'll explore one such algorithm: FP-Growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[FP-Growth Algorithm (Optional)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FP-Growth (Frequent Pattern Growth) algorithm is an efficient and scalable method for mining the complete set of frequent patterns. Developed by Han et al. in 2000, it addresses some of the limitations of the Apriori algorithm, particularly in dealing with large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP Growth algorithm applies the Apriori Principle too, instead, it build a FP Tree in the beginning. This data structure helps it to mine the frequent itemsets more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the algorithm, let's understand some key concepts:\n",
    "\n",
    "1. **FP-Tree (Frequent Pattern Tree)**: A compact data structure that represents the dataset.\n",
    "2. **Header Table**: A structure that keeps track of frequent items and their frequencies.\n",
    "3. **Node Links**: Pointers that connect nodes containing the same item across different branches of the FP-Tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fp-growth.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick overview of the FP-Growth algorithm:\n",
    "\n",
    "1. Basic Concept:\n",
    "   FP-Growth uses a compressed representation of the dataset called an FP-tree (Frequent Pattern tree). This tree structure allows the algorithm to extract frequent itemsets directly from the tree, avoiding the need for repeated database scans.\n",
    "\n",
    "2. Key Steps:\n",
    "\n",
    "   a. Scan the dataset to determine the frequency of individual items.\n",
    "\n",
    "   b. Sort items in each transaction based on their frequency (most frequent first).\n",
    "\n",
    "   c. Build the FP-tree:\n",
    "      - Create the root node of the tree, labeled \"null\"\n",
    "      - For each transaction in the dataset:\n",
    "        - Sort items based on frequency\n",
    "        - Insert the sorted transaction into the tree\n",
    "\n",
    "   d. Mine the FP-tree to extract frequent patterns.\n",
    "\n",
    "3. FP-Tree Construction:\n",
    "   - Each node in the FP-tree represents an item and contains:\n",
    "     - Item name\n",
    "     - Frequency count\n",
    "     - Link to the next node with the same item name (node-link)\n",
    "   - Transactions with common prefixes share the same path in the tree\n",
    "   - This compression significantly reduces the size of the dataset representation\n",
    "\n",
    "4. Mining the FP-Tree:\n",
    "   - Start from the least frequent item\n",
    "   - Find all paths containing this item (conditional pattern base)\n",
    "   - Create a conditional FP-tree for this item\n",
    "   - Recursively mine the conditional FP-tree\n",
    "   - Combine the item with the patterns found in the conditional FP-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Algorithm Overview](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FP-Growth algorithm consists of two main steps:\n",
    "\n",
    "1. **FP-Tree Construction**: Build a compact data structure called the FP-Tree.\n",
    "2. **Frequent Itemset Extraction**: Extract frequent itemsets directly from the FP-Tree.\n",
    "\n",
    "Let's explore each step in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Step 1: FP-Tree Construction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 First Database Scan**\n",
    "- Count the frequency of each item in the database.\n",
    "- Create a list of frequent items (meeting minimum support threshold), sorted in descending order of frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Second Database Scan and Tree Building**\n",
    "- Create the root of the tree, labeled with \"null\".\n",
    "- For each transaction in the database:\n",
    "  - Sort the frequent items in the transaction according to the frequency list.\n",
    "  - Insert the sorted transaction into the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** The FP-Tree structure allows multiple transactions to overlap when they share common prefixes, leading to a compact representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Step 2: Frequent Itemset Extraction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction process involves these sub-steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Bottom-up Traversal**\n",
    "- Start from each frequent item in the header table.\n",
    "- Generate a conditional pattern base for each item.\n",
    "- Construct a conditional FP-Tree from each conditional pattern base.\n",
    "- Recursively mine the conditional FP-Trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Pattern Growth**\n",
    "- Frequent itemsets are grown by concatenating suffix patterns with frequent patterns generated from conditional FP-Trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Example Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simplified Python implementation of the FP-Growth algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "{'A'}: 3\n",
      "{'C', 'A'}: 3\n",
      "{'C', 'B', 'A'}: 3\n",
      "{'B', 'A'}: 3\n",
      "{'B'}: 5\n",
      "{'C'}: 4\n",
      "{'C', 'B'}: 4\n",
      "{'D'}: 3\n",
      "{'D', 'A'}: 2\n",
      "{'B', 'D', 'A'}: 2\n",
      "{'C', 'D'}: 2\n",
      "{'C', 'D', 'A'}: 2\n",
      "{'C', 'D', 'B'}: 2\n",
      "{'C', 'A', 'D', 'B'}: 2\n",
      "{'D', 'B'}: 3\n",
      "{'E'}: 3\n",
      "{'C', 'E'}: 2\n",
      "{'C', 'B', 'E'}: 2\n",
      "{'B', 'E'}: 3\n",
      "{'A', 'E'}: 1\n",
      "{'C', 'A', 'E'}: 1\n",
      "{'C', 'B', 'A', 'E'}: 1\n",
      "{'B', 'A', 'E'}: 1\n",
      "{'D', 'E'}: 1\n",
      "{'D', 'B', 'E'}: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class FPNode:\n",
    "    def __init__(self, item, count, parent):\n",
    "        self.item = item\n",
    "        self.count = count\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.next = None\n",
    "\n",
    "class FPTree:\n",
    "    def __init__(self, transactions, threshold):\n",
    "        self.freq = self.get_frequency(transactions, threshold)\n",
    "        self.root = FPNode(None, 0, None)\n",
    "        self.header_table = defaultdict(lambda: None)\n",
    "        self.build(transactions)\n",
    "\n",
    "    def get_frequency(self, transactions, threshold):\n",
    "        items = Counter([item for transaction in transactions for item in transaction])\n",
    "        return {item: count for item, count in items.items() if count >= threshold}\n",
    "\n",
    "    def build(self, transactions):\n",
    "        for transaction in transactions:\n",
    "            sorted_items = [item for item in sorted(transaction, key=lambda x: (-self.freq[x], x)) if item in self.freq]\n",
    "            if len(sorted_items) > 0:\n",
    "                self.insert_tree(sorted_items, self.root, 1)\n",
    "\n",
    "    def insert_tree(self, items, node, count):\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "\n",
    "        if items[0] in node.children:\n",
    "            node.children[items[0]].count += count\n",
    "        else:\n",
    "            new_node = FPNode(items[0], count, node)\n",
    "            node.children[items[0]] = new_node\n",
    "            self.update_header_table(new_node)\n",
    "\n",
    "        self.insert_tree(items[1:], node.children[items[0]], count)\n",
    "\n",
    "    def update_header_table(self, node):\n",
    "        if node.item not in self.header_table:\n",
    "            self.header_table[node.item] = node\n",
    "        else:\n",
    "            current = self.header_table[node.item]\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = node\n",
    "\n",
    "def fp_growth(tree, alpha=set()):\n",
    "    if len(tree.root.children) == 0:\n",
    "        return\n",
    "\n",
    "    for item in tree.freq:\n",
    "        new_alpha = alpha.copy()\n",
    "        new_alpha.add(item)\n",
    "        support = sum(node.count for node in get_path(tree.header_table[item]))\n",
    "        yield (new_alpha, support)\n",
    "\n",
    "        cond_tree = build_cond_tree(tree, item)\n",
    "        yield from fp_growth(cond_tree, new_alpha)\n",
    "\n",
    "def get_path(node):\n",
    "    while node:\n",
    "        yield node\n",
    "        node = node.next\n",
    "\n",
    "def build_cond_tree(tree, item):\n",
    "    cond_transactions = []\n",
    "    node = tree.header_table[item]\n",
    "    while node:\n",
    "        path = []\n",
    "        support = node.count\n",
    "        parent = node.parent\n",
    "        while parent.item:\n",
    "            path.append(parent.item)\n",
    "            parent = parent.parent\n",
    "        if len(path) > 0:\n",
    "            cond_transactions.extend([path] * support)\n",
    "        node = node.next\n",
    "    return FPTree(cond_transactions, 1)\n",
    "\n",
    "# Example usage\n",
    "transactions = [\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "    ['B', 'C', 'E'],\n",
    "    ['A', 'B', 'C', 'E'],\n",
    "    ['B', 'D', 'E'],\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "]\n",
    "min_support = 2\n",
    "\n",
    "fp_tree = FPTree(transactions, min_support)\n",
    "frequent_itemsets = list(fp_growth(fp_tree))\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset, support in frequent_itemsets:\n",
    "    print(f\"{itemset}: {support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP-growth implementation is also available in pip as `fpgrowth_py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpgrowth_py\n",
      "  Downloading fpgrowth_py-1.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading fpgrowth_py-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: fpgrowth_py\n",
      "Successfully installed fpgrowth_py-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fpgrowth_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'bacon'}, {'eggs'}, 0.6666666666666666], [{'eggs'}, {'bacon'}, 1.0], [{'bacon'}, {'soup'}, 0.6666666666666666], [{'soup'}, {'bacon'}, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "from fpgrowth_py import fpgrowth\n",
    "itemSetList = [['eggs', 'bacon', 'soup'],\n",
    "                ['eggs', 'bacon', 'apple'],\n",
    "                ['soup', 'bacon', 'banana']]\n",
    "freqItemSet, rules = fpgrowth(itemSetList, minSupRatio=0.5, minConf=0.5)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_'></a>[Advantages and Limitations of FP-Growth](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP-Growth has several advantages over Apriori:\n",
    "1. **No Candidate Generation**: Unlike Apriori, FP-Growth doesn't need to generate candidate itemsets.\n",
    "2. **Fewer Database Scans**: It only requires two passes over the dataset.\n",
    "3. **Compact Data Structure**: The FP-Tree provides a compact representation of the transaction database.\n",
    "4. **Efficiency**: It's generally faster than Apriori, especially for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are two main limitations:\n",
    "\n",
    "1. **Memory Intensive**: The FP-Tree structure can be memory-intensive for very large datasets.\n",
    "2. **Complexity**: The algorithm is more complex to implement compared to Apriori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** While FP-Growth is generally more efficient than Apriori, its performance can degrade if the dataset doesn't fit entirely in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_6_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing FP-Growth in real-world scenarios, consider the following:\n",
    "\n",
    "1. **Data Characteristics**: FP-Growth performs best when there are many repeated transactions in the dataset.\n",
    "2. **Memory Management**: For very large datasets, consider techniques like partitioning or disk-based variants of FP-Growth.\n",
    "3. **Parallelization**: FP-Growth can be parallelized, which is useful for processing big data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** When dealing with sparse datasets, consider using compressed or sparse representations of the FP-Tree to save memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FP-Growth algorithm represents a significant advancement in frequent itemset mining. Its ability to mine patterns without candidate generation makes it particularly suited for dense datasets with many frequent patterns. Understanding both Apriori and FP-Growth provides a solid foundation for tackling association rule mining problems in various domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll explore how to evaluate the quality of the discovered patterns using various interestingness measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Measures of Interestingness](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In association rule mining, not all discovered rules are equally valuable or interesting. Measures of interestingness help us evaluate the quality and utility of the mined rules. These measures provide quantitative metrics to assess the strength, significance, and usefulness of association rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are they important?\n",
    "\n",
    "1. **Rule Filtering**: They help in filtering out less useful or trivial rules.\n",
    "2. **Rule Ranking**: They allow for ranking rules based on their potential importance.\n",
    "3. **Insight Quality**: They provide a way to assess the quality of insights derived from the data.\n",
    "4. **Business Relevance**: They help in identifying rules that are most likely to be actionable in a business context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some of the most commonly used measures of interestingness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Support](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support measures how frequently an itemset appears in the dataset.\n",
    "\n",
    "$Support(X \\Rightarrow Y) = \\frac{\\text{Number of transactions containing both X and Y}}{\\text{Total number of transactions}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support indicates the frequency of a rule. Higher support suggests a more common pattern, while low support might reveal rare but potentially interesting associations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** Support is often used as an initial filter to identify frequent itemsets before generating rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Confidence](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence measures how often the rule is true.\n",
    "\n",
    "$Confidence(X \\Rightarrow Y) = \\frac{Support(X \\cup Y)}{Support(X)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher confidence indicates a stronger association between X and Y. A confidence of 1 means that whenever X occurs, Y always occurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** High confidence alone doesn't necessarily imply a good rule, as it doesn't account for the base probability of Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Lift](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lift measures how much more often X and Y occur together than expected if they were statistically independent.\n",
    "\n",
    "$Lift(X \\Rightarrow Y) = \\frac{Confidence(X \\Rightarrow Y)}{Support(Y)} = \\frac{Support(X \\cup Y)}{Support(X) \\times Support(Y)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lift > 1: Positive correlation (X and Y appear together more often than expected)\n",
    "- Lift = 1: No correlation (X and Y are independent)\n",
    "- Lift < 1: Negative correlation (X and Y appear together less often than expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_'></a>[Conviction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviction compares the probability that X appears without Y if they were independent with the actual frequency of X occurring without Y.\n",
    "\n",
    "$Conviction(X \\Rightarrow Y) = \\frac{1 - Support(Y)}{1 - Confidence(X \\Rightarrow Y)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher conviction indicates a stronger implication. Conviction is infinite for rules with 100% confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_'></a>[Leverage](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage measures the difference between the observed frequency of X and Y appearing together and the frequency that would be expected if X and Y were independent.\n",
    "\n",
    "$Leverage(X \\Rightarrow Y) = Support(X \\cup Y) - Support(X) \\times Support(Y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive leverage indicates that X and Y appear together more often than expected, while negative leverage suggests the opposite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_'></a>[Jaccard Index](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard Index measures the similarity between itemsets.\n",
    "\n",
    "$Jaccard(X, Y) = \\frac{Support(X \\cup Y)}{Support(X) + Support(Y) - Support(X \\cup Y)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values range from 0 to 1, with 1 indicating perfect similarity. It's useful for comparing the similarity of itemsets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_7_'></a>[Kulczynski Measure](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kulczynski measure is an average of two conditional probabilities.\n",
    "\n",
    "$Kulczynski(X \\Rightarrow Y) = \\frac{1}{2} \\left(\\frac{Support(X \\cup Y)}{Support(X)} + \\frac{Support(X \\cup Y)}{Support(Y)}\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values range from 0 to 1, with higher values indicating stronger associations. It's less sensitive to imbalanced datasets compared to some other measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_8_'></a>[Example Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python function that calculates various interestingness measures:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interestingness_measures(rule, transactions):\n",
    "    X, Y = rule\n",
    "    N = len(transactions)\n",
    "\n",
    "    support_X = sum(1 for t in transactions if X.issubset(t)) / N\n",
    "    support_Y = sum(1 for t in transactions if Y.issubset(t)) / N\n",
    "    support_XY = sum(1 for t in transactions if X.union(Y).issubset(t)) / N\n",
    "\n",
    "    confidence = support_XY / support_X if support_X > 0 else 0\n",
    "    lift = confidence / support_Y if support_Y > 0 else 0\n",
    "    conviction = (1 - support_Y) / (1 - confidence) if confidence < 1 else float('inf')\n",
    "    leverage = support_XY - (support_X * support_Y)\n",
    "    jaccard = support_XY / (support_X + support_Y - support_XY) if (support_X + support_Y - support_XY) > 0 else 0\n",
    "    kulczynski = 0.5 * (support_XY / support_X + support_XY / support_Y) if support_X > 0 and support_Y > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'support': support_XY,\n",
    "        'confidence': confidence,\n",
    "        'lift': lift,\n",
    "        'conviction': conviction,\n",
    "        'leverage': leverage,\n",
    "        'jaccard': jaccard,\n",
    "        'kulczynski': kulczynski\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support: 0.8000\n",
      "confidence: 0.8000\n",
      "lift: 1.0000\n",
      "conviction: 1.0000\n",
      "leverage: 0.0000\n",
      "jaccard: 0.8000\n",
      "kulczynski: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "transactions = [\n",
    "    {'A', 'B', 'C', 'D'},\n",
    "    {'B', 'C', 'E'},\n",
    "    {'A', 'B', 'C', 'E'},\n",
    "    {'B', 'D', 'E'},\n",
    "    {'A', 'B', 'C', 'D'},\n",
    "]\n",
    "\n",
    "rule = ({'B'}, {'C'})\n",
    "measures = calculate_interestingness_measures(rule, transactions)\n",
    "\n",
    "for measure, value in measures.items():\n",
    "    print(f\"{measure}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_9_'></a>[Choosing the Right Measures](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of interestingness measures depends on the specific application and the nature of the data:\n",
    "\n",
    "1. **Business Context**: Consider which measures align best with business objectives.\n",
    "2. **Data Characteristics**: Some measures perform better with balanced datasets, others with imbalanced ones.\n",
    "3. **Interpretability**: Choose measures that can be easily explained to stakeholders.\n",
    "4. **Complementary Measures**: Use a combination of measures to get a comprehensive view of rule quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** It's often beneficial to use multiple measures in conjunction, as each captures different aspects of interestingness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures of interestingness play a crucial role in making association rule mining results actionable and meaningful. By applying these measures, we can filter, rank, and interpret the discovered rules effectively. Remember that the ultimate test of a rule's interestingness often lies in its practical applicability and the insights it provides in the specific domain context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll explore practical applications and case studies of association rule mining, seeing how these measures are applied in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Practical Applications and Case Studies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining (ARM) has found widespread applications across various industries due to its ability to uncover hidden patterns and relationships in large datasets. In this section, we'll explore some practical applications and case studies that demonstrate the power and versatility of ARM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Retail and E-commerce](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common applications of ARM is in the retail and e-commerce sector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Basket Analysis**\n",
    "\n",
    "Case Study: Amazon's Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon uses ARM to analyze customer purchase history and browsing behavior to generate product recommendations.\n",
    "\n",
    "- Approach: They apply ARM algorithms to identify frequently co-occurring items in customer baskets.\n",
    "- Outcome: This led to the \"Customers who bought this item also bought\" feature, which reportedly generates up to 35% of Amazon's sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: When implementing market basket analysis, consider using temporal data to capture seasonal trends and changing customer preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store Layout Optimization**\n",
    "\n",
    "Case Study: Walmart's Store Layout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walmart utilized ARM to optimize its store layouts and product placements.\n",
    "\n",
    "- Approach: They analyzed transaction data to identify unexpected associations between products.\n",
    "- Outcome: One famous discovery was the correlation between beer and diaper sales, leading to strategic product placements and increased sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[Healthcare and Pharmaceuticals](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARM has significant applications in healthcare for improving patient care and drug discovery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disease Association Studies**\n",
    "\n",
    "Case Study: Comorbidity Analysis in Diabetes Patients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers used ARM to study comorbidities in diabetes patients.\n",
    "\n",
    "- Approach: They applied ARM to electronic health records to identify conditions frequently occurring together with diabetes.\n",
    "- Outcome: The study revealed strong associations between diabetes and conditions like hypertension and hyperlipidemia, helping in developing comprehensive treatment plans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drug Interaction Discovery**\n",
    "\n",
    "Case Study: Adverse Drug Reaction Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pharmaceutical companies use ARM to detect potential adverse drug reactions.\n",
    "\n",
    "- Approach: ARM is applied to large databases of patient records and reported side effects.\n",
    "- Outcome: This has led to the early detection of previously unknown drug interactions, improving patient safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_'></a>[Financial Services](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARM is valuable in the financial sector for fraud detection and risk management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraud Detection**\n",
    "\n",
    "Case Study: Credit Card Fraud Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major credit card company implemented ARM to enhance their fraud detection system.\n",
    "\n",
    "- Approach: They used ARM to identify unusual patterns in transaction data that might indicate fraudulent activity.\n",
    "- Outcome: The system significantly improved fraud detection rates while reducing false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk Assessment**\n",
    "\n",
    "Case Study: Insurance Risk Profiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An insurance company used ARM to improve their risk assessment models.\n",
    "\n",
    "- Approach: They applied ARM to customer data, claim history, and external factors.\n",
    "- Outcome: This led to more accurate risk profiles and personalized insurance offerings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_4_'></a>[Web Usage Mining](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARM is extensively used in analyzing web usage patterns to improve user experience and website design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Recommendation**\n",
    "\n",
    "Case Study: Netflix's Viewing Suggestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netflix employs ARM as part of its recommendation engine.\n",
    "\n",
    "- Approach: They analyze viewing history and ratings to find associations between different shows and movies.\n",
    "- Outcome: This resulted in highly personalized content recommendations, significantly improving user engagement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website Navigation Optimization**\n",
    "\n",
    "Case Study: E-learning Platform User Journey Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An e-learning platform used ARM to optimize its course navigation and recommendations.\n",
    "\n",
    "- Approach: They applied ARM to user clickstream data to identify common navigation patterns.\n",
    "- Outcome: This led to improved course recommendations and a more intuitive site structure, increasing user satisfaction and course completion rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_5_'></a>[Manufacturing and Quality Control](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARM has applications in manufacturing for improving quality control and optimizing production processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defect Analysis**\n",
    "\n",
    "Case Study: Semiconductor Manufacturing Yield Improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A semiconductor manufacturer used ARM to improve production yield.\n",
    "\n",
    "- Approach: They applied ARM to production data to identify factors associated with defects.\n",
    "- Outcome: This led to the discovery of previously unknown relationships between production parameters and defect rates, resulting in significant yield improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_6_'></a>[Implementation Challenges and Best Practices](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While implementing ARM in these diverse fields, several common challenges and best practices emerge:\n",
    "\n",
    "1. Data Quality: Ensure data is clean, consistent, and properly formatted before applying ARM.\n",
    "\n",
    "2. Scalability: For large datasets, consider using distributed computing frameworks like Apache Spark.\n",
    "\n",
    "3. Interpretability: Focus on generating rules that are not only statistically significant but also actionable and interpretable in the business context.\n",
    "\n",
    "4. Privacy Concerns: Especially in healthcare and finance, ensure that ARM implementations comply with data protection regulations.\n",
    "\n",
    "5. Dynamic Nature of Data: Regularly update your models to capture changing patterns and trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note**: Always validate the discovered rules with domain experts to ensure they make practical sense and are not just statistical artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These case studies demonstrate the wide-ranging applicability of Association Rule Mining across various industries. From retail to healthcare, finance to manufacturing, ARM has proven to be a powerful tool for uncovering hidden insights in large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to successful implementation lies in not just the technical application of ARM algorithms, but also in the careful interpretation of results and their integration into business processes. As data continues to grow in volume and complexity, the role of ARM in extracting valuable insights is likely to become even more significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining (ARM) is a powerful technique in data mining that uncovers interesting relationships between variables in large datasets. Throughout this course, we've explored:\n",
    "\n",
    "1. **Fundamental Concepts**: We learned about itemsets, transactions, and the basic principles of ARM.\n",
    "\n",
    "2. **Key Algorithms**: We delved into the Apriori algorithm and the FP-Growth algorithm, understanding their mechanics and applications.\n",
    "\n",
    "3. **Measures of Interestingness**: We studied various metrics like support, confidence, lift, and others to evaluate the quality of discovered rules.\n",
    "\n",
    "4. **Practical Applications**: We explored real-world case studies across different industries, demonstrating ARM's versatility and impact.\n",
    "\n",
    "5. **Challenges and Best Practices**: We discussed implementation challenges and strategies to overcome them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARM has proven to be an invaluable tool in various domains, from retail and healthcare to finance and web analytics. Its ability to uncover hidden patterns in data makes it a crucial technique in the data scientist's toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Apriori and FP-Growth are the most well-known, several other algorithms have been developed for association rule mining. Here are some noteworthy ones:\n",
    "\n",
    "1. **Eclat (Equivalence Class Transformation)**\n",
    "   - Uses a depth-first search strategy and a vertical database layout.\n",
    "   - Efficient for sparse datasets.\n",
    "   - Doesn't require multiple database scans.\n",
    "\n",
    "2. **CHARM (Closed Association Rule Mining)**\n",
    "   - Focuses on mining closed itemsets.\n",
    "   - Reduces the number of rules generated without loss of information.\n",
    "   - Particularly useful when dealing with dense datasets.\n",
    "\n",
    "3. **MAFIA (Maximal Frequent Itemset Algorithm)**\n",
    "   - Designed for mining maximal frequent itemsets.\n",
    "   - Uses a depth-first traversal of the itemset lattice.\n",
    "   - Employs efficient pruning techniques to reduce the search space.\n",
    "\n",
    "4. **LCM (Linear time Closed itemset Miner)**\n",
    "   - Mines frequent closed itemsets in linear time.\n",
    "   - Known for its efficiency, especially on dense datasets.\n",
    "\n",
    "5. **GSP (Generalized Sequential Pattern)**\n",
    "   - Extends association rule mining to sequential data.\n",
    "   - Useful for analyzing time-ordered transactions.\n",
    "\n",
    "6. **OPUS (Optimized Pruning for Unordered Search)**\n",
    "   - An any-time algorithm that can be interrupted at any point to return the best rules found so far.\n",
    "   - Particularly useful when dealing with large datasets where complete processing might be time-prohibitive.\n",
    "\n",
    "7. **Quantitative Association Rules (QAR)**\n",
    "   - Extends traditional ARM to handle numerical attributes.\n",
    "   - Allows for mining rules that involve ranges of numerical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each algorithm has its strengths and is suited for different types of datasets and problem domains:\n",
    "\n",
    "- **Apriori**: Simple and widely used, but can be slow on large datasets.\n",
    "- **FP-Growth**: Efficient for large datasets, especially when they fit in memory.\n",
    "- **Eclat**: Performs well on sparse datasets.\n",
    "- **CHARM and LCM**: Excellent for dense datasets and when closed itemsets are of interest.\n",
    "- **MAFIA**: Useful when only maximal frequent itemsets are needed.\n",
    "- **GSP**: The go-to choice for sequential pattern mining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: The choice of algorithm often depends on the specific characteristics of your dataset and the computational resources available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data continues to grow in volume and complexity, we can expect to see:\n",
    "\n",
    "1. **Scalable Algorithms**: Development of more efficient algorithms capable of handling extremely large datasets.\n",
    "2. **Integration with Machine Learning**: Combining ARM with other ML techniques for more sophisticated pattern discovery.\n",
    "3. **Real-time ARM**: Algorithms capable of mining rules from streaming data in real-time.\n",
    "4. **Privacy-Preserving ARM**: Techniques that can mine rules while preserving data privacy, crucial in sensitive domains like healthcare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Mining remains a vital technique in data science, continually evolving to meet new challenges. While Apriori and FP-Growth form the foundation, the field has expanded with algorithms tailored to specific data characteristics and problem domains. As data scientists, understanding the range of available algorithms and their applicability is crucial for effectively uncovering valuable insights from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The journey through ARM doesn't end here. As you apply these techniques in real-world scenarios, you'll discover nuances and challenges that will further deepen your understanding and expertise in this fascinating field of data mining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
