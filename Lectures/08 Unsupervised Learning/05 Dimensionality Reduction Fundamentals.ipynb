{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a crucial concept in machine learning and data science, playing a vital role in handling high-dimensional datasets. It refers to the process of reducing the number of features (or dimensions) in a dataset while retaining as much of the important information as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a technique used to transform high-dimensional data into a lower-dimensional form, making it easier to process and analyze. It can be thought of as a form of data compression, where we aim to represent our data using fewer features without significant loss of information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dim-reduction.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** Think of dimensionality reduction as creating a \"summary\" of your data that captures its essence using fewer words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering why we need dimensionality reduction. Here are some reasons:\n",
    "\n",
    "1. **Data Visualization**: High-dimensional data is difficult to visualize. Reducing dimensions to 2D or 3D allows for effective plotting and visual analysis.\n",
    "\n",
    "2. **Computational Efficiency**: Lower-dimensional data requires less computational power and storage, speeding up machine learning algorithms.\n",
    "\n",
    "3. **Noise Reduction**: By focusing on the most important features, dimensionality reduction can help filter out noise in the data.\n",
    "\n",
    "4. **Feature Selection**: It can help identify the most relevant features in your dataset, providing insights into the underlying data structure.\n",
    "\n",
    "5. **Overcoming the Curse of Dimensionality**: As we'll discuss in the next section, high-dimensional spaces can lead to counterintuitive behaviors in data. Dimensionality reduction helps mitigate these issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent dimensionality reduction mathematically:\n",
    "\n",
    "Given a dataset $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of samples and $d$ is the number of features, dimensionality reduction aims to find a transformation $f$ such that:\n",
    "\n",
    "$$Y = f(X)$$\n",
    "\n",
    "where $Y \\in \\mathbb{R}^{n \\times k}$, and $k < d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to choose $f$ such that $Y$ retains as much relevant information from $X$ as possible, despite having fewer dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a dataset of images of handwritten digits, where each image is 28x28 pixels. Each image can be represented as a vector of 784 dimensions (28 * 28 = 784). However, not all of these pixels are equally important for distinguishing between digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques could help us represent each image using, say, only 50 dimensions, capturing the most important features that distinguish one digit from another. This reduced representation would be much easier to work with for tasks like classification or clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ **Important Note:** While dimensionality reduction can be extremely useful, it's crucial to apply it thoughtfully. Reducing dimensions too aggressively can lead to loss of important information and negatively impact your model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we'll dive deeper into the motivations behind dimensionality reduction, explore different techniques, and discuss how to apply and evaluate these methods effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [The Curse of Dimensionality](#toc1_)    \n",
    "  - [High-Dimensional Data in Machine Learning](#toc1_1_)    \n",
    "  - [Key Aspects of the Curse of Dimensionality](#toc1_2_)    \n",
    "  - [The Curse of Dimensionality in Machine Learning](#toc1_3_)    \n",
    "  - [Mitigating the Curse of Dimensionality](#toc1_4_)    \n",
    "- [Goals and Benefits of Dimensionality Reduction](#toc2_)    \n",
    "  - [Key Benefits of Dimensionality Reduction](#toc2_1_)    \n",
    "  - [Practical Considerations](#toc2_2_)    \n",
    "- [Types of Dimensionality Reduction Techniques](#toc3_)    \n",
    "  - [Feature Selection](#toc3_1_)    \n",
    "  - [Feature Extraction](#toc3_2_)    \n",
    "  - [Choosing the Right Technique](#toc3_3_)    \n",
    "- [Evaluation and Selection of Reduced Dimensions](#toc4_)    \n",
    "  - [Methods for Evaluating Dimensionality Reduction](#toc4_1_)    \n",
    "  - [Selecting the Optimal Number of Dimensions](#toc4_2_)    \n",
    "  - [Challenges and Considerations](#toc4_3_)    \n",
    "- [Summary](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[The Curse of Dimensionality](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a phenomenon that occurs when dealing with high-dimensional data, which can have significant implications for machine learning and data analysis tasks. Understanding this concept is crucial for appreciating the importance of dimensionality reduction techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coined by mathematician Richard E. Bellman in 1957, the curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces. These phenomena can lead to issues in statistical analysis, machine learning, and data mining, typically resulting in an increase in computational efforts required for processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/higher-dim-performance.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** The curse of dimensionality is also known as the Hughes Phenomenon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[High-Dimensional Data in Machine Learning](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we often encounter high-dimensional data:\n",
    "\n",
    "- If we're recording 60 different metrics for each of our shoppers, we're working in a space with 60 dimensions.\n",
    "- If we're analyzing grayscale images sized 50x50, we're working in a space with 2,500 dimensions.\n",
    "- If the images are RGB-colored, the dimensionality increases to 7,500 dimensions (one dimension for each color channel in each pixel in the image).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-dimensional data is formally defined when the number of features (p) is much larger than the number of observations (N), often written as p >> N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Key Aspects of the Curse of Dimensionality](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sparsity of Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of dimensions increases, the amount of data needed to provide a statistically sound and reliable result grows exponentially. In high-dimensional spaces, data becomes sparse, making it challenging to find meaningful patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this example of maintaining the average distance of 10 observations uniformly distributed:\n",
    "- In 1D: 10¹ points needed\n",
    "- In 2D: 10² points needed\n",
    "- In 3D: 10³ points needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/1d.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/2d.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/3d.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern continues exponentially as dimensions increase, quickly becoming computationally infeasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Distance Metrics Lose Meaning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high-dimensional spaces, the concept of distance becomes less meaningful. As dimensionality increases, the distance between any two points in a dataset converges. This phenomenon is sometimes referred to as the \"distance concentration effect.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, for high dimensions $d$, the ratio of the distances of the nearest and farthest neighbors to a given point converges to 1:\n",
    "\n",
    "$\\lim_{d \\to \\infty} \\frac{\\text{dist}_\\text{max} - \\text{dist}_\\text{min}}{\\text{dist}_\\text{min}} \\to 0$\n",
    "\n",
    "This makes it difficult for algorithms that rely on distance metrics (like k-nearest neighbors) to perform effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/avg-distance.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euclidean Distance in High Dimensions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euclidean distance between n-dimensional vectors p = (p1, p2, …, pn) and q = (q1, q2, …, qn) is computed as:\n",
    "\n",
    "$d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i - q_i)^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each new dimension adds a non-negative term to the sum, so the distance increases with the number of dimensions for distinct vectors. This leads to increased sparsity in the feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/123d.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Increased Computational Complexity**\n",
    "\n",
    "As the number of dimensions grows, the computational requirements for processing the data increase exponentially. This affects both the time complexity and memory requirements of algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **The Hughes Phenomenon**\n",
    "\n",
    "The Hughes Phenomenon shows that as the number of features increases, the classifier's performance increases until we reach the optimal number of features. Adding more features based on the same size training set will then degrade the classifier's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[The Curse of Dimensionality in Machine Learning](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the curse of dimensionality manifests in several ways:\n",
    "\n",
    "1. **Overfitting**: With high-dimensional data, models have more parameters to fit, increasing the risk of overfitting, especially when the number of samples is limited.\n",
    "\n",
    "2. **Feature Selection**: As dimensions increase, identifying relevant features becomes more challenging, potentially leading to the inclusion of irrelevant or noisy features.\n",
    "\n",
    "3. **Data Visualization**: High-dimensional data is impossible to visualize directly, making it difficult to gain intuitive understanding of the data structure.\n",
    "\n",
    "4. **Increased Variance**: Higher dimensions provide more opportunities for models to overfit to noise, resulting in poor generalization performance.\n",
    "\n",
    "5. **Sparse Data**: The number of possible unique rows grows exponentially as the number of features increases, making it harder to efficiently generalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ **Important Note:** While machine learning excels at analyzing data with many dimensions (where humans struggle to find patterns), the increased processing power and training data requirements can become prohibitive as dimensions increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple Python code snippet to demonstrate how the volume of a hypercube grows with dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 1, Volume: 2\n",
      "Dimensions: 2, Volume: 4\n",
      "Dimensions: 3, Volume: 8\n",
      "Dimensions: 4, Volume: 16\n",
      "Dimensions: 5, Volume: 32\n",
      "Dimensions: 6, Volume: 64\n",
      "Dimensions: 7, Volume: 128\n",
      "Dimensions: 8, Volume: 256\n",
      "Dimensions: 9, Volume: 512\n",
      "Dimensions: 10, Volume: 1024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hypercube_volume(side_length, dimensions):\n",
    "    return side_length ** dimensions\n",
    "\n",
    "side = 2\n",
    "for dim in range(1, 11):\n",
    "    volume = hypercube_volume(side, dim)\n",
    "    print(f\"Dimensions: {dim}, Volume: {volume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows how rapidly the volume grows, illustrating the sparsity problem in high dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Mitigating the Curse of Dimensionality](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, features are often correlated or do not exhibit much variation. For these reasons, dimensionality reduction helps compress the data without losing much of the signal, and combat the curse while also economizing on memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the curse of dimensionality helps us appreciate why dimensionality reduction is crucial in many data science and machine learning applications. In the next sections, we'll explore various techniques to combat this curse and effectively work with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Goals and Benefits of Dimensionality Reduction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques are employed to address the challenges posed by high-dimensional data. Understanding the goals and benefits of these methods is crucial for effectively applying them in data science and machine learning projects. Here are the primary goals of dimensionality reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Compression**\n",
    "\n",
    "One of the main goals of dimensionality reduction is to compress the data while retaining as much relevant information as possible. This involves representing the data using fewer features or dimensions.\n",
    "\n",
    "2. **Noise Reduction**\n",
    "\n",
    "High-dimensional data often contains noise or irrelevant features. Dimensionality reduction aims to filter out this noise, focusing on the most important aspects of the data.\n",
    "\n",
    "3. **Feature Extraction**\n",
    "\n",
    "Dimensionality reduction techniques can help in extracting meaningful features from the data, potentially uncovering latent structures or patterns that are not immediately apparent in the original high-dimensional space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Key Benefits of Dimensionality Reduction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Improved Computational Efficiency**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the number of dimensions, we can significantly decrease the computational resources required for data processing and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 10000 dimensions: 0.56 seconds\n",
      "Time taken for 1000 dimensions: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example: Impact on computation time\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "def compute_covariance(X):\n",
    "    return np.cov(X.T)\n",
    "\n",
    "# Generate random data\n",
    "n_samples, n_features = 1000, 10000\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Compute covariance matrix\n",
    "start = time()\n",
    "cov_matrix = compute_covariance(X)\n",
    "print(f\"Time taken for {n_features} dimensions: {time() - start:.2f} seconds\")\n",
    "\n",
    "# Reduce dimensions and recompute\n",
    "X_reduced = X[:, :1000]  # Taking only first 100 features\n",
    "start = time()\n",
    "cov_matrix_reduced = compute_covariance(X_reduced)\n",
    "print(f\"Time taken for {X_reduced.shape[1]} dimensions: {time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Enhanced Visualization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing data to two or three dimensions allows for effective visualization, making it easier to gain insights and identify patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** Tools like t-SNE and UMAP are particularly useful for creating low-dimensional visualizations of high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Mitigation of the Curse of Dimensionality**\n",
    "\n",
    "As discussed in the previous section, dimensionality reduction helps address various issues associated with high-dimensional spaces, such as sparsity and the convergence of distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Improved Model Performance**\n",
    "\n",
    "In many cases, reducing dimensions can lead to better performance in machine learning models by:\n",
    "- Reducing overfitting\n",
    "- Improving generalization\n",
    "- Speeding up training times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Feature Selection and Importance**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dimensionality reduction techniques can help identify the most important features in your dataset, providing valuable insights into the underlying data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 importance: 0.1536\n",
      "Feature 2 importance: 0.1335\n",
      "Feature 3 importance: 0.1276\n",
      "Feature 4 importance: 0.1182\n",
      "Feature 5 importance: 0.1017\n",
      "Feature 6 importance: 0.0961\n",
      "Feature 7 importance: 0.0825\n",
      "Feature 8 importance: 0.0734\n",
      "Feature 9 importance: 0.0597\n",
      "Feature 10 importance: 0.0537\n"
     ]
    }
   ],
   "source": [
    "# Example: PCA for feature importance\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "X = np.random.randn(100, 10)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pca.explained_variance_ratio_\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    print(f\"Feature {i+1} importance: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Noise Reduction and Data Cleaning**\n",
    "\n",
    "By focusing on the most significant dimensions, dimensionality reduction can help in filtering out noise and irrelevant variations in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Data Compression for Storage and Transmission**\n",
    "\n",
    "In scenarios where data storage or transmission is a concern, dimensionality reduction can be used to compress the data while retaining most of its informational content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While dimensionality reduction offers numerous benefits, it's important to consider potential drawbacks:\n",
    "\n",
    "❗️ **Important Note:** Aggressive dimensionality reduction can lead to loss of important information. It's crucial to balance the trade-off between data compression and information retention.\n",
    "\n",
    "1. **Interpretability**: Some dimensionality reduction techniques (like PCA) can make it harder to interpret the meaning of individual features.\n",
    "\n",
    "2. **Computational Overhead**: While it reduces long-term computational costs, the initial process of dimensionality reduction itself can be computationally expensive.\n",
    "\n",
    "3. **Loss of Information**: There's always a risk of losing some potentially useful information when reducing dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals and benefits of dimensionality reduction make it a powerful tool in the data scientist's toolkit. By compressing data, reducing noise, and extracting meaningful features, dimensionality reduction techniques can significantly enhance the efficiency and effectiveness of data analysis and machine learning processes. However, it's important to apply these techniques judiciously, always considering the specific requirements and constraints of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Types of Dimensionality Reduction Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques can be broadly categorized into two main types: feature selection and feature extraction. Each type has its own set of methods, each with unique characteristics and applications. Understanding these techniques is crucial for choosing the most appropriate method for your specific data and problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Feature Selection](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection involves choosing a subset of the original features without transforming them. The goal is to identify and retain the most relevant features while discarding the less important ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection.ppm\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Filter Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter methods select features based on their statistical properties, independent of any specific machine learning algorithm.\n",
    "\n",
    "- **Variance Threshold**: Removes features with low variance.\n",
    "- **Correlation-based Feature Selection**: Selects features that are highly correlated with the target variable but have low correlation with each other.\n",
    "- **Mutual Information**: Measures the mutual dependence between features and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example: Variance Threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
    "selector = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "X_selected = selector.fit_transform(X)\n",
    "print(\"Original features:\", X)\n",
    "print(\"Selected features:\", X_selected)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Wrapper Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods use a predictive model to score feature subsets and select the best performing subset.\n",
    "\n",
    "- **Recursive Feature Elimination (RFE)**: Recursively removes features and builds a model on those features that remain.\n",
    "- **Forward Feature Selection**: Iteratively adds the best performing features.\n",
    "- **Backward Feature Elimination**: Starts with all features and iteratively removes the least significant ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Embedded Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods perform feature selection as part of the model construction process.\n",
    "\n",
    "- **Lasso Regression**: Uses L1 regularization to shrink some feature coefficients to zero.\n",
    "- **Random Forest Feature Importance**: Uses the feature importance scores from random forest models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Feature Extraction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction creates new features by transforming the original feature space. These methods aim to find a lower-dimensional representation that captures the essential characteristics of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feat-extraction.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Linear Dimensionality Reduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods assume linear relationships between features.\n",
    "\n",
    "- **Principal Component Analysis (PCA)**: Finds the directions of maximum variance in the data.\n",
    "- **Linear Discriminant Analysis (LDA)**: Finds the directions that maximize the separation between classes.\n",
    "- **Independent Component Analysis (ICA)**: Separates a multivariate signal into additive subcomponents that are statistically independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example: PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(\"Original data shape:\", X.shape)\n",
    "print(\"Reduced data shape:\", X_reduced.shape)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Non-linear Dimensionality Reduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods can capture non-linear relationships in the data.\n",
    "\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Particularly good for visualization of high-dimensional data.\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)**: Similar to t-SNE but often faster and better at preserving global structure.\n",
    "- **Kernel PCA**: A non-linear version of PCA using the kernel trick.\n",
    "- **Autoencoders**: Neural networks that learn to compress and reconstruct data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** Non-linear methods often perform better on complex, real-world datasets but can be more computationally expensive and harder to interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Matrix Factoriza tion Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods decompose the data matrix into lower-rank approximations.\n",
    "\n",
    "- **Singular Value Decomposition (SVD)**: Factorizes the data matrix into three matrices.\n",
    "- **Non-negative Matrix Factorization (NMF)**: Similar to SVD but with non-negativity constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Choosing the Right Technique](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the appropriate dimensionality reduction technique depends on various factors:\n",
    "\n",
    "1. **Data characteristics**: Linear vs. non-linear relationships, sparsity, noise level.\n",
    "2. **Task objective**: Visualization, feature extraction, noise reduction.\n",
    "3. **Interpretability requirements**: Some methods (like PCA) produce features that are harder to interpret.\n",
    "4. **Computational resources**: Non-linear methods often require more computational power.\n",
    "5. **Dataset size**: Some methods (like t-SNE) don't scale well to very large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ **Important Note:** There's no one-size-fits-all solution in dimensionality reduction. It's often beneficial to try multiple methods and compare their performance on your specific dataset and problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the various types of dimensionality reduction techniques allows data scientists to make informed decisions when tackling high-dimensional datasets. Whether you choose feature selection to retain original features or feature extraction to create new representations, these methods provide powerful tools for managing the complexity of modern datasets. In the next sections, we'll delve deeper into how to apply and evaluate these techniques effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Evaluation and Selection of Reduced Dimensions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying dimensionality reduction techniques, it's crucial to evaluate the effectiveness of the reduction and determine the optimal number of dimensions to retain. This process ensures that we strike the right balance between data compression and information preservation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper evaluation of dimensionality reduction results is critical because:\n",
    "\n",
    "1. It helps prevent over-reduction, which can lead to significant information loss.\n",
    "2. It ensures that the reduced dataset still captures the essential patterns and relationships in the original data.\n",
    "3. It aids in optimizing computational efficiency without sacrificing model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Methods for Evaluating Dimensionality Reduction](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Explained Variance Ratio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is primarily used with PCA and similar techniques. It measures the proportion of variance explained by each principal component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** Look for the \"elbow\" in the curve, where the explained variance starts to level off. This point often indicates a good trade-off between dimensionality reduction and information retention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Reconstruction Error**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method measures how well the reduced dimensions can reconstruct the original data. It's particularly useful for autoencoder-based dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def reconstruction_error(original, reconstructed):\n",
    "    return np.mean((original - reconstructed) ** 2)\n",
    "\n",
    "# Assuming we have original_data and reconstructed_data\n",
    "error = reconstruction_error(original_data, reconstructed_data)\n",
    "print(f\"Reconstruction Error: {error}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Downstream Task Performance**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of a machine learning model on the reduced dataset compared to its performance on the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original data: 0.956140350877193\n",
      "Accuracy on reduced data: 0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate on original data\n",
    "model_original = LogisticRegression()\n",
    "model_original.fit(X_train, y_train)\n",
    "accuracy_original = accuracy_score(y_test, model_original.predict(X_test))\n",
    "\n",
    "# Train and evaluate on reduced data\n",
    "pca = PCA(n_components=5)  # Example: reducing to 5 components\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "model_reduced = LogisticRegression()\n",
    "model_reduced.fit(X_train_reduced, y_train)\n",
    "accuracy_reduced = accuracy_score(y_test, model_reduced.predict(X_test_reduced))\n",
    "\n",
    "print(f\"Accuracy on original data: {accuracy_original}\")\n",
    "print(f\"Accuracy on reduced data: {accuracy_reduced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Silhouette Score**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clustering tasks, the silhouette score can be used to evaluate how well-separated the reduced clusters are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.5471360437420352\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "labels = kmeans.fit_predict(X)\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.6467889615132902\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "labels = kmeans.fit_predict(X_reduced)\n",
    "silhouette_avg = silhouette_score(X_reduced, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Selecting the Optimal Number of Dimensions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of selecting the optimal number of dimensions often involves a trade-off between computational efficiency and model performance. Here are some strategies:\n",
    "\n",
    "1. **Elbow Method**: Plot the explained variance ratio against the number of dimensions and look for the \"elbow\" point.\n",
    "\n",
    "2. **Cumulative Explained Variance Threshold**: Choose the number of dimensions that explain a certain percentage (e.g., 95%) of the total variance.\n",
    "\n",
    "3. **Cross-Validation**: Use cross-validation to evaluate model performance with different numbers of dimensions and choose the one that gives the best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "best_score = 0\n",
    "best_n_components = 0\n",
    "\n",
    "for n_components in range(1, X.shape[1] + 1):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    scores = cross_val_score(LogisticRegression(), X_reduced, y, cv=5)\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_n_components = n_components\n",
    "\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "print(f\"Best cross-validation score: {best_score}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Challenges and Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Interpretability**: As dimensions are reduced, it may become harder to interpret what each dimension represents, especially in non-linear methods.\n",
    "\n",
    "2. **Scalability**: Some evaluation methods may become computationally expensive for very large datasets.\n",
    "\n",
    "3. **Domain Knowledge**: In some cases, domain expertise may be necessary to determine if the reduced dimensions still capture the essential aspects of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ **Important Note:** The \"best\" number of dimensions can vary depending on the specific problem, dataset, and downstream task. Always consider the practical implications of your choice in the context of your project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating and selecting the appropriate number of reduced dimensions is a critical step in the dimensionality reduction process. By using a combination of quantitative metrics and domain knowledge, you can ensure that your reduced dataset retains the most important information while achieving the desired level of compression. Remember that this process often requires experimentation and iteration to find the optimal balance for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a crucial technique in machine learning and data science for managing high-dimensional data. Let's recap the key points we've covered:\n",
    "\n",
    "1. **The Curse of Dimensionality**: As dimensions increase, data becomes sparse, distances lose meaning, and computational complexity grows exponentially.\n",
    "\n",
    "2. **Goals of Dimensionality Reduction**:\n",
    "   - Data compression\n",
    "   - Noise reduction\n",
    "   - Feature extraction\n",
    "   - Improved computational efficiency\n",
    "   - Enhanced visualization\n",
    "\n",
    "3. **Types of Dimensionality Reduction**:\n",
    "   - Feature Selection: Choosing a subset of original features\n",
    "   - Feature Extraction: Creating new features by transforming the original feature space\n",
    "\n",
    "4. **Linear Methods**:\n",
    "   - Principal Component Analysis (PCA)\n",
    "   - Linear Discriminant Analysis (LDA)\n",
    "   - Factor Analysis\n",
    "\n",
    "5. **Non-linear Methods**:\n",
    "   - t-SNE\n",
    "   - UMAP\n",
    "   - Kernel PCA\n",
    "   - Autoencoders\n",
    "\n",
    "6. **Feature Selection Methods**:\n",
    "   - Filter methods (e.g., variance threshold)\n",
    "   - Wrapper methods (e.g., recursive feature elimination)\n",
    "   - Embedded methods (e.g., Lasso regression)\n",
    "\n",
    "7. **Evaluation Methods**:\n",
    "   - Explained variance ratio\n",
    "   - Reconstruction error\n",
    "   - Downstream task performance\n",
    "   - Silhouette score for clustering tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** The choice of dimensionality reduction technique depends on your specific dataset, problem, and requirements. Experimentation is often key to finding the best approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember these best practices:\n",
    "\n",
    "1. **Start Simple**: Begin with linear methods like PCA before moving to more complex non-linear techniques.\n",
    "\n",
    "2. **Visualize**: Use dimensionality reduction for data visualization to gain insights into your dataset.\n",
    "\n",
    "3. **Evaluate Carefully**: Always assess the impact of dimensionality reduction on your downstream tasks.\n",
    "\n",
    "4. **Balance Trade-offs**: Consider the trade-off between data compression and information retention.\n",
    "\n",
    "5. **Domain Knowledge**: Incorporate domain expertise when interpreting reduced dimensions and selecting features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ **Important Note:** While dimensionality reduction is powerful, it's not always necessary or beneficial. For some problems, working with the original high-dimensional data might yield better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As datasets continue to grow in size and complexity, dimensionality reduction techniques are likely to evolve. Keep an eye on:\n",
    "\n",
    "- Advancements in deep learning-based dimensionality reduction\n",
    "- Scalable methods for big data\n",
    "- Interpretable dimensionality reduction techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By mastering dimensionality reduction, you'll be better equipped to handle the challenges of high-dimensional data in machine learning and data science projects. Remember, the goal is not just to reduce dimensions, but to do so in a way that enhances your ability to extract meaningful insights from your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
