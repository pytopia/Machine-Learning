{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous lecture on ensemble methods, we learned that ensemble learning combines multiple models to create a more robust and accurate predictor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods generally fall into two categories:\n",
    "- **Parallel ensembles** (like bagging) where base learners are built independently\n",
    "- **Sequential ensembles** (like boosting) where base learners are built sequentially\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, short for **B**ootstrap **Agg**regat**ing**, is a parallel ensemble method introduced by Leo Breiman in 1996. It's designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea of bagging is remarkably intuitive:\n",
    "1. Create multiple versions of the training dataset through bootstrap sampling\n",
    "2. Train a model on each sampled dataset\n",
    "3. Aggregate predictions from all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is particularly effective because it addresses a fundamental challenge in machine learning: the variance-bias tradeoff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Bagging primarily helps reduce variance while keeping bias relatively unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand this mathematically. For any predictor $f(x)$, the expected prediction error can be decomposed as:\n",
    "\n",
    "$Error = Bias^2 + Variance + \\text{Irreducible Error}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging reduces variance through:\n",
    "- Averaging multiple predictions\n",
    "- Using bootstrap samples to create diverse training sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $B$ bagged predictors, if we assume they're identically distributed with variance $\\sigma^2$ and pairwise correlation $\\rho$, the variance of the bagged predictor is:\n",
    "\n",
    "$\\sigma_{bag}^2 = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is most effective when:\n",
    "1. The base model has high variance and low bias\n",
    "2. The dataset is of moderate to large size\n",
    "3. The base models are sensitive to changes in the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common base learners for bagging include:\n",
    "- Decision trees (most common)\n",
    "- Neural networks\n",
    "- Linear regression (in some cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While bagging is powerful, it's important to understand its limitations:\n",
    "- Computational overhead due to multiple models\n",
    "- No improvement in bias\n",
    "- May not be as effective with stable algorithms (like k-nearest neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example of bagging concept in scikit-learn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a bagging classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,    # number of base models\n",
    "    max_samples=0.8,     # size of bootstrap sample\n",
    "    bootstrap=True,      # enable bootstrap sampling\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** When implementing bagging, start with these guidelines:\n",
    "- Use 50-100 base estimators initially\n",
    "- Bootstrap sample size around 63.2% of original dataset (default)\n",
    "- Monitor out-of-bag error to assess performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll dive deeper into bootstrap sampling, which forms the foundation of the bagging methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Sampling\n",
    "\n",
    "Bootstrap sampling is a statistical method that involves randomly sampling a dataset with replacement. Think of it like drawing balls from a bag where you:\n",
    "1. Draw a ball\n",
    "2. Record its value\n",
    "3. Put it back in the bag\n",
    "4. Repeat until you have as many samples as you want\n",
    "\n",
    "üí° **Tip:** The term \"bootstrap\" comes from the phrase \"pulling oneself up by one's bootstraps,\" suggesting the method's ability to use limited data to generate insights about a larger population.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Let's say we have a dataset $D$ with $n$ observations: $D = \\{x_1, x_2, ..., x_n\\}$\n",
    "\n",
    "When we create a bootstrap sample $D^*$, each observation has:\n",
    "- Probability $\\frac{1}{n}$ of being selected each time\n",
    "- Can be selected multiple times\n",
    "- Might not be selected at all\n",
    "\n",
    "The probability of an observation not being selected in a single draw is $(1-\\frac{1}{n})$\n",
    "\n",
    "For a bootstrap sample of size $n$, the probability of an observation never being selected is:\n",
    "\n",
    "$P(\\text{not selected}) = (1-\\frac{1}{n})^n \\approx e^{-1} \\approx 0.368$\n",
    "\n",
    "This means:\n",
    "- Approximately 63.2% of original observations appear in each bootstrap sample\n",
    "- About 36.8% of observations are left out (these form the Out-of-Bag sample)\n",
    "\n",
    "### Properties of Bootstrap Samples\n",
    "\n",
    "1. **Size and Composition**\n",
    "   - Typically same size as original dataset\n",
    "   - Contains duplicates of some observations\n",
    "   - Misses some original observations\n",
    "   - Maintains data distribution properties\n",
    "\n",
    "2. **Statistical Properties**\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   # Example of bootstrap sampling\n",
    "   def create_bootstrap_sample(data, size=None):\n",
    "       if size is None:\n",
    "           size = len(data)\n",
    "       indices = np.random.randint(0, len(data), size=size)\n",
    "       return data[indices]\n",
    "   ```\n",
    "\n",
    "3. **Variance Estimation**\n",
    "   Bootstrap samples help estimate the variance of a statistic $\\theta$ using:\n",
    "\n",
    "   $Var(\\theta) \\approx \\frac{1}{B-1}\\sum_{i=1}^B(\\theta_i - \\bar{\\theta})^2$\n",
    "\n",
    "   where $B$ is the number of bootstrap samples and $\\theta_i$ is the statistic computed on the $i$-th bootstrap sample.\n",
    "\n",
    "### Out-of-Bag (OOB) Samples\n",
    "\n",
    "OOB samples are a crucial concept in bagging, providing:\n",
    "1. **Built-in Validation Set**\n",
    "   - No need for separate cross-validation\n",
    "   - Unbiased performance estimation\n",
    "\n",
    "2. **Error Estimation**\n",
    "   ```python\n",
    "   # Example of OOB score in scikit-learn\n",
    "   from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "   bagging = BaggingClassifier(\n",
    "       oob_score=True,  # Enable OOB scoring\n",
    "       n_estimators=100\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Feature Importance**\n",
    "   - Can be used to estimate variable importance\n",
    "   - Helps in feature selection\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "‚ùóÔ∏è **Important Note:** When implementing bootstrap sampling, consider:\n",
    "\n",
    "1. **Sample Size Selection**\n",
    "   - Standard: Same as original dataset\n",
    "   - Smaller: Reduces computation, might increase diversity\n",
    "   - Larger: More stable but potentially less diverse\n",
    "\n",
    "2. **Number of Bootstrap Samples**\n",
    "   - More samples ‚Üí more stable estimates\n",
    "   - Diminishing returns after certain point\n",
    "   - Trade-off with computational resources\n",
    "\n",
    "```python\n",
    "# Example showing effect of different bootstrap sample sizes\n",
    "def demonstrate_bootstrap_properties(data, n_bootstraps=1000):\n",
    "    original_mean = np.mean(data)\n",
    "    bootstrap_means = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        boot_sample = create_bootstrap_sample(data)\n",
    "        bootstrap_means.append(np.mean(boot_sample))\n",
    "\n",
    "    return {\n",
    "        'original_mean': original_mean,\n",
    "        'bootstrap_mean': np.mean(bootstrap_means),\n",
    "        'bootstrap_std': np.std(bootstrap_means)\n",
    "    }\n",
    "```\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "1. **Statistical Inference**\n",
    "   - Confidence interval estimation\n",
    "   - Hypothesis testing\n",
    "   - Parameter uncertainty estimation\n",
    "\n",
    "2. **Machine Learning**\n",
    "   - Model validation\n",
    "   - Ensemble creation\n",
    "   - Uncertainty quantification\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Performance metric estimation\n",
    "   - Model stability assessment\n",
    "   - Feature importance calculation\n",
    "\n",
    "### Bootstrap Sampling Variations\n",
    "\n",
    "1. **Balanced Bootstrap**\n",
    "   - Ensures each observation appears exactly same number of times\n",
    "   - Useful for imbalanced datasets\n",
    "\n",
    "2. **Weighted Bootstrap**\n",
    "   - Observations have different selection probabilities\n",
    "   - Useful for dealing with sample bias\n",
    "\n",
    "3. **Block Bootstrap**\n",
    "   - For time series or dependent data\n",
    "   - Samples blocks of consecutive observations\n",
    "\n",
    "üí° **Tip:** Choose the appropriate bootstrap variation based on your data structure and problem requirements.\n",
    "\n",
    "In the next section, we'll explore how these bootstrap samples are used in practice to create bagged models and improve prediction accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
