{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous lecture on ensemble methods, we learned that combining multiple models often performs better than individual models. This is similar to how a group of experts might make better decisions than a single expert. Today, we'll dive deep into one of the most fundamental ensemble techniques: Bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, short for **B**ootstrap **Agg**regat**ing**, is an ensemble learning technique introduced by Leo Breiman in 1996.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Insight:** Bagging creates multiple versions of a model trained on different subsets of the data, then combines their predictions to create a more robust model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand this with a simple analogy:\n",
    "\n",
    "Imagine you're trying to estimate the average height of people in a city:\n",
    "- Instead of measuring everyone (impossible!), you take multiple random samples\n",
    "- Each sample gives you an estimate\n",
    "- By averaging these estimates, you get a more reliable result\n",
    "- The more samples you take, the more stable your estimate becomes\n",
    "\n",
    "This is exactly how bagging works in machine learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Bagging-classifier.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging helps reduce a critical problem in machine learning: **variance**. \n",
    "\n",
    "Let's break this down:\n",
    "1. Individual models (especially complex ones like deep trees) are often sensitive to their training data\n",
    "2. Small changes in the training data can lead to large changes in the model\n",
    "3. Bagging reduces this sensitivity by:\n",
    "   - Training each model on a different subset of data\n",
    "   - Averaging their predictions to get a more stable result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Bagging is particularly effective for high-variance algorithms (like decision trees) but might not help much with high-bias algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple mathematical example to understand why averaging helps:\n",
    "\n",
    "Assume we have a true value $y$ and $n$ different predictions $\\hat{y}_i$ where each prediction has some random error $\\epsilon_i$:\n",
    "\n",
    "$\\hat{y}_i = y + \\epsilon_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we average $n$ predictions:\n",
    "\n",
    "$\\bar{\\hat{y}} = \\frac{1}{n}\\sum_{i=1}^n \\hat{y}_i = y + \\frac{1}{n}\\sum_{i=1}^n \\epsilon_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of the average prediction is:\n",
    "\n",
    "$Var(\\bar{\\hat{y}}) = \\frac{Var(\\epsilon)}{n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bias-variance.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bagging-variance.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that averaging reduces variance by a factor of $n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll dive deeper into how these different datasets (D1, D2, D3) are created through a process called bootstrap sampling. This is the \"Bootstrap\" part of \"Bootstrap Aggregating\" that gives bagging its name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Preview:** Bootstrap sampling is a statistical technique that allows us to create multiple training sets from a single dataset, which is crucial for making bagging work in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you like me to proceed with the content for the next section on Bootstrap Sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Bootstrap Sampling](#toc1_)    \n",
    "  - [The Bootstrap Process](#toc1_1_)    \n",
    "  - [Mathematical Properties](#toc1_2_)    \n",
    "  - [Out-of-Bag (OOB) Samples](#toc1_3_)    \n",
    "- [Bagging in Practice](#toc2_)    \n",
    "  - [Core Components of Bagging](#toc2_1_)    \n",
    "  - [Parallel Processing Advantage](#toc2_2_)    \n",
    "  - [Practical Considerations](#toc2_3_)    \n",
    "  - [Common Use Cases and Limitations](#toc2_4_)    \n",
    "  - [Performance Monitoring](#toc2_5_)    \n",
    "- [Practical Implementation](#toc3_)    \n",
    "  - [Setting Up the Environment](#toc3_1_)    \n",
    "  - [A Complete Bagging Example](#toc3_2_)    \n",
    "  - [Model Configuration and Training](#toc3_3_)    \n",
    "  - [Performance Analysis](#toc3_4_)    \n",
    "  - [Best Practices and Common Pitfalls](#toc3_5_)    \n",
    "  - [Putting It All Together](#toc3_6_)    \n",
    "- [Summary](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Bootstrap Sampling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on our previous section where we introduced bagging, let's dive into how we actually create those different training sets. This is where bootstrap sampling comes in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap sampling is a statistical method that involves:\n",
    "- Sampling from a dataset **with replacement**\n",
    "- Creating a sample of the same size as the original dataset\n",
    "- Allowing data points to be selected multiple times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Insight:** Think of bootstrap sampling like drawing balls from a bag where you put each ball back after drawing it. This means some balls might be drawn multiple times while others might not be drawn at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[The Bootstrap Process](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand this with a simple example:\n",
    "\n",
    "Original dataset with 5 points: [A, B, C, D, E]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible bootstrap samples:\n",
    "```python\n",
    "Sample 1: [A, B, B, D, E]  # C is not selected, B appears twice\n",
    "Sample 2: [B, C, C, D, E]  # A is not selected, C appears twice\n",
    "Sample 3: [A, A, C, D, E]  # B is not selected, A appears twice\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è**Important Note:** Each bootstrap sample:\n",
    "- Has the same size as the original dataset\n",
    "- Contains some duplicates\n",
    "- Misses some original data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Mathematical Properties](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a data point being selected in one draw is $\\frac{1}{n}$ where n is the dataset size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a data point NOT being selected after n draws is:\n",
    "$(1 - \\frac{1}{n})^n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As n ‚Üí ‚àû, this approaches $\\frac{1}{e} ‚âà 0.368$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means:\n",
    "- Approximately 63.2% of unique original data points appear in each bootstrap sample\n",
    "- About 36.8% of data points are left out (these form the Out-of-Bag sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Out-of-Bag (OOB) Samples](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points not selected in a bootstrap sample form what we call the Out-of-Bag (OOB) sample. These samples are incredibly useful because:\n",
    "1. They provide an unbiased estimate of model performance\n",
    "2. They can be used for validation without needing a separate validation set\n",
    "3. They help in detecting overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/oob-samples.avif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple Python implementation of bootstrap sampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bootstrap_sample(data, size=None):\n",
    "    if size is None:\n",
    "        size = len(data)\n",
    "    indices = np.random.randint(0, len(data), size=size)\n",
    "    return data[indices]\n",
    "\n",
    "# Example usage\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "bootstrap = bootstrap_sample(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, 1, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how bootstrap sampling connects to bagging:\n",
    "1. Create multiple bootstrap samples from training data\n",
    "2. Train a separate model on each bootstrap sample\n",
    "3. Combine predictions from all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to our next section, where we'll explore how to implement bagging in practice and see how these concepts come together in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll learn how to implement bagging efficiently and explore its practical advantages and limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Bagging in Practice](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand bootstrap sampling and the principles of bagging, let's explore how to implement bagging effectively and what makes it work in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Core Components of Bagging](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bagging implementation consists of three main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Base Estimator Selection**\n",
    "   - Typically high-variance models (e.g., deep decision trees)\n",
    "   - Must be sensitive to training data changes\n",
    "   - Common choices:\n",
    "     - Decision trees (most common)\n",
    "     - Neural networks\n",
    "     - K-nearest neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Ensemble Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Pseudocode for bagging implementation\n",
    "class BaggingEnsemble:\n",
    "    def __init__(self, base_estimator, n_estimators=10):\n",
    "        self.estimators = []\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            # Create bootstrap sample\n",
    "            X_boot, y_boot = bootstrap_sample(X, y)\n",
    "            # Train model on bootstrap sample\n",
    "            model = clone(base_estimator)\n",
    "            model.fit(X_boot, y_boot)\n",
    "            self.estimators.append(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Prediction Aggregation**\n",
    "   - For regression: Average predictions\n",
    "   - For classification: Majority voting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Insight:** The magic of bagging comes from the diversity of the base models combined with the wisdom of the crowd effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Parallel Processing Advantage](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest practical advantages of bagging is that it's naturally parallel:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def parallel_fit(X, y, base_estimator):\n",
    "    # Create bootstrap sample and fit model\n",
    "    X_boot, y_boot = bootstrap_sample(X, y)\n",
    "    model = clone(base_estimator)\n",
    "    model.fit(X_boot, y_boot)\n",
    "    return model\n",
    "\n",
    "# Parallel implementation\n",
    "estimators = Parallel(n_jobs=-1)(\n",
    "    delayed(parallel_fit)(X, y, base_estimator)\n",
    "    for _ in range(n_estimators)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è**Important Note:** Parallel processing can significantly reduce training time, especially with many estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Number of Estimators**\n",
    "   - More estimators generally improve performance\n",
    "   - Diminishing returns after certain point\n",
    "   - Trade-off between performance and computational cost\n",
    "   \n",
    "   ```python\n",
    "   # Learning curve for different numbers of estimators\n",
    "   errors = []\n",
    "   for n in [1, 5, 10, 50, 100]:\n",
    "       bag = BaggingRegressor(n_estimators=n)\n",
    "       error = cross_val_score(bag, X, y).mean()\n",
    "       errors.append(error)\n",
    "   ```\n",
    "\n",
    "2. **Sample Size**\n",
    "   - Default: same size as original dataset\n",
    "   - Can be adjusted for specific needs:\n",
    "     - Smaller: faster training, more diversity\n",
    "     - Larger: more stable individual models\n",
    "\n",
    "3. **Base Estimator Parameters**\n",
    "   - Usually use default parameters\n",
    "   - Complex base models ‚Üí better diversity\n",
    "   - Simple base models ‚Üí faster training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Common Use Cases and Limitations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding where and when to use bagging is crucial for its effective application. Like any machine learning technique, bagging has specific scenarios where it shines and others where alternative approaches might be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the scenarios where bagging proves particularly valuable:\n",
    "\n",
    "When we encounter any of these situations, bagging often emerges as a powerful solution:\n",
    "- High-variance models that need stabilization (like deep decision trees)\n",
    "- Datasets with significant noise where individual models might overfit\n",
    "- Scenarios where computational resources allow for parallel processing\n",
    "- Applications where prediction accuracy takes precedence over model interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this, consider a credit card fraud detection system. The dataset is typically noisy and imbalanced, and the cost of false negatives is high. In this case, bagging can help create a robust model that's less sensitive to individual transaction patterns and more reliable in detecting fraudulent activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, bagging isn't a silver bullet. Let's examine its limitations to understand when we might need to consider alternative approaches:\n",
    "\n",
    "1. Model Interpretability\n",
    "   - The ensemble nature of bagging makes it harder to explain individual predictions\n",
    "   - While we can still extract feature importance, the reasoning behind specific predictions becomes more opaque\n",
    "   - This can be problematic in domains like healthcare or finance where decision transparency is crucial\n",
    "\n",
    "2. Memory Requirements\n",
    "   - Each base model in the ensemble needs to be stored in memory\n",
    "   - For large datasets or complex base models, this can lead to significant memory overhead\n",
    "   - This becomes especially challenging in resource-constrained environments\n",
    "\n",
    "3. Prediction Time\n",
    "   - Every prediction requires aggregating results from all base models\n",
    "   - This can lead to slower inference times compared to single models\n",
    "   - In real-time applications, this overhead might be problematic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding these trade-offs helps us make informed decisions about when to use bagging. For instance, in a real-time recommendation system where quick predictions are crucial, we might need to carefully balance the number of base models against the required prediction speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Performance Monitoring](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor these metrics to ensure effective bagging:\n",
    "\n",
    "1. **Individual Model Performance**\n",
    "   ```python\n",
    "   # Check performance of individual models\n",
    "   for model in bagging.estimators_:\n",
    "       score = model.score(X_val, y_val)\n",
    "       print(f\"Model score: {score:.3f}\")\n",
    "   ```\n",
    "\n",
    "2. **Ensemble Diversity**\n",
    "   - Track prediction correlations\n",
    "   - Monitor unique predictions\n",
    "\n",
    "3. **OOB Score**\n",
    "   ```python\n",
    "   bagging = BaggingRegressor(oob_score=True)\n",
    "   bagging.fit(X, y)\n",
    "   print(f\"OOB Score: {bagging.oob_score_}\")\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to implement bagging effectively, we'll move on to practical implementation using scikit-learn, where we'll see these concepts in action with real code and datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll work with scikit-learn's implementation of bagging and explore a complete example from data preparation to model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Practical Implementation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout our previous sections, we've built a strong theoretical foundation of bagging and its components. Now, let's put this knowledge into practice by implementing a bagging ensemble using scikit-learn. We'll walk through a complete example that demonstrates the concepts we've learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Setting Up the Environment](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary libraries and prepare our workspace. We'll use a real-world dataset to make our example more concrete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[A Complete Bagging Example](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a practical example using the California Housing dataset, which is a perfect case for demonstrating bagging due to its complexity and real-world nature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "# Load and prepare data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Insight:** The choice of base estimator and number of estimators significantly impacts the model's performance. Let's explore this through experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Model Configuration and Training](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement bagging with different configurations to understand their impact:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=10),\n",
       "                 max_features=0.8, max_samples=0.8, n_estimators=100, n_jobs=-1,\n",
       "                 oob_score=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;BaggingRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.BaggingRegressor.html\">?<span>Documentation for BaggingRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=10),\n",
       "                 max_features=0.8, max_samples=0.8, n_estimators=100, n_jobs=-1,\n",
       "                 oob_score=True, random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: DecisionTreeRegressor</label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeRegressor(max_depth=10)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;DecisionTreeRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">?<span>Documentation for DecisionTreeRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeRegressor(max_depth=10)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=10),\n",
       "                 max_features=0.8, max_samples=0.8, n_estimators=100, n_jobs=-1,\n",
       "                 oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic bagging model\n",
    "base_tree = DecisionTreeRegressor(max_depth=10)\n",
    "bagging_model = BaggingRegressor(\n",
    "    estimator=base_tree,\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Parallel processing\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "bagging_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Performance Analysis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key advantages of bagging is the ability to analyze performance using out-of-bag estimates. Let's explore different evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8020028189326132"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_model.oob_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the training and testing set to evaluate the performance of the bagging model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.1691\n",
      "Test MSE: 0.2625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.16914477707404912, 0.262538416417678)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to evaluate and visualize performance\n",
    "def evaluate_bagging_performance(model, X_train, X_test, y_train, y_test):\n",
    "    # Training and test predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate errors\n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "    print(f\"Training MSE: {train_mse:.4f}\")\n",
    "    print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "    return train_mse, test_mse\n",
    "\n",
    "evaluate_bagging_performance(bagging_model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging involves several important hyperparameters that need careful tuning. Let's explore how to optimize them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to perform hyperparameter search\n",
    "def tune_bagging_params(X, y, param_grid):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    bagging = BaggingRegressor()\n",
    "    grid_search = GridSearchCV(\n",
    "        bagging, param_grid,\n",
    "        cv=5, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "# Example parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [3, 5, 10],\n",
    "    # 'max_samples': [0.7, 0.8, 0.9],\n",
    "    # 'max_features': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "tune_bagging_params(X_train, y_train, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_'></a>[Best Practices and Common Pitfalls](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing bagging in practice, keep these important considerations in mind:\n",
    "\n",
    "1. **Data Preprocessing**\n",
    "   - Always scale your features before bagging\n",
    "   - Handle missing values appropriately\n",
    "   - Consider feature engineering to improve base model performance\n",
    "\n",
    "2. **Model Selection**\n",
    "   - Start with simple base models and gradually increase complexity\n",
    "   - Monitor training time vs. performance improvements\n",
    "   - Use cross-validation to ensure robust performance estimates\n",
    "\n",
    "3. **Resource Management**\n",
    "   - Be mindful of memory usage with large ensembles\n",
    "   - Utilize parallel processing when possible\n",
    "   - Consider model compression techniques for deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è**Important Note:** Always validate your model's performance on a separate test set to ensure generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_6_'></a>[Putting It All Together](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a complete workflow combining all these elements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete implementation example\n",
    "def build_optimal_bagging_model(X, y):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Find optimal parameters\n",
    "    best_params = tune_bagging_params(X_train, y_train, param_grid)\n",
    "\n",
    "    # Create and train model with best parameters\n",
    "    optimal_model = BaggingRegressor(**best_params)\n",
    "    optimal_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate performance\n",
    "    evaluate_bagging_performance(\n",
    "        optimal_model, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    return optimal_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation brings together all the concepts we've discussed and provides a solid foundation for using bagging in practice. Remember that the key to successful implementation lies in understanding your specific use case and adjusting these components accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude our exploration of bagging, we've seen how to implement it effectively in practice. In our next lecture, we'll build upon these concepts as we dive into Random Forests, which extend the bagging principle with additional randomization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lecture on bagging and bootstrap sampling, we've explored fundamental concepts that form the backbone of modern ensemble learning. Let's connect the key ideas we've covered and understand how they fit together in the bigger picture of machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with the core concept of bagging, understanding how it leverages the power of multiple models to create more robust predictions. The key insight here is that combining multiple \"noisy\" models can produce a more stable and accurate result, much like how averaging multiple expert opinions often leads to better decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap sampling, the mechanism that makes bagging possible, showed us how we can create diverse training sets from a single dataset. Remember our analogy of drawing balls from a bag with replacement - this simple yet powerful technique allows us to:\n",
    "- Generate multiple training sets\n",
    "- Ensure model diversity\n",
    "- Create out-of-bag samples for validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we moved to practical implementation, we learned that bagging isn't just theoretical - it's a powerful tool with real-world applications. We saw how:\n",
    "- Parallel processing makes bagging computationally efficient\n",
    "- The number of estimators affects model performance\n",
    "- Out-of-bag scoring provides built-in validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key Insight:** The true power of bagging lies in its ability to reduce variance while maintaining the predictive strength of complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This understanding of bagging and bootstrap sampling sets the foundation for our next lecture on Random Forests, which builds upon these concepts by adding feature randomization to create even more powerful ensemble models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è**Important Note:** As you move forward, remember that bagging is just one tool in your machine learning toolkit. Understanding when to use it - and when not to - is crucial for successful application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solidify your understanding, consider:\n",
    "1. How does bootstrap sampling contribute to model diversity?\n",
    "2. Why does averaging predictions reduce variance?\n",
    "3. In what scenarios would you choose bagging over a single complex model?\n",
    "4. How can you use out-of-bag samples to validate your model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These concepts will continue to be relevant as we explore more advanced ensemble methods in upcoming lectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
