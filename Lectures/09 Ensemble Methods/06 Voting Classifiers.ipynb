{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling is a powerful concept in machine learning that combines multiple models (often referred to as *weak learners* or *base estimators*) to produce a final prediction. **Voting** is one of the simplest yet effective ensemble methods where each model ‚Äúvotes‚Äù for a particular outcome, and the aggregated vote forms the final prediction. This approach can often outperform individual models by reducing variance and leveraging the complementary strengths of different learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/voting.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting can be intuitive when you consider how committees make decisions. By combining *a variety of opinions*, the group‚Äôs decision is often more reliable than any single individual‚Äôs perspective. The same principle applies to machine learning:\n",
    "\n",
    "- **Improved Accuracy**: A mix of models helps reduce overfitting and stabilizes predictions.\n",
    "- **Model Diversity**: Different algorithms capture different patterns, and their combined synergy can boost performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** Even simple models like *Logistic Regression* and *Decision Trees* can form a strong ensemble if they contribute *unique error patterns*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of voting in ensemble methods can be understood in two primary forms: **hard voting** and **soft voting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Hard Voting**: Each classifier outputs a predicted label, and the final label is chosen by majority vote. Mathematically, if we have classifiers $ C_1, C_2, \\ldots, C_k $ producing labels $ \\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_k $, then the final label $ \\hat{y} $ is:\n",
    "  $$\n",
    "  \\hat{y} = \\operatorname{mode} \\{\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_k\\}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Soft Voting**: Each classifier outputs a probability distribution over possible labels, and the final label is decided by averaging these probabilities. Specifically, if $ p_{i,j} $ is the predicted probability of the $ i$-th classifier for class $ j $, then the combined probability for class $ j $ is:\n",
    "  $$\n",
    "  \\bar{p}_j = \\frac{1}{k}\\sum_{i=1}^{k} p_{i,j}\n",
    "  $$\n",
    "  and the predicted class is $ \\hat{y} = \\arg \\max_j \\bar{p}_j $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Soft voting often achieves better performance than hard voting *if* probabilities are well-calibrated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting is straightforward to implement and can significantly improve model performance under the right conditions. However, **not all problems benefit equally.** Here are some key points:\n",
    "\n",
    "- **Advantages**:\n",
    "  ‚Ä¢ Simple to implement using libraries like *scikit-learn*.\n",
    "  ‚Ä¢ Naturally reduces variance when combining diverse models.\n",
    "  ‚Ä¢ Often boosts performance even if individual models are relatively weak.\n",
    "\n",
    "- **Challenges**:\n",
    "  ‚Ä¢ Requires multiple models, increasing computational cost.\n",
    "  ‚Ä¢ Performance gains depend on the diversity and quality of base estimators.\n",
    "  ‚Ä¢ Hard voting can ignore *confidence information* if models differ in predictive certainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a quick illustrative snippet showing how to set up a basic voting ensemble in Python using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create base estimators\n",
    "log_clf = LogisticRegression()\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "svm_clf = SVC(probability=True)  # probability=True for soft voting\n",
    "\n",
    "# Combine them into a voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', log_clf),\n",
    "        ('dt', tree_clf),\n",
    "        ('svm', svm_clf)\n",
    "    ],\n",
    "    voting='soft'  # change to 'hard' for majority voting\n",
    ")\n",
    "\n",
    "# voting_clf can now be fit and used for predictions like any other scikit-learn model\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding these foundational ideas of *why* we use voting, *how* it works, and *what* challenges it comes with, you will be well-prepared to explore more advanced ensemble techniques later in this lecture series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Types of Voting Techniques](#toc1_)    \n",
    "  - [Hard Voting](#toc1_1_)    \n",
    "  - [Soft Voting](#toc1_2_)    \n",
    "  - [Weighted Voting](#toc1_3_)    \n",
    "  - [Choosing the Right Voting Approach](#toc1_4_)    \n",
    "- [Practical Implementation and Examples](#toc2_)    \n",
    "  - [Implementation Steps and Workflow](#toc2_1_)    \n",
    "  - [Common Libraries and Functions](#toc2_2_)    \n",
    "  - [Basic Voting Example in Python](#toc2_3_)    \n",
    "  - [Hyperparameter Tuning for Voting](#toc2_4_)    \n",
    "- [Summary](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Types of Voting Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, **voting** generally refers to how multiple base estimators (classifiers or regressors) merge their individual predictions into one final output. While the concept is straightforward, there are practical variations that can have a profound impact on performance. In this section, we‚Äôll explore the most common approaches: *Hard Voting*, *Soft Voting*, and *Weighted Voting*, and then discuss **Choosing the Right Voting Approach** based on your problem requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Hard Voting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard voting is the simplest form of voting. Each classifier predicts a class label, and the **final decision** is made by a majority vote.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you have three classifiers $(C_1, C_2, C_3)$ outputting the labels $(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3)$, the final prediction $\\hat{y}$ becomes:\n",
    "$$\n",
    "\\hat{y} = \\operatorname{mode}\\{\\hat{y}_1, \\hat{y}_2, \\hat{y}_3\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Insight:**  \n",
    "‚Ä¢ Hard voting is easy to explain and implement, but since it discards the confidence level of each prediction, it may not always yield the best results.  \n",
    "‚Ä¢ In simple tasks (or when you‚Äôre just getting started with ensemble methods), hard voting can be a quick solution that already improves upon individual models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a brief pseudo-code example of a simple majority voting procedure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "predictions = [model.predict(X_test) for model in models]  # list of label arrays\n",
    "final_predictions = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    # Gather the labels for each sample\n",
    "    votes = [pred[i] for pred in predictions]\n",
    "    # Find the mode (most common label)\n",
    "    final_label = max(set(votes), key=votes.count)\n",
    "    final_predictions.append(final_label)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Soft Voting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to hard voting, **soft voting** takes into account the predicted probabilities from each classifier. Instead of seeing only the final labels, we look at how confident each model is about its predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Each model outputs probabilities for each class: $p_{i,j}$ for the $i$-th model and class $j$.  \n",
    "2. We then average these probabilities to obtain $\\bar{p}_j$:  \n",
    "   $$\n",
    "   \\bar{p}_j = \\frac{1}{k}\\sum_{i=1}^{k} p_{i,j}\n",
    "   $$\n",
    "3. The final class label is the one with the highest average probability:\n",
    "   $$\n",
    "   \\hat{y} = \\arg \\max_j \\bar{p}_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Insight:**  \n",
    "‚Ä¢ Soft voting usually outperforms hard voting when probabilities are well-calibrated.  \n",
    "‚Ä¢ Implementing it in libraries like scikit-learn simply requires enabling `voting='soft'` and ensuring each model can produce probability estimates (e.g., using `SVC(probability=True)`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** If your models are **not** well-calibrated, they might output misleading probabilities, rendering soft voting less effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Weighted Voting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted voting is an extension of **soft voting**, though it can also be applied in a hard-voting scenario. Instead of giving each model an equal say, you assign weights to each model's prediction to reflect its relative importance or historical performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose model $ i $ has weight $ w_i $. In a soft-voting context, the combined probability becomes:\n",
    "$$\n",
    "\\bar{p}_j = \\frac{\\sum_{i=1}^{k} w_i \\cdot p_{i,j}}{\\sum_{i=1}^{k} w_i}.\n",
    "$$\n",
    "The final class is still:\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_j \\bar{p}_j.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Insight:**  \n",
    "‚Ä¢ Assigning weights can be based on cross-validation scores, domain expertise, or model architecture (e.g., boosting kernels).  \n",
    "‚Ä¢ Properly calibrated weights can lead to a noticeable performance improvement‚Äî*but poor weight selection can degrade results*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a short code snippet illustrating how to specify weights in a scikit-learn `VotingClassifier`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf_weighted = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', log_clf),\n",
    "        ('dt', tree_clf),\n",
    "        ('svm', svm_clf)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1.5, 1, 2]  # custom weights for each model\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Choosing the Right Voting Approach](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting between **hard voting**, **soft voting**, or **weighted voting** often depends on factors such as data size, model performance, model diversity, and the **confidence calibration** of each base estimator. If your models produce trustworthy probability estimates, soft voting or weighted voting may be ideal. On the other hand, if calibration is challenging or your models are purely outputting class labels without probabilities, hard voting is the simpler and more direct choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** Start with a simple **soft voting** approach. Then, experiment with weighting strategies if you notice that some of your models consistently perform better than others, or if your domain knowledge suggests certain models should have more influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Practical Implementation and Examples](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and deploying a **voting ensemble** in a real-world scenario involves careful planning, implementation, verification, and optimization. In this section, we will walk through a typical workflow for creating a voting-based ensemble, highlight commonly used libraries, provide hands-on examples, and discuss techniques to tune hyperparameters for best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Implementation Steps and Workflow](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An organized workflow ensures that your voting ensemble is built on strong foundations:\n",
    "\n",
    "1. **Data Preparation**  \n",
    "   Gather clean, representative data. Handle missing values, outliers, and perform feature engineering as necessary.  \n",
    "2. **Model Selection**  \n",
    "   Choose diverse base estimators that can balance each other‚Äôs weaknesses. For classification, you might pick a *Decision Tree*, a *Logistic Regression*, and an *SVM*.  \n",
    "3. **Voting Mechanism**  \n",
    "   Decide on either *hard voting*, *soft voting*, or *weighted voting*. This choice often depends on whether your base models can output *probability estimates* and how confident you are in their calibration.  \n",
    "4. **Implementation & Training**  \n",
    "   Use libraries like *scikit-learn* to implement a `VotingClassifier` or `VotingRegressor`. Train the voting ensemble on your dataset.  \n",
    "5. **Evaluation & Validation**  \n",
    "   Evaluate model performance using metrics like *accuracy*, *F1 score*, or *ROC AUC* for classification, and compare with standalone models. Apply cross-validation to ensure the ensemble‚Äôs robustness.  \n",
    "6. **Hyperparameter Tuning**  \n",
    "   Employ methods like *Grid Search* or *Randomized Search* to find optimal settings (e.g., weights for weighted voting, or hyperparameters of the base estimators).  \n",
    "7. **Deployment & Monitoring**  \n",
    "   Once satisfied with the performance, move the ensemble model into production and continually monitor for *data drift* or performance degradation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Common Libraries and Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python‚Äôs machine learning ecosystem provides a **rich set of tools** to facilitate voting ensembles. The most popular one for quick implementation is **scikit-learn**:\n",
    "\n",
    "- **VotingClassifier** and **VotingRegressor**: Offered as part of `sklearn.ensemble`.  \n",
    "- **Base Estimators**: Such as `LogisticRegression` (from `sklearn.linear_model`), `DecisionTreeClassifier` (from `sklearn.tree`), or `SVC` (from `sklearn.svm` for classification). For regression, you might use `LinearRegression`, `Ridge`, `SVR`, and so on.  \n",
    "- **Metrics**: Accuracy (`accuracy_score`), precision (`precision_score`), recall (`recall_score`), F1 (`f1_score`), and regression metrics (e.g., RMSE, MAE) help gauge the ensemble‚Äôs performance.  \n",
    "- **Model Selection Tools**: `GridSearchCV` and `RandomizedSearchCV` can optimize parameters of base estimators and the voting mechanism (like the `weights` parameter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** Before deciding on your final ensemble, test each estimator independently to verify its strengths and limitations. This helps in determining effective weighting and voting strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Basic Voting Example in Python](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a hands-on feel for voting ensembles, let‚Äôs build a simple classification system using three different models on the *Iris* dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and split data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define individual models\n",
    "log_clf = LogisticRegression(max_iter=1000)  # logistic regression\n",
    "svc_clf = SVC(probability=True)             # SVM for soft voting\n",
    "dt_clf  = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Construct a VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svc_clf), ('dt', dt_clf)],\n",
    "    voting='soft'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
       "                             (&#x27;svc&#x27;, SVC(probability=True)),\n",
       "                             (&#x27;dt&#x27;, DecisionTreeClassifier())],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;VotingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
       "                             (&#x27;svc&#x27;, SVC(probability=True)),\n",
       "                             (&#x27;dt&#x27;, DecisionTreeClassifier())],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(probability=True)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
       "                             ('svc', SVC(probability=True)),\n",
       "                             ('dt', DecisionTreeClassifier())],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Train the ensemble\n",
    "voting_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Ensemble Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Voting Ensemble Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 1.0\n",
      "SVC Accuracy: 1.0\n",
      "DecisionTreeClassifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's compare with individual models\n",
    "for clf in (log_clf, svc_clf, dt_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_indiv = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, \"Accuracy:\", accuracy_score(y_test, y_pred_indiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet:\n",
    "\n",
    "‚Ä¢ We used *soft voting* by enabling `probability=True` in **SVC**.  \n",
    "‚Ä¢ The **VotingClassifier** automatically combines predictions of the three base models.  \n",
    "‚Ä¢ We can compare the ensemble‚Äôs accuracy with each individual model to observe the benefits of voting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Hyperparameter Tuning for Voting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning** is critical to maximize your ensemble‚Äôs potential. You can tune both the base estimators‚Äô hyperparameters and the ensemble‚Äôs configuration (e.g., weights if you‚Äôre using weighted voting). Scikit-learn‚Äôs `GridSearchCV` or `RandomizedSearchCV` can handle both levels of optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical procedure involves:\n",
    "\n",
    "1. Defining a parameter grid for each base estimator (e.g., `max_depth` for decision trees, `C` for Logistic Regression or SVM).  \n",
    "2. Including the ensemble-level parameters like `voting` (hard or soft) or `weights` for each model in the search space.  \n",
    "3. Using cross-validation to systematically evaluate each parameter combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simplified example of how one might encapsulate the above with `GridSearchCV`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'dt__max_depth': 5, 'lr__C': 10, 'svc__C': 1, 'weights': (1, 2, 1)}\n",
      "Best score: 0.9619047619047618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'lr__C': [0.1, 1, 10],\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'dt__max_depth': [None, 3, 5],\n",
    "    'weights': [(1,1,1), (1,2,1), (2,1,2)]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=voting_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Be mindful of computational costs, as searching over multiple parameters in multiple models can grow the search space rapidly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core concepts and examples, you should now feel comfortable implementing and experimenting with voting ensembles. Next, we‚Äôll explore real-world applications and case studies to see how voting can shine in more complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting techniques provide a straightforward yet powerful way to enhance predictive performance by combining multiple models. Whether you use simple **hard voting** for quick majority decisions or more nuanced **soft voting** and **weighted voting** to incorporate confidence levels and model contributions, the ensemble can often outperform individual estimators. Here are some key takeaways from this lecture:\n",
    "\n",
    "- **Versatility**: Voting can be applied to classification and regression tasks alike, thanks to libraries like scikit-learn‚Äôs `VotingClassifier` and `VotingRegressor`.  \n",
    "- **Simple but Effective**: Even a handful of base estimators (e.g., *Logistic Regression*, *Decision Tree*, *SVM*) can show remarkable improvements when combined.  \n",
    "- **Importance of Calibration**: Models that output well-calibrated probabilities can significantly benefit from **soft voting**.  \n",
    "- **Weighting for Fine-Tuning**: Weighted voting allows you to give certain models a stronger say, reflecting their relative performance or domain importance.  \n",
    "- **Hyperparameter Tuning**: Optimize both individual models and ensemble-level parameters (like weights) to get the most out of your voting approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** While voting techniques are often overshadowed by more advanced ensemble methods like **Bagging** or **Boosting**, they provide a simple and effective starting point to increase predictive performance without an exhaustive amount of feature engineering or model tinkering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some recommended resources:\n",
    "- [Ensemble Methods in scikit-learn: Official Documentation](https://scikit-learn.org/stable/modules/ensemble.html)  \n",
    "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur√©lien G√©ron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)  \n",
    "- [Medium / Towards Data Science articles on Ensemble Methods](https://towardsdatascience.com/search?q=ensemble%20methods)  \n",
    "- [Kaggle for real-world examples of Ensemble Methods in competitions](https://www.kaggle.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With voting ensembles under your belt, you are well-prepared to explore more sophisticated methods like **Bagging**, **Random Forests**, **Boosting**, and **Stacking**‚Äîall of which expand on the idea of leveraging multiple models to generate superior predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
