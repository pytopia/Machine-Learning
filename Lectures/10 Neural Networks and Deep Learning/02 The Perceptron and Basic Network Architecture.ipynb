{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron and Basic Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron marks a crucial milestone in the history of **artificial neural networks**. It served as one of the earliest computational models inspired by neurons in the human brain. Understanding its origins helps us appreciate how neural networks evolved and why they continue to shape modern **machine learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of the perceptron was introduced by **Frank Rosenblatt** in 1957 while working on computational models of learning at the Cornell Aeronautical Laboratory. Rosenblatt was fascinated by how the brain could process and interpret visual data, and he strove to replicate these processes using mathematics and simple linear functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its inception, the perceptron represented a significant departure from traditional computing methods:  \n",
    "- Traditional computing used *rigidly defined rules or algorithms*.  \n",
    "- The perceptron attempted to *learn these rules* from data and adjust itself accordingly.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** The perceptron showcased that machines could be taught rather than strictly programmed. This laid the foundation for the idea of *learning from data*, the central tenet of modern machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest form, a perceptron can be seen as a **binary classifier** that maps input features to one of two classes (e.g., 0 or 1). Key elements include:\n",
    "\n",
    "- **Inputs**: A set of features or variables $x_1, x_2, \\ldots, x_n$.  \n",
    "- **Weights**: Numeric coefficients $w_1, w_2, \\ldots, w_n$ that represent the importance of each input.  \n",
    "- **Bias**: A constant term $b$ that shifts the decision boundary independently of the input values.  \n",
    "- **Linear Combination**: The perceptron computes a weighted sum of inputs $z = w \\cdot x + b$.  \n",
    "- **Activation Function**: A function that converts the weighted sum $z$ into an output. For a basic perceptron, the output $y$ is given by:\n",
    "\n",
    "  $$\n",
    "  y =\n",
    "  \\begin{cases}\n",
    "  1 & \\text{if } w \\cdot x + b \\ge 0 \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:** Think of the **activation function** as a *gate* that decides whether the neuron should \"fire\" (output 1) or not (output 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is more than a historical curiosity; it established many **core principles** still relevant today:\n",
    "\n",
    "1. **Learning from Data**: By adjusting weights and bias in response to errors, the perceptron introduced the notion of iteratively improving performance.  \n",
    "2. **Linear Separability**: It highlights the concept of drawing a straight line (or hyperplane in higher dimensions) to separate data into distinct groups.  \n",
    "3. **Foundation for Larger Networks**: While a single-layer perceptron has limitations (it cannot solve non-linearly separable problems), it paved the way for *multi-layered architectures* where more complex decision boundaries are possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, the perceptron proved that machines could *learn from examples*, making it a critical stepping stone to the field of **deep learning**. It also sparked curiosity and debates in the scientific community, ultimately guiding researchers toward more sophisticated models and training algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
