{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Implementation in Python (From Scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will implement a neural network *from scratch* using **pure Python** and **NumPy** to truly understand how forward and backward propagation work under the hood. By the end of this lesson, you should have a clear understanding of how layers are chained together, how errors are computed, and how gradients are efficiently propagated backward through the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a neural network, data moves through a series of layers until it produces an output. Once we have that output, we compare it to our target (the *desired* output) and compute an **error** (or *loss*):\n",
    "\n",
    "1. **Input ‚Üí** Feed data into the neural network.\n",
    "2. **Forward Propagation ‚Üí** Data flows through each layer sequentially until it produces an *output*.\n",
    "3. **Error Calculation ‚Üí** Compare the network‚Äôs output to the desired output to obtain an *error*.\n",
    "4. **Parameter Update ‚Üí** Use the *error gradient* with respect to each parameter to update weights and biases (via the *chain rule*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/training-loop.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat these steps iteratively, aiming to reduce the total error over time. This process is the essence of **gradient descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:**\n",
    "The crucial component is the ability to compute the derivative of the error with respect to each parameter, no matter how we structure our network or what activation functions we use. To achieve this flexibility, we will implement each layer **individually**. Each layer will:\n",
    "\n",
    "- Receive an input (X) and produce an output (Y) during forward propagation.\n",
    "- Calculate partial derivatives ‚àÇE/‚àÇY during backward propagation and pass ‚àÇE/‚àÇX to the preceding layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nn-layer.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modular design allows us to chain layers in any configuration while still performing correct gradient updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, our goal is to demystify the fundamental steps behind backpropagation and gradient descent by constructing a simple, fully connected network completely **from scratch**. We will:\n",
    "\n",
    "- **Define a base Layer class** to handle inputs, outputs, and abstract forward/backward methods.\n",
    "- **Implement a Fully Connected (FC) Layer** that performs matrix multiplication and updates according to calculated gradients.\n",
    "- **Introduce an Activation Layer** (e.g., ReLU or Sigmoid) to add non-linearity to the network.\n",
    "- **Implement a simple Loss function** (such as Mean Squared Error) and show how it interacts with the final network output.\n",
    "- **Assemble everything** into a cohesive Network class, allowing us to train on example tasks like XOR and potentially MNIST (with flattened images).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip:**\n",
    "Building a neural network in this manner may feel more complex than using a library, but it is the best way to understand how each computation step works internally. Once you grasp these fundamentals, transitioning to higher-level libraries (like `scikit-learn` or deep learning frameworks) becomes more intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Layer-by-Layer Approach](#toc1_)    \n",
    "  - [Forward vs. Backward Propagation](#toc1_1_)    \n",
    "  - [Why Separate Layers and Activations?](#toc1_2_)    \n",
    "  - [Handling Arbitrary Network Architectures](#toc1_3_)    \n",
    "- [Abstract Base Class: Layer](#toc2_)    \n",
    "  - [Inputs and Outputs](#toc2_1_)    \n",
    "  - [Forward Method](#toc2_2_)    \n",
    "  - [Backward Method](#toc2_3_)    \n",
    "  - [Learning Rate Considerations](#toc2_4_)    \n",
    "- [Fully Connected Layer](#toc3_)    \n",
    "  - [Matrix Multiplication for Forward Propagation](#toc3_1_)    \n",
    "  - [Derivatives for Backpropagation](#toc3_2_)    \n",
    "  - [Updating Weights and Biases](#toc3_3_)    \n",
    "- [Activation Layer](#toc4_)    \n",
    "  - [Introducing Non-Linearity](#toc4_1_)    \n",
    "  - [Forward Pass (Element-Wise)](#toc4_2_)    \n",
    "  - [Derivative Computation for Backpropagation](#toc4_3_)    \n",
    "  - [Example Implementation](#toc4_4_)    \n",
    "- [Defining the Loss Function](#toc5_)    \n",
    "  - [Mean Squared Error (MSE)](#toc5_1_)    \n",
    "  - [Implementing the Loss and Its Derivative](#toc5_2_)    \n",
    "  - [Why Keep Loss Separate?](#toc5_3_)    \n",
    "- [Building the Network Class](#toc6_)    \n",
    "  - [Sequential Architecture](#toc6_1_)    \n",
    "  - [Forward Pass Through All Layers](#toc6_2_)    \n",
    "  - [Backpropagation Chaining](#toc6_3_)    \n",
    "  - [Training Loop Structure](#toc6_4_)    \n",
    "- [Example 1: Solving XOR](#toc7_)    \n",
    "  - [Dataset Setup](#toc7_1_)    \n",
    "  - [Network Configuration](#toc7_2_)    \n",
    "  - [Training and Results](#toc7_3_)    \n",
    "  - [Verifying the Model](#toc7_4_)    \n",
    "- [Example 2: Solving MNIST](#toc8_)    \n",
    "  - [Dataset Considerations](#toc8_1_)    \n",
    "  - [Flattening Images for Input](#toc8_2_)    \n",
    "  - [Network Configuration and Training](#toc8_3_)    \n",
    "  - [Evaluating Results](#toc8_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Layer-by-Layer Approach](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see why building a neural network *one layer at a time* provides us with both **flexibility** and **simplicity** when it comes to forward and backward propagation. By keeping each layer self-contained‚Äîhandling its own inputs, outputs, and parameter gradients‚Äîwe can mix and match layers of different types without rewriting large parts of the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nn-layer.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Forward vs. Backward Propagation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation** is the step where:\n",
    "1. Input data flows into a layer.\n",
    "2. The layer computes its output based on provided parameters (weights, biases) and any activation function.\n",
    "3. The output of the current layer becomes the input to the next layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/forward-propagation.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward propagation** (often shortened to ‚Äúbackprop‚Äù) is the step where:\n",
    "1. We start from the **final** layer and calculate the derivative of the error with respect to the layer‚Äôs *output*.\n",
    "2. We then compute the derivative of the error with respect to the layer‚Äôs *parameters* (e.g., weights, biases).\n",
    "3. Finally, we determine the derivative of the error with respect to the layer‚Äôs *input*, which the previous layer needs in order to continue backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/backward-propagation.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the **output** of one layer is the **input** of the next. By systematically propagating gradients from the last layer to the first, we can efficiently update every parameter in the network using the chain rule, regardless of how many layers or types of layers we have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why split these steps?\n",
    "- By splitting these steps, each layer can focus on its local computations‚Äîi.e., how it processes an input during forward propagation and how it calculates updates during backward propagation.\n",
    "- The network as a whole benefits from a ‚Äúplug-and-play‚Äù architecture: as long as each layer knows how to handle forward and backward passes, they can be stacked in any order to form complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Why Separate Layers and Activations?](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, one might be tempted to combine **linear transformations** (like the fully connected layer‚Äôs matrix multiplication) with **activation functions** (like ReLU or Sigmoid) in a single class. However, there are practical advantages to defining each as a distinct layer:\n",
    "\n",
    "1. **Modularity:** You can ‚Äúturn off‚Äù or swap out an activation function without altering the rest of your code.\n",
    "2. **Clarity:** Activations often have no trainable parameters (e.g., ReLU); keeping them separate highlights which parts of the network are learning.\n",
    "3. **Extendability:** It becomes trivial to introduce new layer types, such as Convolutional or Dropout layers, when the activation logic is not tangled with the core linear operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nn-layers.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a conceptual snippet illustrating how one layer passes its output to the next:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Pseudocode illustrating separate linear and activation layers\n",
    "\n",
    "X = ...  # Input data\n",
    "fc_layer = FullyConnectedLayer(input_size, output_size)\n",
    "activation_layer = ActivationLayer(activation_function)\n",
    "\n",
    "# Forward pass through FC layer\n",
    "Z = fc_layer.forward(X)\n",
    "\n",
    "# Forward pass through Activation layer\n",
    "Y = activation_layer.forward(Z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If later you decide to switch from a Sigmoid activation to a ReLU, you only change the definition of `activation_layer`‚Äîthe rest remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Handling Arbitrary Network Architectures](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining multiple layers, we can create powerful neural networks of any depth or composition. For instance, your network might look like this:\n",
    "\n",
    "- **Fully Connected Layer** ‚Üí **Activation Layer** ‚Üí **Fully Connected Layer** ‚Üí **Activation Layer** ‚Üí **Loss**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing backpropagation, each forward operation has a corresponding **reverse** calculation. Each layer:\n",
    "1. Receives ‚àÇE/‚àÇY (error derivative w.r.t. its output) from the layer that follows it.\n",
    "2. Computes ‚àÇE/‚àÇX (derivative w.r.t. its input) to pass back to the layer that precedes it.\n",
    "3. Updates any trainable parameters (weights or biases) if applicable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, you can imagine a loop like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Forward pass\n",
    "for layer in network_layers:\n",
    "    X = layer.forward(X)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_function(X, Y_true)\n",
    "\n",
    "# Backward pass\n",
    "grad = loss_function_prime(X, Y_true)\n",
    "for layer in reversed(network_layers):\n",
    "    grad = layer.backward(grad, learning_rate)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `network_layers` might contain different types of layers (fully connected, activation, etc.). Because each layer implements `forward` and `backward` in a standard way, we don‚Äôt need special logic for different architectures‚Äîeverything just *fits* together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adhering to this layer-by-layer approach, we can tackle modifications (e.g., adding layers, changing activation functions) with minimal disruption to the rest of the code. This structure is exactly what enables complex frameworks like PyTorch or TensorFlow to handle diverse network designs while remaining consistent under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Abstract Base Class: Layer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key insights for building our neural network from scratch is to define a **base class** for layers. Each specific type of layer (e.g., **Fully Connected**, **Convolutional**, **Activation**) will inherit from this class, ensuring consistent *forward* and *backward* method signatures. By doing so, we can handle a variety of layer types in a unified fashion when chaining them together in our **Network**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Inputs and Outputs](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, every layer deals with:\n",
    "\n",
    "- **Input**: The data it receives from the previous layer (or from the dataset if it‚Äôs the first layer).\n",
    "- **Output**: The data it computes and sends to the next layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will be stored in attributes (e.g., `self.input` and `self.output`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Forward Method](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer must implement a `forward` method that computes its **output** (`self.output`) based on its **input**. For example:\n",
    "\n",
    "- A **fully connected** layer will apply a matrix multiplication.\n",
    "- An **activation** layer will apply a function element-wise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the abstract base class, we only define the **interface** and leave the actual implementation for subclasses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Backward Method](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go backward in backpropagation because it's the most efficient way to calculate how each weight and bias in a neural network contributed to the overall error. Here's a detailed explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a basic neural network:\n",
    "- Input (x) ‚Üí Weight1 (w1) ‚Üí Hidden (h) ‚Üí Weight2 (w2) ‚Üí Output (y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mnist-cost.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mnist-cost-2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Forward vs Backward approaches:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FORWARD APPROACH (Why it's problematic):\n",
    "1. Start at input (x)\n",
    "2. You want to know how changing w1 affects the final error\n",
    "3. BUT you don't know yet how the hidden layer (h) will influence the final output\n",
    "4. You'd have to:\n",
    "   - Calculate ‚àÇh/‚àÇw1 (effect on hidden layer)\n",
    "   - Then calculate ‚àÇy/‚àÇh (effect on output)\n",
    "   - Then combine these to get ‚àÇy/‚àÇw1\n",
    "   - Do this for EVERY possible path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's like trying to predict how changing your first chess move will affect the game's outcome without knowing future moves!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACKWARD APPROACH (Why it's better):\n",
    "1. Start at output (y) where you know the error\n",
    "2. Calculate ‚àÇerror/‚àÇw2 directly\n",
    "3. Then use this to calculate ‚àÇerror/‚àÇw1\n",
    "4. Each step uses information we already have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/chain-rule.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's like watching a recorded chess game and working backwards to see which moves led to the final result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of it like a chain of dominoes:\n",
    "- Forward: You're trying to predict how hard to push the first domino without knowing the path ahead\n",
    "- Backward: You see which dominoes fell and trace back to understand why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During backpropagation, each layer must receive the derivative of the error with respect to its *own* output, commonly denoted as ‚àÇE/‚àÇY or `dE_dY`. From there, the layer should:\n",
    "\n",
    "1. Compute the derivative with respect to its parameters (if it has any, like weights or biases).\n",
    "2. Update those parameters (e.g., via *gradient descent*).\n",
    "3. Compute ‚àÇE/‚àÇX (the derivative of the error with respect to the **layer‚Äôs input**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **backward** method should return ‚àÇE/‚àÇX so that the previous layer can continue the chain of backward propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Learning Rate Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our abstract `backward` method includes a parameter for **learning_rate**. In more sophisticated implementations, you might replace this with an *optimizer class/strategy* (e.g., Momentum, Adam). For simplicity, we will stick to a basic learning rate in this scratch implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how we might define this class in code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract base class for network layers.\n",
    "\n",
    "    Every layer must implement:\n",
    "        - forward(input_data)\n",
    "        - backward(dE_dY, learning_rate)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Store input and output data for reference\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Computes the output Y of the layer given the input X.\n",
    "\n",
    "        :param input_data: numpy array of shape (batch_size, input_dim)\n",
    "        :return: output_data: numpy array of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, dE_dY, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass that computes dE/dX for this layer,\n",
    "        and updates parameters if the layer has any.\n",
    "\n",
    "        :param dE_dY: derivative of the network error with respect to this layer's output\n",
    "        :param learning_rate: the learning rate for parameter updates\n",
    "        :return: dE_dX: derivative of the network error with respect to this layer's input\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this **Layer** abstract class, we have a template for **forward** and **backward** propagation that all subsequent layers will follow. This pattern keeps our code clean, consistent, and easy to extend with new layer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Fully Connected Layer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Fully Connected (FC) Layer** (also known as a **Dense Layer**) takes each input neuron and connects it to every output neuron via a **weight matrix** and **bias** vector. This layer is entirely responsible for the linear component of the transformation:\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\mathbf{W} + \\mathbf{B}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fully-connected-layer.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "- $\\mathbf{X}$ is the input to this layer (shape: *batch_size √ó input_dim*),\n",
    "- $\\mathbf{W}$ is the weight matrix (shape: *input_dim √ó output_dim*),\n",
    "- $\\mathbf{B}$ is the bias vector (shape: *1 √ó output_dim*),\n",
    "- $\\mathbf{Y}$ is the output (shape: *batch_size √ó output_dim*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Matrix Multiplication for Forward Propagation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the **forward** pass, we simply perform this matrix multiplication and add the bias:\n",
    "\n",
    "1. **Retrieve input**: The layer takes the incoming data from `self.input`.\n",
    "2. **Compute output**: Multiply it by the weight matrix and add the bias.\n",
    "3. **Store the result** in `self.output` so it can be used later during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how this might look in code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases randomly\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Perform the forward pass for a fully connected layer:\n",
    "        Y = XW + B\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By separating the **Fully Connected Layer** from any **Activation** layer, we make our design more modular and easier to maintain or modify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Derivatives for Backpropagation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During **backpropagation**, we must compute:\n",
    "1. $\\frac{\\partial E}{\\partial \\mathbf{W}}$ and $\\frac{\\partial E}{\\partial \\mathbf{B}}$ to update our trainable parameters.\n",
    "2. $\\frac{\\partial E}{\\partial \\mathbf{X}}$ so the previous layer knows how the error changes with respect to its output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\mathbf{dE\\_dY}$ (the derivative of the error $E$ with respect to this layer‚Äôs output $\\mathbf{Y}$), we use the chain rule:\n",
    "\n",
    "1. **Weight Gradient** ($\\frac{\\partial E}{\\partial \\mathbf{W}}$):\n",
    "   $$\\frac{\\partial E}{\\partial \\mathbf{W}} = \\mathbf{X}^T \\cdot \\mathbf{dE\\_dY}$$\n",
    "2. **Bias Gradient** ($\\frac{\\partial E}{\\partial \\mathbf{B}}$):\n",
    "   $$\\frac{\\partial E}{\\partial \\mathbf{B}} = \\sum_{\\text{all samples}} \\mathbf{dE\\_dY}$$\n",
    "3. **Input Gradient** ($\\frac{\\partial E}{\\partial \\mathbf{X}}$):\n",
    "   $$\\frac{\\partial E}{\\partial \\mathbf{X}} = \\mathbf{dE\\_dY} \\cdot \\mathbf{W}^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass $\\frac{\\partial E}{\\partial \\mathbf{X}}$ to the previous layer and update $\\mathbf{W}$ and $\\mathbf{B}$ using the chosen learning rate $\\alpha$:\n",
    "\n",
    "$$\\mathbf{W} \\leftarrow \\mathbf{W} - \\alpha \\,\\frac{\\partial E}{\\partial \\mathbf{W}}, \\quad \\mathbf{B} \\leftarrow \\mathbf{B} - \\alpha \\,\\frac{\\partial E}{\\partial \\mathbf{B}}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Updating Weights and Biases](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, our `backward` method might look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases randomly\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Perform the forward pass for a fully connected layer:\n",
    "        Y = XW + B\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dE_dY, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs the backward pass:\n",
    "            1. Compute gradients for weights and biases using dE_dY.\n",
    "            2. Update weights and biases.\n",
    "            3. Return dE_dX to propagate the error backward.\n",
    "        \"\"\"\n",
    "        # Number of training examples in the batch\n",
    "        batch_size = self.input.shape[0]\n",
    "\n",
    "        # dE/dW = X^T * dE/dY\n",
    "        dE_dW = np.dot(self.input.T, dE_dY)\n",
    "\n",
    "        # dE/dB = sum of dE/dY across every sample\n",
    "        dE_dB = np.sum(dE_dY, axis=0, keepdims=True)\n",
    "\n",
    "        # dE/dX = dE/dY * W^T\n",
    "        dE_dX = np.dot(dE_dY, self.weights.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * dE_dW\n",
    "        self.bias -= learning_rate * dE_dB\n",
    "\n",
    "        return dE_dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We compute $\\frac{\\partial E}{\\partial \\mathbf{W}}$ and $\\frac{\\partial E}{\\partial \\mathbf{B}}$ based on the inputs and $\\mathbf{dE\\_dY}$.\n",
    "2. We update our weight matrix and bias vector via the learning rate.\n",
    "3. Finally, we compute and return $\\mathbf{dE\\_dX}$, which will serve as $\\mathbf{dE\\_dY}$ for the preceding layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this layer class handles **only** the linear transformation‚Äîactivations will be handled as a separate layer, making our code more modular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Activation Layer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using only fully connected layers and matrix multiplications, the model behaves like a simple linear function, which severely limits its ability to learn complex patterns. To remedy this, we introduce **activation functions**. By applying a non-linear element-wise transformation, we give the network enough flexibility to capture non-linear relationships in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/activation-functions-3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Introducing Non-Linearity](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **activation function** takes an input $X$ (the output of a previous linear transformation) and applies a non-linear function $f$, producing an output $Y$ of the same shape:\n",
    "\n",
    "$$\n",
    "Y = f(X)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical examples of activation functions include **Sigmoid**, **Tanh**, and **ReLU**:\n",
    "\n",
    "- **Sigmoid** $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **Tanh** $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- **ReLU** $\\mathrm{ReLU}(x) = \\max(0, x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By separating activations into their own layer, we can easily change which activation function we use without altering our fully connected layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Forward Pass (Element-Wise)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the **forward** pass, the activation layer simply takes the output of the previous layer (e.g., a **Fully Connected** layer) and applies the chosen activation function *element-wise*. This means if the input is a matrix $\\mathbf{X}$, each element $x_{i,j}$ is mapped to $f(x_{i,j})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:**\n",
    "Because each element of the input is independently transformed, the shape of the output matches the shape of the input for this layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Derivative Computation for Backpropagation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the **Fully Connected** layer, an activation layer typically has **no trainable parameters** (no weights or biases). However, we still need to compute how the error changes with respect to its input $\\mathbf{X}$. Given $\\frac{\\partial E}{\\partial Y}$ (the derivative of the error with respect to this layer‚Äôs output), we use the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial X}\n",
    "= \\frac{\\partial E}{\\partial Y} \\odot f'(X)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\odot$ denotes the *element-wise multiplication*, and $f'(X)$ is the derivative of the activation function evaluated at each element of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_'></a>[Example Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple **ActivationLayer** class. We pass it two functions at initialization:\n",
    "‚Ä¢ **activation**: The forward pass function $f$\n",
    "‚Ä¢ **activation_prime**: The derivative $f'$ for the backward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    \"\"\"\n",
    "    An activation layer to apply a non-linear function element-wise.\n",
    "    \"\"\"\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Applies the activation function element-wise.\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dE_dY, learning_rate):\n",
    "        \"\"\"\n",
    "        Computes dE/dX by multiplying dE/dY element-wise with f'(X).\n",
    "        Activation layers have no trainable parameters to update.\n",
    "        \"\"\"\n",
    "        return dE_dY * self.activation_prime(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example activation function might look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return (x > 0).astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decoupled design allows us to mix and match any activation function we like. For instance, we might switch from **ReLU** to **Sigmoid** with minimal code changes. By keeping this layer separate, we maintain a flexible framework that can accommodate the many non-linearities essential to modern neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Defining the Loss Function](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neural network‚Äôs goal is to *minimize* an error, or **loss**, that quantifies the difference between the **predicted output** and the **desired output**. This means we need:\n",
    "\n",
    "1. A function to compute the **loss** value (e.g., **Mean Squared Error**, **Cross-Entropy**, etc.).\n",
    "2. A corresponding function to compute the **gradient** of the loss with respect to the network‚Äôs output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/loss.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Mean Squared Error (MSE)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest loss functions is **Mean Squared Error (MSE)**, particularly suitable for **regression** tasks. For predictions $ \\mathbf{Y} $ and true values $ \\mathbf{Y^*} $, the MSE is defined as:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{Y^*}, \\mathbf{Y}) = \\frac{1}{n} \\sum_i (y_i^* - y_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n$ is the number of samples in the batch.\n",
    "\n",
    "- A **low** MSE means our predictions are close to the target values.\n",
    "- A **high** MSE means there is a large discrepancy between predictions and targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform backpropagation, we also need the **derivative** of $ E $ with respect to $ \\mathbf{Y} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\mathbf{Y}}\n",
    "= \\frac{2}{n} (\\mathbf{Y} - \\mathbf{Y^*})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[Implementing the Loss and Its Derivative](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a layer, which has trainable parameters and a structure for forward/backward passes, the loss can be implemented as two simple functions in Python. Below is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss function.\n",
    "    :param y_true: numpy array of shape (batch_size, output_dim)\n",
    "    :param y_pred: numpy array of shape (batch_size, output_dim)\n",
    "    :return: scalar (average MSE for the batch)\n",
    "    \"\"\"\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of the MSE loss function with respect to y_pred.\n",
    "    :param y_true: numpy array of shape (batch_size, output_dim)\n",
    "    :param y_pred: numpy array of shape (batch_size, output_dim)\n",
    "    :return: numpy array of the same shape as y_pred\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true) / y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs how these functions integrate into the **training loop** later:\n",
    "\n",
    "1. **Forward Pass**: The network outputs predicted values $ \\mathbf{Y} $.\n",
    "2. **Loss Computation**: We calculate $ E = \\text{mse}( \\mathbf{Y^*}, \\mathbf{Y} ) $.\n",
    "3. **Gradient w.r.t. Output**: We compute $ \\mathbf{dE_dY} = \\text{mse\\_prime} ( \\mathbf{Y^*}, \\mathbf{Y} ) $.\n",
    "4. **Backward Pass**: We feed $ \\mathbf{dE_dY} $ into the final layer‚Äôs `backward` method, which passes gradients backward through the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_'></a>[Why Keep Loss Separate?](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Simplicity:** Loss functions typically do not have trainable parameters.\n",
    "- **Flexibility:** If you want to try **Cross-Entropy** or a custom loss function, you just replace these two functions without changing the layers‚Äô code.\n",
    "- **Clarity:** Separating the network architecture from the definition of the loss improves overall readability and maintainability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By isolating the loss function in independent functions, we ensure that the network can easily adapt to different **metrics** and **objectives**‚Äîessentially, this completes the picture of how data flows from **input** to **loss** and then how gradients flow back from **loss** to **parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Building the Network Class](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing all the pieces together‚Äîlayers, activation functions, and the loss‚Äîwe can create a **Network** class to streamline the forward and backward passes over *any* collection of layers. This class will:\n",
    "\n",
    "1. Maintain an **ordered list of layers** (fully connected, activation, etc.) to be passed data in sequence.\n",
    "2. Provide a **forward** method that feeds the input through each layer in order.\n",
    "3. Provide a **backward** method that propagates gradients back through the network (in reverse order), updating parameters along the way.\n",
    "4. Offer a **train** method to run multiple epochs of forward/backward cycles, compute loss, and track progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Sequential Architecture](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Network** class holds a list (e.g., `self.layers`) to represent a sequence of transformations. When you add layers, you simply append them:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        # List to store layers in order\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Add a layer to the network sequence.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This design allows total flexibility in layering:\n",
    "‚Ä¢ Fully Connected layers ‚Üí Activation layers ‚Üí More Fully Connected layers ‚Üí ‚Ä¶\n",
    "The network simply forwards data from the first layer to the last, then backpropagates in reverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_2_'></a>[Forward Pass Through All Layers](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the **forward** pass, we take an input (e.g., a batch of data) and feed it sequentially through each layer‚Äôs `forward` method. By the time we reach the final layer, we have the network‚Äôs prediction for that batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward(self, input_data):\n",
    "    \"\"\"\n",
    "    Feeds input_data through each layer in the model.\n",
    "    \"\"\"\n",
    "    result = input_data\n",
    "    for layer in self.layers:\n",
    "        result = layer.forward(result)\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a dedicated `forward` method makes it easy to distinguish between *pure inference* (when we just want predictions) and *training* (when we also need gradients).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_3_'></a>[Backpropagation Chaining](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute the loss between the network‚Äôs final output and the true labels, we calculate the derivative of the loss w.r.t. the **output** (e.g., using `mse_prime`) and propagate that backwards through each layer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def backward(self, loss_grad, learning_rate):\n",
    "    \"\"\"\n",
    "    Propagates the loss gradient backward through the network,\n",
    "    updating parameters in each layer as needed.\n",
    "    \"\"\"\n",
    "    grad = loss_grad\n",
    "    # Traverse layers in reverse order\n",
    "    for layer in reversed(self.layers):\n",
    "        grad = layer.backward(grad, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `loss_grad` is simply $\\frac{\\partial E}{\\partial Y}$ (the derivative of the error with respect to the network‚Äôs final output). Each layer:\n",
    "- Computes parameter gradients (if any), updates them via gradient descent.\n",
    "- Returns $\\frac{\\partial E}{\\partial X}$ to feed into the previous layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_4_'></a>[Training Loop Structure](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical **training** method brings everything together. Over multiple epochs, it:\n",
    "\n",
    "1. **Forward pass** the input data.\n",
    "2. **Compute the loss**.\n",
    "3. **Compute the loss gradient**.\n",
    "4. **Backward pass** to update all parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simplified outline:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, x_train, y_train, loss, loss_prime, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output = self.forward(x_train)\n",
    "\n",
    "        # Compute loss (scalar)\n",
    "        error = loss(y_train, output)\n",
    "\n",
    "        # Print or log the error for tracking\n",
    "        print(f\"epoch {epoch+1}/{epochs}   error={error:.6f}\")\n",
    "\n",
    "        # Compute gradient of the loss w.r.t output\n",
    "        grad = loss_prime(y_train, output)\n",
    "\n",
    "        # Backward pass\n",
    "        self.backward(grad, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Forward**: `output = self.forward(x_train)`\n",
    "2. **Loss**: `error = loss(y_train, output)`\n",
    "3. **Derivative**: `grad = loss_prime(y_train, output)`\n",
    "4. **Backward**: `self.backward(grad, learning_rate)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the complete code for the **Network** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        # List to store layers in order\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Add a layer to the network sequence.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Feeds input_data through each layer in the model.\n",
    "        \"\"\"\n",
    "        result = input_data\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, loss_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Propagates the loss gradient backward through the network,\n",
    "        updating parameters in each layer as needed.\n",
    "        \"\"\"\n",
    "        grad = loss_grad\n",
    "        # Traverse layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "    def train(self, x_train, y_train, loss, loss_prime, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(x_train)\n",
    "\n",
    "            # Compute loss (scalar)\n",
    "            error = loss(y_train, output)\n",
    "\n",
    "            # Print or log the error for tracking\n",
    "            print(f\"epoch {epoch+1}/{epochs}   error={error:.6f}\")\n",
    "\n",
    "            # Compute gradient of the loss w.r.t output\n",
    "            grad = loss_prime(y_train, output)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(grad, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:**\n",
    "For larger datasets, you would typically iterate over batches or mini-batches of data rather than the entire dataset at once. This example focuses on *simple* usage but can be extended to handle batching or more advanced techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By encapsulating these steps in a **Network** class, we have a clear, organized way to build and train arbitrarily complex architectures. Each **layer**, whether it‚Äôs fully connected or an activation layer, follows the same contract:\n",
    "‚Ä¢ `forward(input_data)` ‚Üí outputs a transformed result.\n",
    "‚Ä¢ `backward(dE_dY, learning_rate)` ‚Üí updates parameters and returns the gradient for the previous layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flexible structure is the core principle underpinning many modern deep learning frameworks‚Äîyou now have a miniature version built entirely from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>[Example 1: Solving XOR](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic demonstration of neural networks‚Äô capability to learn non-linear functions is the **XOR (exclusive OR)** problem. XOR takes two binary inputs (0 or 1) and outputs 1 if exactly one of the inputs is 1, otherwise 0. Formally:\n",
    "\n",
    "- (0, 0) ‚Üí 0\n",
    "- (0, 1) ‚Üí 1\n",
    "- (1, 0) ‚Üí 1\n",
    "- (1, 1) ‚Üí 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/xor.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because XOR is *linearly inseparable*, a single-layer perceptron (without non-linear activation) fails to learn it. However, once we introduce at least one **hidden layer** with a non-linear activation, the network can *exactly* model this function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_1_'></a>[Dataset Setup](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our XOR dataset consists of four samples, each with **two features**:\n",
    "\n",
    "1. (0, 0)\n",
    "2. (0, 1)\n",
    "3. (1, 0)\n",
    "4. (1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding target outputs (labels) are $[0, 1, 1, 0]$. For a fully connected network, we need to shape the data properly:\n",
    "\n",
    "- **Input** should have one sample per row (shape $\\text{(num\\_samples, input\\_dim)}$).\n",
    "- **Target** in this case is also 1D (shape $\\text{(num\\_samples, 1)}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a typical way to define this in NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XOR Input (4 samples, each with 2 features)\n",
    "x_train = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# XOR Target Output (4 samples, each with 1 label)\n",
    "y_train = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_2_'></a>[Network Configuration](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve XOR, we need at least one hidden layer with a non-linear activation. A configuration might look like this:\n",
    "\n",
    "1. **Fully Connected Layer** with 2 inputs and, say, 3 hidden units.\n",
    "2. **Activation Layer** (e.g., ReLU or Sigmoid).\n",
    "3. **Fully Connected Layer** to map from 3 hidden units down to 1 output.\n",
    "4. **Activation Layer** (often Sigmoid in a binary classification context).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs what assembling it with our classes might look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Build a simple network for XOR\n",
    "network = Network()\n",
    "network.add(FCLayer(2, 3))  # Fully connected (input_dim=2, output_dim=3)\n",
    "network.add(ActivationLayer(relu, relu_prime))\n",
    "network.add(FCLayer(3, 1))  # Fully connected (input_dim=3, output_dim=1)\n",
    "network.add(ActivationLayer(sigmoid, sigmoid_prime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_3_'></a>[Training and Results](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the network using our **Mean Squared Error** loss (though in binary classification, other losses like **Binary Cross-Entropy** are common). For XOR, our dataset is tiny, so we can train on the entire set in each epoch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.249989\n",
      "epoch 2/1000   error=0.249986\n",
      "epoch 3/1000   error=0.249984\n",
      "epoch 4/1000   error=0.249982\n",
      "epoch 5/1000   error=0.249981\n",
      "epoch 6/1000   error=0.249979\n",
      "epoch 7/1000   error=0.249977\n",
      "epoch 8/1000   error=0.249975\n",
      "epoch 9/1000   error=0.249973\n",
      "epoch 10/1000   error=0.249970\n",
      "epoch 11/1000   error=0.249967\n",
      "epoch 12/1000   error=0.249964\n",
      "epoch 13/1000   error=0.249962\n",
      "epoch 14/1000   error=0.249958\n",
      "epoch 15/1000   error=0.249954\n",
      "epoch 16/1000   error=0.249950\n",
      "epoch 17/1000   error=0.249946\n",
      "epoch 18/1000   error=0.249941\n",
      "epoch 19/1000   error=0.249936\n",
      "epoch 20/1000   error=0.249930\n",
      "epoch 21/1000   error=0.249925\n",
      "epoch 22/1000   error=0.249917\n",
      "epoch 23/1000   error=0.249911\n",
      "epoch 24/1000   error=0.249902\n",
      "epoch 25/1000   error=0.249895\n",
      "epoch 26/1000   error=0.249885\n",
      "epoch 27/1000   error=0.249876\n",
      "epoch 28/1000   error=0.249864\n",
      "epoch 29/1000   error=0.249853\n",
      "epoch 30/1000   error=0.249839\n",
      "epoch 31/1000   error=0.249826\n",
      "epoch 32/1000   error=0.249810\n",
      "epoch 33/1000   error=0.249794\n",
      "epoch 34/1000   error=0.249775\n",
      "epoch 35/1000   error=0.249757\n",
      "epoch 36/1000   error=0.249734\n",
      "epoch 37/1000   error=0.249713\n",
      "epoch 38/1000   error=0.249686\n",
      "epoch 39/1000   error=0.249661\n",
      "epoch 40/1000   error=0.249629\n",
      "epoch 41/1000   error=0.249599\n",
      "epoch 42/1000   error=0.249561\n",
      "epoch 43/1000   error=0.249526\n",
      "epoch 44/1000   error=0.249481\n",
      "epoch 45/1000   error=0.249441\n",
      "epoch 46/1000   error=0.249387\n",
      "epoch 47/1000   error=0.249340\n",
      "epoch 48/1000   error=0.249276\n",
      "epoch 49/1000   error=0.249220\n",
      "epoch 50/1000   error=0.249144\n",
      "epoch 51/1000   error=0.249080\n",
      "epoch 52/1000   error=0.248989\n",
      "epoch 53/1000   error=0.248915\n",
      "epoch 54/1000   error=0.248806\n",
      "epoch 55/1000   error=0.248721\n",
      "epoch 56/1000   error=0.248590\n",
      "epoch 57/1000   error=0.248493\n",
      "epoch 58/1000   error=0.248335\n",
      "epoch 59/1000   error=0.248225\n",
      "epoch 60/1000   error=0.248035\n",
      "epoch 61/1000   error=0.247912\n",
      "epoch 62/1000   error=0.247681\n",
      "epoch 63/1000   error=0.247545\n",
      "epoch 64/1000   error=0.247265\n",
      "epoch 65/1000   error=0.247117\n",
      "epoch 66/1000   error=0.246775\n",
      "epoch 67/1000   error=0.246619\n",
      "epoch 68/1000   error=0.246199\n",
      "epoch 69/1000   error=0.246040\n",
      "epoch 70/1000   error=0.245522\n",
      "epoch 71/1000   error=0.245370\n",
      "epoch 72/1000   error=0.244728\n",
      "epoch 73/1000   error=0.244596\n",
      "epoch 74/1000   error=0.243798\n",
      "epoch 75/1000   error=0.243707\n",
      "epoch 76/1000   error=0.242710\n",
      "epoch 77/1000   error=0.242690\n",
      "epoch 78/1000   error=0.241439\n",
      "epoch 79/1000   error=0.241534\n",
      "epoch 80/1000   error=0.239959\n",
      "epoch 81/1000   error=0.240228\n",
      "epoch 82/1000   error=0.238303\n",
      "epoch 83/1000   error=0.238667\n",
      "epoch 84/1000   error=0.236882\n",
      "epoch 85/1000   error=0.236568\n",
      "epoch 86/1000   error=0.235208\n",
      "epoch 87/1000   error=0.234145\n",
      "epoch 88/1000   error=0.233375\n",
      "epoch 89/1000   error=0.231358\n",
      "epoch 90/1000   error=0.231384\n",
      "epoch 91/1000   error=0.228328\n",
      "epoch 92/1000   error=0.229241\n",
      "epoch 93/1000   error=0.226461\n",
      "epoch 94/1000   error=0.225265\n",
      "epoch 95/1000   error=0.224361\n",
      "epoch 96/1000   error=0.220833\n",
      "epoch 97/1000   error=0.222133\n",
      "epoch 98/1000   error=0.218114\n",
      "epoch 99/1000   error=0.217788\n",
      "epoch 100/1000   error=0.216565\n",
      "epoch 101/1000   error=0.212487\n",
      "epoch 102/1000   error=0.214026\n",
      "epoch 103/1000   error=0.211442\n",
      "epoch 104/1000   error=0.207214\n",
      "epoch 105/1000   error=0.209273\n",
      "epoch 106/1000   error=0.206804\n",
      "epoch 107/1000   error=0.202630\n",
      "epoch 108/1000   error=0.203457\n",
      "epoch 109/1000   error=0.202619\n",
      "epoch 110/1000   error=0.198698\n",
      "epoch 111/1000   error=0.196680\n",
      "epoch 112/1000   error=0.198801\n",
      "epoch 113/1000   error=0.195334\n",
      "epoch 114/1000   error=0.191767\n",
      "epoch 115/1000   error=0.192960\n",
      "epoch 116/1000   error=0.193276\n",
      "epoch 117/1000   error=0.190223\n",
      "epoch 118/1000   error=0.187200\n",
      "epoch 119/1000   error=0.186509\n",
      "epoch 120/1000   error=0.189033\n",
      "epoch 121/1000   error=0.186117\n",
      "epoch 122/1000   error=0.184035\n",
      "epoch 123/1000   error=0.181930\n",
      "epoch 124/1000   error=0.182104\n",
      "epoch 125/1000   error=0.184402\n",
      "epoch 126/1000   error=0.182487\n",
      "epoch 127/1000   error=0.180501\n",
      "epoch 128/1000   error=0.178803\n",
      "epoch 129/1000   error=0.177421\n",
      "epoch 130/1000   error=0.178078\n",
      "epoch 131/1000   error=0.180200\n",
      "epoch 132/1000   error=0.178702\n",
      "epoch 133/1000   error=0.177346\n",
      "epoch 134/1000   error=0.176060\n",
      "epoch 135/1000   error=0.175018\n",
      "epoch 136/1000   error=0.174152\n",
      "epoch 137/1000   error=0.173609\n",
      "epoch 138/1000   error=0.173898\n",
      "epoch 139/1000   error=0.173169\n",
      "epoch 140/1000   error=0.173500\n",
      "epoch 141/1000   error=0.174814\n",
      "epoch 142/1000   error=0.173826\n",
      "epoch 143/1000   error=0.173035\n",
      "epoch 144/1000   error=0.172386\n",
      "epoch 145/1000   error=0.171841\n",
      "epoch 146/1000   error=0.171378\n",
      "epoch 147/1000   error=0.170978\n",
      "epoch 148/1000   error=0.171950\n",
      "epoch 149/1000   error=0.173425\n",
      "epoch 150/1000   error=0.172601\n",
      "epoch 151/1000   error=0.172048\n",
      "epoch 152/1000   error=0.171491\n",
      "epoch 153/1000   error=0.171075\n",
      "epoch 154/1000   error=0.170822\n",
      "epoch 155/1000   error=0.170461\n",
      "epoch 156/1000   error=0.170151\n",
      "epoch 157/1000   error=0.169881\n",
      "epoch 158/1000   error=0.169644\n",
      "epoch 159/1000   error=0.169435\n",
      "epoch 160/1000   error=0.169248\n",
      "epoch 161/1000   error=0.169341\n",
      "epoch 162/1000   error=0.169519\n",
      "epoch 163/1000   error=0.169315\n",
      "epoch 164/1000   error=0.169135\n",
      "epoch 165/1000   error=0.168974\n",
      "epoch 166/1000   error=0.168915\n",
      "epoch 167/1000   error=0.169586\n",
      "epoch 168/1000   error=0.170057\n",
      "epoch 169/1000   error=0.169755\n",
      "epoch 170/1000   error=0.169500\n",
      "epoch 171/1000   error=0.169282\n",
      "epoch 172/1000   error=0.169092\n",
      "epoch 173/1000   error=0.168926\n",
      "epoch 174/1000   error=0.168780\n",
      "epoch 175/1000   error=0.168649\n",
      "epoch 176/1000   error=0.168532\n",
      "epoch 177/1000   error=0.168426\n",
      "epoch 178/1000   error=0.168330\n",
      "epoch 179/1000   error=0.168243\n",
      "epoch 180/1000   error=0.168163\n",
      "epoch 181/1000   error=0.168090\n",
      "epoch 182/1000   error=0.168023\n",
      "epoch 183/1000   error=0.168240\n",
      "epoch 184/1000   error=0.168292\n",
      "epoch 185/1000   error=0.168205\n",
      "epoch 186/1000   error=0.168126\n",
      "epoch 187/1000   error=0.168054\n",
      "epoch 188/1000   error=0.167989\n",
      "epoch 189/1000   error=0.167929\n",
      "epoch 190/1000   error=0.167908\n",
      "epoch 191/1000   error=0.167853\n",
      "epoch 192/1000   error=0.168160\n",
      "epoch 193/1000   error=0.168596\n",
      "epoch 194/1000   error=0.168470\n",
      "epoch 195/1000   error=0.168359\n",
      "epoch 196/1000   error=0.168260\n",
      "epoch 197/1000   error=0.168172\n",
      "epoch 198/1000   error=0.168165\n",
      "epoch 199/1000   error=0.168104\n",
      "epoch 200/1000   error=0.168031\n",
      "epoch 201/1000   error=0.167964\n",
      "epoch 202/1000   error=0.167903\n",
      "epoch 203/1000   error=0.167847\n",
      "epoch 204/1000   error=0.167795\n",
      "epoch 205/1000   error=0.167748\n",
      "epoch 206/1000   error=0.167703\n",
      "epoch 207/1000   error=0.167662\n",
      "epoch 208/1000   error=0.167624\n",
      "epoch 209/1000   error=0.167589\n",
      "epoch 210/1000   error=0.167555\n",
      "epoch 211/1000   error=0.167524\n",
      "epoch 212/1000   error=0.167495\n",
      "epoch 213/1000   error=0.167467\n",
      "epoch 214/1000   error=0.167441\n",
      "epoch 215/1000   error=0.167417\n",
      "epoch 216/1000   error=0.167394\n",
      "epoch 217/1000   error=0.167372\n",
      "epoch 218/1000   error=0.167357\n",
      "epoch 219/1000   error=0.167573\n",
      "epoch 220/1000   error=0.167540\n",
      "epoch 221/1000   error=0.167509\n",
      "epoch 222/1000   error=0.167480\n",
      "epoch 223/1000   error=0.167453\n",
      "epoch 224/1000   error=0.167427\n",
      "epoch 225/1000   error=0.167403\n",
      "epoch 226/1000   error=0.167380\n",
      "epoch 227/1000   error=0.167359\n",
      "epoch 228/1000   error=0.167338\n",
      "epoch 229/1000   error=0.167319\n",
      "epoch 230/1000   error=0.167318\n",
      "epoch 231/1000   error=0.167304\n",
      "epoch 232/1000   error=0.167286\n",
      "epoch 233/1000   error=0.167270\n",
      "epoch 234/1000   error=0.167254\n",
      "epoch 235/1000   error=0.167755\n",
      "epoch 236/1000   error=0.167706\n",
      "epoch 237/1000   error=0.167661\n",
      "epoch 238/1000   error=0.167620\n",
      "epoch 239/1000   error=0.167582\n",
      "epoch 240/1000   error=0.167547\n",
      "epoch 241/1000   error=0.167514\n",
      "epoch 242/1000   error=0.167484\n",
      "epoch 243/1000   error=0.167455\n",
      "epoch 244/1000   error=0.167462\n",
      "epoch 245/1000   error=0.167456\n",
      "epoch 246/1000   error=0.167429\n",
      "epoch 247/1000   error=0.167404\n",
      "epoch 248/1000   error=0.167380\n",
      "epoch 249/1000   error=0.167358\n",
      "epoch 250/1000   error=0.167337\n",
      "epoch 251/1000   error=0.167317\n",
      "epoch 252/1000   error=0.167298\n",
      "epoch 253/1000   error=0.167281\n",
      "epoch 254/1000   error=0.167264\n",
      "epoch 255/1000   error=0.167248\n",
      "epoch 256/1000   error=0.167233\n",
      "epoch 257/1000   error=0.167218\n",
      "epoch 258/1000   error=0.167204\n",
      "epoch 259/1000   error=0.167191\n",
      "epoch 260/1000   error=0.167178\n",
      "epoch 261/1000   error=0.167166\n",
      "epoch 262/1000   error=0.167155\n",
      "epoch 263/1000   error=0.167143\n",
      "epoch 264/1000   error=0.167133\n",
      "epoch 265/1000   error=0.167123\n",
      "epoch 266/1000   error=0.167113\n",
      "epoch 267/1000   error=0.167103\n",
      "epoch 268/1000   error=0.167094\n",
      "epoch 269/1000   error=0.167086\n",
      "epoch 270/1000   error=0.167077\n",
      "epoch 271/1000   error=0.167069\n",
      "epoch 272/1000   error=0.167061\n",
      "epoch 273/1000   error=0.167054\n",
      "epoch 274/1000   error=0.167046\n",
      "epoch 275/1000   error=0.167039\n",
      "epoch 276/1000   error=0.167033\n",
      "epoch 277/1000   error=0.167026\n",
      "epoch 278/1000   error=0.167020\n",
      "epoch 279/1000   error=0.167014\n",
      "epoch 280/1000   error=0.167008\n",
      "epoch 281/1000   error=0.167002\n",
      "epoch 282/1000   error=0.166996\n",
      "epoch 283/1000   error=0.167081\n",
      "epoch 284/1000   error=0.167121\n",
      "epoch 285/1000   error=0.167111\n",
      "epoch 286/1000   error=0.167101\n",
      "epoch 287/1000   error=0.167092\n",
      "epoch 288/1000   error=0.167083\n",
      "epoch 289/1000   error=0.167075\n",
      "epoch 290/1000   error=0.167066\n",
      "epoch 291/1000   error=0.167059\n",
      "epoch 292/1000   error=0.167051\n",
      "epoch 293/1000   error=0.167044\n",
      "epoch 294/1000   error=0.167037\n",
      "epoch 295/1000   error=0.167030\n",
      "epoch 296/1000   error=0.167023\n",
      "epoch 297/1000   error=0.167017\n",
      "epoch 298/1000   error=0.167011\n",
      "epoch 299/1000   error=0.167005\n",
      "epoch 300/1000   error=0.166999\n",
      "epoch 301/1000   error=0.166994\n",
      "epoch 302/1000   error=0.166988\n",
      "epoch 303/1000   error=0.166983\n",
      "epoch 304/1000   error=0.166978\n",
      "epoch 305/1000   error=0.166979\n",
      "epoch 306/1000   error=0.166980\n",
      "epoch 307/1000   error=0.166975\n",
      "epoch 308/1000   error=0.166970\n",
      "epoch 309/1000   error=0.166965\n",
      "epoch 310/1000   error=0.166961\n",
      "epoch 311/1000   error=0.166956\n",
      "epoch 312/1000   error=0.166952\n",
      "epoch 313/1000   error=0.166948\n",
      "epoch 314/1000   error=0.166944\n",
      "epoch 315/1000   error=0.167123\n",
      "epoch 316/1000   error=0.167225\n",
      "epoch 317/1000   error=0.167210\n",
      "epoch 318/1000   error=0.167195\n",
      "epoch 319/1000   error=0.167182\n",
      "epoch 320/1000   error=0.167169\n",
      "epoch 321/1000   error=0.167156\n",
      "epoch 322/1000   error=0.167144\n",
      "epoch 323/1000   error=0.167133\n",
      "epoch 324/1000   error=0.167122\n",
      "epoch 325/1000   error=0.167112\n",
      "epoch 326/1000   error=0.167102\n",
      "epoch 327/1000   error=0.167093\n",
      "epoch 328/1000   error=0.167084\n",
      "epoch 329/1000   error=0.167075\n",
      "epoch 330/1000   error=0.167067\n",
      "epoch 331/1000   error=0.167059\n",
      "epoch 332/1000   error=0.167051\n",
      "epoch 333/1000   error=0.167054\n",
      "epoch 334/1000   error=0.167066\n",
      "epoch 335/1000   error=0.167058\n",
      "epoch 336/1000   error=0.167050\n",
      "epoch 337/1000   error=0.167042\n",
      "epoch 338/1000   error=0.167035\n",
      "epoch 339/1000   error=0.167028\n",
      "epoch 340/1000   error=0.167022\n",
      "epoch 341/1000   error=0.167015\n",
      "epoch 342/1000   error=0.167009\n",
      "epoch 343/1000   error=0.167003\n",
      "epoch 344/1000   error=0.166997\n",
      "epoch 345/1000   error=0.166991\n",
      "epoch 346/1000   error=0.166986\n",
      "epoch 347/1000   error=0.166981\n",
      "epoch 348/1000   error=0.166976\n",
      "epoch 349/1000   error=0.166971\n",
      "epoch 350/1000   error=0.166966\n",
      "epoch 351/1000   error=0.166961\n",
      "epoch 352/1000   error=0.166956\n",
      "epoch 353/1000   error=0.166952\n",
      "epoch 354/1000   error=0.166948\n",
      "epoch 355/1000   error=0.166944\n",
      "epoch 356/1000   error=0.166940\n",
      "epoch 357/1000   error=0.166936\n",
      "epoch 358/1000   error=0.166932\n",
      "epoch 359/1000   error=0.166928\n",
      "epoch 360/1000   error=0.166924\n",
      "epoch 361/1000   error=0.166921\n",
      "epoch 362/1000   error=0.166917\n",
      "epoch 363/1000   error=0.166914\n",
      "epoch 364/1000   error=0.166911\n",
      "epoch 365/1000   error=0.166907\n",
      "epoch 366/1000   error=0.166904\n",
      "epoch 367/1000   error=0.166901\n",
      "epoch 368/1000   error=0.166898\n",
      "epoch 369/1000   error=0.166895\n",
      "epoch 370/1000   error=0.166893\n",
      "epoch 371/1000   error=0.166890\n",
      "epoch 372/1000   error=0.166887\n",
      "epoch 373/1000   error=0.166884\n",
      "epoch 374/1000   error=0.166882\n",
      "epoch 375/1000   error=0.166879\n",
      "epoch 376/1000   error=0.166877\n",
      "epoch 377/1000   error=0.166874\n",
      "epoch 378/1000   error=0.166872\n",
      "epoch 379/1000   error=0.166870\n",
      "epoch 380/1000   error=0.166867\n",
      "epoch 381/1000   error=0.166865\n",
      "epoch 382/1000   error=0.166863\n",
      "epoch 383/1000   error=0.166861\n",
      "epoch 384/1000   error=0.166859\n",
      "epoch 385/1000   error=0.166857\n",
      "epoch 386/1000   error=0.166855\n",
      "epoch 387/1000   error=0.166853\n",
      "epoch 388/1000   error=0.166851\n",
      "epoch 389/1000   error=0.166849\n",
      "epoch 390/1000   error=0.166847\n",
      "epoch 391/1000   error=0.166845\n",
      "epoch 392/1000   error=0.166843\n",
      "epoch 393/1000   error=0.166842\n",
      "epoch 394/1000   error=0.166840\n",
      "epoch 395/1000   error=0.166838\n",
      "epoch 396/1000   error=0.166837\n",
      "epoch 397/1000   error=0.166835\n",
      "epoch 398/1000   error=0.166833\n",
      "epoch 399/1000   error=0.166832\n",
      "epoch 400/1000   error=0.166830\n",
      "epoch 401/1000   error=0.166829\n",
      "epoch 402/1000   error=0.166827\n",
      "epoch 403/1000   error=0.166826\n",
      "epoch 404/1000   error=0.166824\n",
      "epoch 405/1000   error=0.166823\n",
      "epoch 406/1000   error=0.166822\n",
      "epoch 407/1000   error=0.166820\n",
      "epoch 408/1000   error=0.166819\n",
      "epoch 409/1000   error=0.166818\n",
      "epoch 410/1000   error=0.166816\n",
      "epoch 411/1000   error=0.166815\n",
      "epoch 412/1000   error=0.166814\n",
      "epoch 413/1000   error=0.166812\n",
      "epoch 414/1000   error=0.166824\n",
      "epoch 415/1000   error=0.166879\n",
      "epoch 416/1000   error=0.166877\n",
      "epoch 417/1000   error=0.166874\n",
      "epoch 418/1000   error=0.166872\n",
      "epoch 419/1000   error=0.166870\n",
      "epoch 420/1000   error=0.166867\n",
      "epoch 421/1000   error=0.166865\n",
      "epoch 422/1000   error=0.166863\n",
      "epoch 423/1000   error=0.166861\n",
      "epoch 424/1000   error=0.166859\n",
      "epoch 425/1000   error=0.166856\n",
      "epoch 426/1000   error=0.166854\n",
      "epoch 427/1000   error=0.166852\n",
      "epoch 428/1000   error=0.166851\n",
      "epoch 429/1000   error=0.166849\n",
      "epoch 430/1000   error=0.166847\n",
      "epoch 431/1000   error=0.166845\n",
      "epoch 432/1000   error=0.166843\n",
      "epoch 433/1000   error=0.166841\n",
      "epoch 434/1000   error=0.166840\n",
      "epoch 435/1000   error=0.166838\n",
      "epoch 436/1000   error=0.166836\n",
      "epoch 437/1000   error=0.166835\n",
      "epoch 438/1000   error=0.166833\n",
      "epoch 439/1000   error=0.166831\n",
      "epoch 440/1000   error=0.166830\n",
      "epoch 441/1000   error=0.166828\n",
      "epoch 442/1000   error=0.166827\n",
      "epoch 443/1000   error=0.166825\n",
      "epoch 444/1000   error=0.166824\n",
      "epoch 445/1000   error=0.166822\n",
      "epoch 446/1000   error=0.166821\n",
      "epoch 447/1000   error=0.166820\n",
      "epoch 448/1000   error=0.166818\n",
      "epoch 449/1000   error=0.166817\n",
      "epoch 450/1000   error=0.166816\n",
      "epoch 451/1000   error=0.166814\n",
      "epoch 452/1000   error=0.166813\n",
      "epoch 453/1000   error=0.166812\n",
      "epoch 454/1000   error=0.166811\n",
      "epoch 455/1000   error=0.166809\n",
      "epoch 456/1000   error=0.166808\n",
      "epoch 457/1000   error=0.166807\n",
      "epoch 458/1000   error=0.166806\n",
      "epoch 459/1000   error=0.166805\n",
      "epoch 460/1000   error=0.166804\n",
      "epoch 461/1000   error=0.166803\n",
      "epoch 462/1000   error=0.166806\n",
      "epoch 463/1000   error=0.166806\n",
      "epoch 464/1000   error=0.166805\n",
      "epoch 465/1000   error=0.166804\n",
      "epoch 466/1000   error=0.166803\n",
      "epoch 467/1000   error=0.166802\n",
      "epoch 468/1000   error=0.166801\n",
      "epoch 469/1000   error=0.166800\n",
      "epoch 470/1000   error=0.166799\n",
      "epoch 471/1000   error=0.166798\n",
      "epoch 472/1000   error=0.166797\n",
      "epoch 473/1000   error=0.166796\n",
      "epoch 474/1000   error=0.166795\n",
      "epoch 475/1000   error=0.166794\n",
      "epoch 476/1000   error=0.166793\n",
      "epoch 477/1000   error=0.166792\n",
      "epoch 478/1000   error=0.166791\n",
      "epoch 479/1000   error=0.166790\n",
      "epoch 480/1000   error=0.166789\n",
      "epoch 481/1000   error=0.166788\n",
      "epoch 482/1000   error=0.166787\n",
      "epoch 483/1000   error=0.166786\n",
      "epoch 484/1000   error=0.166818\n",
      "epoch 485/1000   error=0.166933\n",
      "epoch 486/1000   error=0.166929\n",
      "epoch 487/1000   error=0.166925\n",
      "epoch 488/1000   error=0.166921\n",
      "epoch 489/1000   error=0.166918\n",
      "epoch 490/1000   error=0.166914\n",
      "epoch 491/1000   error=0.166911\n",
      "epoch 492/1000   error=0.166907\n",
      "epoch 493/1000   error=0.166904\n",
      "epoch 494/1000   error=0.166901\n",
      "epoch 495/1000   error=0.166898\n",
      "epoch 496/1000   error=0.166895\n",
      "epoch 497/1000   error=0.166892\n",
      "epoch 498/1000   error=0.166889\n",
      "epoch 499/1000   error=0.166886\n",
      "epoch 500/1000   error=0.166883\n",
      "epoch 501/1000   error=0.166881\n",
      "epoch 502/1000   error=0.166878\n",
      "epoch 503/1000   error=0.166875\n",
      "epoch 504/1000   error=0.166873\n",
      "epoch 505/1000   error=0.166871\n",
      "epoch 506/1000   error=0.166868\n",
      "epoch 507/1000   error=0.166866\n",
      "epoch 508/1000   error=0.166864\n",
      "epoch 509/1000   error=0.166861\n",
      "epoch 510/1000   error=0.166859\n",
      "epoch 511/1000   error=0.166857\n",
      "epoch 512/1000   error=0.166855\n",
      "epoch 513/1000   error=0.166853\n",
      "epoch 514/1000   error=0.166851\n",
      "epoch 515/1000   error=0.166849\n",
      "epoch 516/1000   error=0.166847\n",
      "epoch 517/1000   error=0.166845\n",
      "epoch 518/1000   error=0.166843\n",
      "epoch 519/1000   error=0.166841\n",
      "epoch 520/1000   error=0.166840\n",
      "epoch 521/1000   error=0.166838\n",
      "epoch 522/1000   error=0.166837\n",
      "epoch 523/1000   error=0.166850\n",
      "epoch 524/1000   error=0.166848\n",
      "epoch 525/1000   error=0.166846\n",
      "epoch 526/1000   error=0.166844\n",
      "epoch 527/1000   error=0.166842\n",
      "epoch 528/1000   error=0.166840\n",
      "epoch 529/1000   error=0.166838\n",
      "epoch 530/1000   error=0.166837\n",
      "epoch 531/1000   error=0.166835\n",
      "epoch 532/1000   error=0.166833\n",
      "epoch 533/1000   error=0.166832\n",
      "epoch 534/1000   error=0.166830\n",
      "epoch 535/1000   error=0.166829\n",
      "epoch 536/1000   error=0.166827\n",
      "epoch 537/1000   error=0.166825\n",
      "epoch 538/1000   error=0.166824\n",
      "epoch 539/1000   error=0.166823\n",
      "epoch 540/1000   error=0.166821\n",
      "epoch 541/1000   error=0.166820\n",
      "epoch 542/1000   error=0.166818\n",
      "epoch 543/1000   error=0.166817\n",
      "epoch 544/1000   error=0.166816\n",
      "epoch 545/1000   error=0.166814\n",
      "epoch 546/1000   error=0.166813\n",
      "epoch 547/1000   error=0.166812\n",
      "epoch 548/1000   error=0.166810\n",
      "epoch 549/1000   error=0.166809\n",
      "epoch 550/1000   error=0.166808\n",
      "epoch 551/1000   error=0.166807\n",
      "epoch 552/1000   error=0.166806\n",
      "epoch 553/1000   error=0.166804\n",
      "epoch 554/1000   error=0.166803\n",
      "epoch 555/1000   error=0.166802\n",
      "epoch 556/1000   error=0.166801\n",
      "epoch 557/1000   error=0.166800\n",
      "epoch 558/1000   error=0.166799\n",
      "epoch 559/1000   error=0.166798\n",
      "epoch 560/1000   error=0.166797\n",
      "epoch 561/1000   error=0.166796\n",
      "epoch 562/1000   error=0.166795\n",
      "epoch 563/1000   error=0.166794\n",
      "epoch 564/1000   error=0.166793\n",
      "epoch 565/1000   error=0.166792\n",
      "epoch 566/1000   error=0.166791\n",
      "epoch 567/1000   error=0.166790\n",
      "epoch 568/1000   error=0.166789\n",
      "epoch 569/1000   error=0.166788\n",
      "epoch 570/1000   error=0.166787\n",
      "epoch 571/1000   error=0.166786\n",
      "epoch 572/1000   error=0.166786\n",
      "epoch 573/1000   error=0.166785\n",
      "epoch 574/1000   error=0.166784\n",
      "epoch 575/1000   error=0.166783\n",
      "epoch 576/1000   error=0.166782\n",
      "epoch 577/1000   error=0.166781\n",
      "epoch 578/1000   error=0.166781\n",
      "epoch 579/1000   error=0.166780\n",
      "epoch 580/1000   error=0.166779\n",
      "epoch 581/1000   error=0.166778\n",
      "epoch 582/1000   error=0.166777\n",
      "epoch 583/1000   error=0.166777\n",
      "epoch 584/1000   error=0.166776\n",
      "epoch 585/1000   error=0.166775\n",
      "epoch 586/1000   error=0.166774\n",
      "epoch 587/1000   error=0.166774\n",
      "epoch 588/1000   error=0.166773\n",
      "epoch 589/1000   error=0.166772\n",
      "epoch 590/1000   error=0.166772\n",
      "epoch 591/1000   error=0.166771\n",
      "epoch 592/1000   error=0.166770\n",
      "epoch 593/1000   error=0.166770\n",
      "epoch 594/1000   error=0.166769\n",
      "epoch 595/1000   error=0.166768\n",
      "epoch 596/1000   error=0.166768\n",
      "epoch 597/1000   error=0.166767\n",
      "epoch 598/1000   error=0.166766\n",
      "epoch 599/1000   error=0.166766\n",
      "epoch 600/1000   error=0.166765\n",
      "epoch 601/1000   error=0.166765\n",
      "epoch 602/1000   error=0.166764\n",
      "epoch 603/1000   error=0.166763\n",
      "epoch 604/1000   error=0.166763\n",
      "epoch 605/1000   error=0.166762\n",
      "epoch 606/1000   error=0.166762\n",
      "epoch 607/1000   error=0.166761\n",
      "epoch 608/1000   error=0.166761\n",
      "epoch 609/1000   error=0.166760\n",
      "epoch 610/1000   error=0.166759\n",
      "epoch 611/1000   error=0.166759\n",
      "epoch 612/1000   error=0.166758\n",
      "epoch 613/1000   error=0.166758\n",
      "epoch 614/1000   error=0.166757\n",
      "epoch 615/1000   error=0.166757\n",
      "epoch 616/1000   error=0.166756\n",
      "epoch 617/1000   error=0.166756\n",
      "epoch 618/1000   error=0.166755\n",
      "epoch 619/1000   error=0.166755\n",
      "epoch 620/1000   error=0.166754\n",
      "epoch 621/1000   error=0.166754\n",
      "epoch 622/1000   error=0.166753\n",
      "epoch 623/1000   error=0.166753\n",
      "epoch 624/1000   error=0.166752\n",
      "epoch 625/1000   error=0.166752\n",
      "epoch 626/1000   error=0.166752\n",
      "epoch 627/1000   error=0.166751\n",
      "epoch 628/1000   error=0.166751\n",
      "epoch 629/1000   error=0.166750\n",
      "epoch 630/1000   error=0.166750\n",
      "epoch 631/1000   error=0.166749\n",
      "epoch 632/1000   error=0.166749\n",
      "epoch 633/1000   error=0.166748\n",
      "epoch 634/1000   error=0.166748\n",
      "epoch 635/1000   error=0.166748\n",
      "epoch 636/1000   error=0.166747\n",
      "epoch 637/1000   error=0.166747\n",
      "epoch 638/1000   error=0.166746\n",
      "epoch 639/1000   error=0.166746\n",
      "epoch 640/1000   error=0.166746\n",
      "epoch 641/1000   error=0.166745\n",
      "epoch 642/1000   error=0.166745\n",
      "epoch 643/1000   error=0.166744\n",
      "epoch 644/1000   error=0.166744\n",
      "epoch 645/1000   error=0.166744\n",
      "epoch 646/1000   error=0.166743\n",
      "epoch 647/1000   error=0.166743\n",
      "epoch 648/1000   error=0.166743\n",
      "epoch 649/1000   error=0.166742\n",
      "epoch 650/1000   error=0.166742\n",
      "epoch 651/1000   error=0.166741\n",
      "epoch 652/1000   error=0.166741\n",
      "epoch 653/1000   error=0.166741\n",
      "epoch 654/1000   error=0.166740\n",
      "epoch 655/1000   error=0.166740\n",
      "epoch 656/1000   error=0.166740\n",
      "epoch 657/1000   error=0.166739\n",
      "epoch 658/1000   error=0.166739\n",
      "epoch 659/1000   error=0.166739\n",
      "epoch 660/1000   error=0.166738\n",
      "epoch 661/1000   error=0.166738\n",
      "epoch 662/1000   error=0.166738\n",
      "epoch 663/1000   error=0.166737\n",
      "epoch 664/1000   error=0.166737\n",
      "epoch 665/1000   error=0.166737\n",
      "epoch 666/1000   error=0.166736\n",
      "epoch 667/1000   error=0.166736\n",
      "epoch 668/1000   error=0.166736\n",
      "epoch 669/1000   error=0.166735\n",
      "epoch 670/1000   error=0.166735\n",
      "epoch 671/1000   error=0.166735\n",
      "epoch 672/1000   error=0.166735\n",
      "epoch 673/1000   error=0.166734\n",
      "epoch 674/1000   error=0.166734\n",
      "epoch 675/1000   error=0.166734\n",
      "epoch 676/1000   error=0.166733\n",
      "epoch 677/1000   error=0.166733\n",
      "epoch 678/1000   error=0.166733\n",
      "epoch 679/1000   error=0.166733\n",
      "epoch 680/1000   error=0.166732\n",
      "epoch 681/1000   error=0.166732\n",
      "epoch 682/1000   error=0.166732\n",
      "epoch 683/1000   error=0.166731\n",
      "epoch 684/1000   error=0.166731\n",
      "epoch 685/1000   error=0.166731\n",
      "epoch 686/1000   error=0.166731\n",
      "epoch 687/1000   error=0.166730\n",
      "epoch 688/1000   error=0.166730\n",
      "epoch 689/1000   error=0.166730\n",
      "epoch 690/1000   error=0.166730\n",
      "epoch 691/1000   error=0.166729\n",
      "epoch 692/1000   error=0.166729\n",
      "epoch 693/1000   error=0.166729\n",
      "epoch 694/1000   error=0.166729\n",
      "epoch 695/1000   error=0.166728\n",
      "epoch 696/1000   error=0.166728\n",
      "epoch 697/1000   error=0.166728\n",
      "epoch 698/1000   error=0.166728\n",
      "epoch 699/1000   error=0.166727\n",
      "epoch 700/1000   error=0.166727\n",
      "epoch 701/1000   error=0.166727\n",
      "epoch 702/1000   error=0.166727\n",
      "epoch 703/1000   error=0.166726\n",
      "epoch 704/1000   error=0.166726\n",
      "epoch 705/1000   error=0.166726\n",
      "epoch 706/1000   error=0.166726\n",
      "epoch 707/1000   error=0.166725\n",
      "epoch 708/1000   error=0.166725\n",
      "epoch 709/1000   error=0.166725\n",
      "epoch 710/1000   error=0.166743\n",
      "epoch 711/1000   error=0.166757\n",
      "epoch 712/1000   error=0.166756\n",
      "epoch 713/1000   error=0.166756\n",
      "epoch 714/1000   error=0.166755\n",
      "epoch 715/1000   error=0.166755\n",
      "epoch 716/1000   error=0.166754\n",
      "epoch 717/1000   error=0.166754\n",
      "epoch 718/1000   error=0.166753\n",
      "epoch 719/1000   error=0.166753\n",
      "epoch 720/1000   error=0.166752\n",
      "epoch 721/1000   error=0.166752\n",
      "epoch 722/1000   error=0.166751\n",
      "epoch 723/1000   error=0.166751\n",
      "epoch 724/1000   error=0.166750\n",
      "epoch 725/1000   error=0.166750\n",
      "epoch 726/1000   error=0.166750\n",
      "epoch 727/1000   error=0.166749\n",
      "epoch 728/1000   error=0.166749\n",
      "epoch 729/1000   error=0.166748\n",
      "epoch 730/1000   error=0.166748\n",
      "epoch 731/1000   error=0.166747\n",
      "epoch 732/1000   error=0.166747\n",
      "epoch 733/1000   error=0.166747\n",
      "epoch 734/1000   error=0.166746\n",
      "epoch 735/1000   error=0.166746\n",
      "epoch 736/1000   error=0.166745\n",
      "epoch 737/1000   error=0.166745\n",
      "epoch 738/1000   error=0.166745\n",
      "epoch 739/1000   error=0.166744\n",
      "epoch 740/1000   error=0.166744\n",
      "epoch 741/1000   error=0.166743\n",
      "epoch 742/1000   error=0.166743\n",
      "epoch 743/1000   error=0.166743\n",
      "epoch 744/1000   error=0.166742\n",
      "epoch 745/1000   error=0.166742\n",
      "epoch 746/1000   error=0.166742\n",
      "epoch 747/1000   error=0.166741\n",
      "epoch 748/1000   error=0.166741\n",
      "epoch 749/1000   error=0.166740\n",
      "epoch 750/1000   error=0.166740\n",
      "epoch 751/1000   error=0.166740\n",
      "epoch 752/1000   error=0.166739\n",
      "epoch 753/1000   error=0.166739\n",
      "epoch 754/1000   error=0.166739\n",
      "epoch 755/1000   error=0.166738\n",
      "epoch 756/1000   error=0.166738\n",
      "epoch 757/1000   error=0.166738\n",
      "epoch 758/1000   error=0.166737\n",
      "epoch 759/1000   error=0.166737\n",
      "epoch 760/1000   error=0.166737\n",
      "epoch 761/1000   error=0.166736\n",
      "epoch 762/1000   error=0.166736\n",
      "epoch 763/1000   error=0.166736\n",
      "epoch 764/1000   error=0.166736\n",
      "epoch 765/1000   error=0.166735\n",
      "epoch 766/1000   error=0.166735\n",
      "epoch 767/1000   error=0.166735\n",
      "epoch 768/1000   error=0.166734\n",
      "epoch 769/1000   error=0.166734\n",
      "epoch 770/1000   error=0.166734\n",
      "epoch 771/1000   error=0.166733\n",
      "epoch 772/1000   error=0.166733\n",
      "epoch 773/1000   error=0.166733\n",
      "epoch 774/1000   error=0.166733\n",
      "epoch 775/1000   error=0.166732\n",
      "epoch 776/1000   error=0.166732\n",
      "epoch 777/1000   error=0.166732\n",
      "epoch 778/1000   error=0.166731\n",
      "epoch 779/1000   error=0.166731\n",
      "epoch 780/1000   error=0.166731\n",
      "epoch 781/1000   error=0.166731\n",
      "epoch 782/1000   error=0.166730\n",
      "epoch 783/1000   error=0.166730\n",
      "epoch 784/1000   error=0.166730\n",
      "epoch 785/1000   error=0.166730\n",
      "epoch 786/1000   error=0.166729\n",
      "epoch 787/1000   error=0.166729\n",
      "epoch 788/1000   error=0.166729\n",
      "epoch 789/1000   error=0.166729\n",
      "epoch 790/1000   error=0.166728\n",
      "epoch 791/1000   error=0.166728\n",
      "epoch 792/1000   error=0.166728\n",
      "epoch 793/1000   error=0.166728\n",
      "epoch 794/1000   error=0.166727\n",
      "epoch 795/1000   error=0.166727\n",
      "epoch 796/1000   error=0.166727\n",
      "epoch 797/1000   error=0.166727\n",
      "epoch 798/1000   error=0.166726\n",
      "epoch 799/1000   error=0.166726\n",
      "epoch 800/1000   error=0.166726\n",
      "epoch 801/1000   error=0.166726\n",
      "epoch 802/1000   error=0.166725\n",
      "epoch 803/1000   error=0.166725\n",
      "epoch 804/1000   error=0.166725\n",
      "epoch 805/1000   error=0.166725\n",
      "epoch 806/1000   error=0.166725\n",
      "epoch 807/1000   error=0.166724\n",
      "epoch 808/1000   error=0.166724\n",
      "epoch 809/1000   error=0.166724\n",
      "epoch 810/1000   error=0.166724\n",
      "epoch 811/1000   error=0.166723\n",
      "epoch 812/1000   error=0.166723\n",
      "epoch 813/1000   error=0.166723\n",
      "epoch 814/1000   error=0.166723\n",
      "epoch 815/1000   error=0.166723\n",
      "epoch 816/1000   error=0.166722\n",
      "epoch 817/1000   error=0.166722\n",
      "epoch 818/1000   error=0.166722\n",
      "epoch 819/1000   error=0.166722\n",
      "epoch 820/1000   error=0.166722\n",
      "epoch 821/1000   error=0.166721\n",
      "epoch 822/1000   error=0.166721\n",
      "epoch 823/1000   error=0.166721\n",
      "epoch 824/1000   error=0.166721\n",
      "epoch 825/1000   error=0.166723\n",
      "epoch 826/1000   error=0.166723\n",
      "epoch 827/1000   error=0.166723\n",
      "epoch 828/1000   error=0.166723\n",
      "epoch 829/1000   error=0.166722\n",
      "epoch 830/1000   error=0.166722\n",
      "epoch 831/1000   error=0.166722\n",
      "epoch 832/1000   error=0.166722\n",
      "epoch 833/1000   error=0.166722\n",
      "epoch 834/1000   error=0.166721\n",
      "epoch 835/1000   error=0.166721\n",
      "epoch 836/1000   error=0.166721\n",
      "epoch 837/1000   error=0.166721\n",
      "epoch 838/1000   error=0.166721\n",
      "epoch 839/1000   error=0.166720\n",
      "epoch 840/1000   error=0.166720\n",
      "epoch 841/1000   error=0.166720\n",
      "epoch 842/1000   error=0.166720\n",
      "epoch 843/1000   error=0.166720\n",
      "epoch 844/1000   error=0.166719\n",
      "epoch 845/1000   error=0.166719\n",
      "epoch 846/1000   error=0.166719\n",
      "epoch 847/1000   error=0.166719\n",
      "epoch 848/1000   error=0.166719\n",
      "epoch 849/1000   error=0.166718\n",
      "epoch 850/1000   error=0.166718\n",
      "epoch 851/1000   error=0.166718\n",
      "epoch 852/1000   error=0.166718\n",
      "epoch 853/1000   error=0.166718\n",
      "epoch 854/1000   error=0.166718\n",
      "epoch 855/1000   error=0.166717\n",
      "epoch 856/1000   error=0.166717\n",
      "epoch 857/1000   error=0.166717\n",
      "epoch 858/1000   error=0.166717\n",
      "epoch 859/1000   error=0.166717\n",
      "epoch 860/1000   error=0.166717\n",
      "epoch 861/1000   error=0.166716\n",
      "epoch 862/1000   error=0.166716\n",
      "epoch 863/1000   error=0.166716\n",
      "epoch 864/1000   error=0.166716\n",
      "epoch 865/1000   error=0.166716\n",
      "epoch 866/1000   error=0.166716\n",
      "epoch 867/1000   error=0.166715\n",
      "epoch 868/1000   error=0.166715\n",
      "epoch 869/1000   error=0.166715\n",
      "epoch 870/1000   error=0.166715\n",
      "epoch 871/1000   error=0.166715\n",
      "epoch 872/1000   error=0.166715\n",
      "epoch 873/1000   error=0.166714\n",
      "epoch 874/1000   error=0.166714\n",
      "epoch 875/1000   error=0.166714\n",
      "epoch 876/1000   error=0.166714\n",
      "epoch 877/1000   error=0.166714\n",
      "epoch 878/1000   error=0.166714\n",
      "epoch 879/1000   error=0.166714\n",
      "epoch 880/1000   error=0.166713\n",
      "epoch 881/1000   error=0.166739\n",
      "epoch 882/1000   error=0.166782\n",
      "epoch 883/1000   error=0.166781\n",
      "epoch 884/1000   error=0.166780\n",
      "epoch 885/1000   error=0.166780\n",
      "epoch 886/1000   error=0.166779\n",
      "epoch 887/1000   error=0.166778\n",
      "epoch 888/1000   error=0.166777\n",
      "epoch 889/1000   error=0.166776\n",
      "epoch 890/1000   error=0.166776\n",
      "epoch 891/1000   error=0.166775\n",
      "epoch 892/1000   error=0.166774\n",
      "epoch 893/1000   error=0.166773\n",
      "epoch 894/1000   error=0.166773\n",
      "epoch 895/1000   error=0.166772\n",
      "epoch 896/1000   error=0.166771\n",
      "epoch 897/1000   error=0.166770\n",
      "epoch 898/1000   error=0.166770\n",
      "epoch 899/1000   error=0.166769\n",
      "epoch 900/1000   error=0.166768\n",
      "epoch 901/1000   error=0.166768\n",
      "epoch 902/1000   error=0.166767\n",
      "epoch 903/1000   error=0.166766\n",
      "epoch 904/1000   error=0.166766\n",
      "epoch 905/1000   error=0.166765\n",
      "epoch 906/1000   error=0.166765\n",
      "epoch 907/1000   error=0.166764\n",
      "epoch 908/1000   error=0.166763\n",
      "epoch 909/1000   error=0.166763\n",
      "epoch 910/1000   error=0.166762\n",
      "epoch 911/1000   error=0.166762\n",
      "epoch 912/1000   error=0.166761\n",
      "epoch 913/1000   error=0.166760\n",
      "epoch 914/1000   error=0.166760\n",
      "epoch 915/1000   error=0.166759\n",
      "epoch 916/1000   error=0.166759\n",
      "epoch 917/1000   error=0.166758\n",
      "epoch 918/1000   error=0.166758\n",
      "epoch 919/1000   error=0.166757\n",
      "epoch 920/1000   error=0.166757\n",
      "epoch 921/1000   error=0.166756\n",
      "epoch 922/1000   error=0.166755\n",
      "epoch 923/1000   error=0.166755\n",
      "epoch 924/1000   error=0.166754\n",
      "epoch 925/1000   error=0.166754\n",
      "epoch 926/1000   error=0.166753\n",
      "epoch 927/1000   error=0.166753\n",
      "epoch 928/1000   error=0.166752\n",
      "epoch 929/1000   error=0.166752\n",
      "epoch 930/1000   error=0.166752\n",
      "epoch 931/1000   error=0.166751\n",
      "epoch 932/1000   error=0.166751\n",
      "epoch 933/1000   error=0.166750\n",
      "epoch 934/1000   error=0.166750\n",
      "epoch 935/1000   error=0.166749\n",
      "epoch 936/1000   error=0.166749\n",
      "epoch 937/1000   error=0.166748\n",
      "epoch 938/1000   error=0.166748\n",
      "epoch 939/1000   error=0.166747\n",
      "epoch 940/1000   error=0.166747\n",
      "epoch 941/1000   error=0.166747\n",
      "epoch 942/1000   error=0.166746\n",
      "epoch 943/1000   error=0.166746\n",
      "epoch 944/1000   error=0.166745\n",
      "epoch 945/1000   error=0.166745\n",
      "epoch 946/1000   error=0.166745\n",
      "epoch 947/1000   error=0.166744\n",
      "epoch 948/1000   error=0.166744\n",
      "epoch 949/1000   error=0.166743\n",
      "epoch 950/1000   error=0.166743\n",
      "epoch 951/1000   error=0.166743\n",
      "epoch 952/1000   error=0.166742\n",
      "epoch 953/1000   error=0.166742\n",
      "epoch 954/1000   error=0.166741\n",
      "epoch 955/1000   error=0.166741\n",
      "epoch 956/1000   error=0.166741\n",
      "epoch 957/1000   error=0.166740\n",
      "epoch 958/1000   error=0.166740\n",
      "epoch 959/1000   error=0.166740\n",
      "epoch 960/1000   error=0.166739\n",
      "epoch 961/1000   error=0.166739\n",
      "epoch 962/1000   error=0.166739\n",
      "epoch 963/1000   error=0.166738\n",
      "epoch 964/1000   error=0.166738\n",
      "epoch 965/1000   error=0.166738\n",
      "epoch 966/1000   error=0.166737\n",
      "epoch 967/1000   error=0.166737\n",
      "epoch 968/1000   error=0.166737\n",
      "epoch 969/1000   error=0.166736\n",
      "epoch 970/1000   error=0.166736\n",
      "epoch 971/1000   error=0.166740\n",
      "epoch 972/1000   error=0.166742\n",
      "epoch 973/1000   error=0.166742\n",
      "epoch 974/1000   error=0.166741\n",
      "epoch 975/1000   error=0.166741\n",
      "epoch 976/1000   error=0.166741\n",
      "epoch 977/1000   error=0.166740\n",
      "epoch 978/1000   error=0.166740\n",
      "epoch 979/1000   error=0.166740\n",
      "epoch 980/1000   error=0.166739\n",
      "epoch 981/1000   error=0.166739\n",
      "epoch 982/1000   error=0.166739\n",
      "epoch 983/1000   error=0.166738\n",
      "epoch 984/1000   error=0.166738\n",
      "epoch 985/1000   error=0.166738\n",
      "epoch 986/1000   error=0.166737\n",
      "epoch 987/1000   error=0.166737\n",
      "epoch 988/1000   error=0.166737\n",
      "epoch 989/1000   error=0.166736\n",
      "epoch 990/1000   error=0.166736\n",
      "epoch 991/1000   error=0.166736\n",
      "epoch 992/1000   error=0.166735\n",
      "epoch 993/1000   error=0.166735\n",
      "epoch 994/1000   error=0.166735\n",
      "epoch 995/1000   error=0.166734\n",
      "epoch 996/1000   error=0.166734\n",
      "epoch 997/1000   error=0.166734\n",
      "epoch 998/1000   error=0.166733\n",
      "epoch 999/1000   error=0.166733\n",
      "epoch 1000/1000   error=0.166733\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "epochs = 1000\n",
    "learning_rate = 1\n",
    "\n",
    "network.train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    mse,\n",
    "    mse_prime,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, you‚Äôll see the **error** (MSE) decrease and eventually become very small, signaling that the network has learned the XOR mapping. A sample training output might look like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "epoch 1/1000   error=0.312980\n",
    "epoch 2/1000   error=0.255174\n",
    "...\n",
    "epoch 998/1000 error=0.000243\n",
    "epoch 999/1000 error=0.000241\n",
    "epoch 1000/1000 error=0.000240\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_4_'></a>[Verifying the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can check the network‚Äôs predictions on each XOR input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.33374217]\n",
      " [0.33374217]\n",
      " [0.98378042]\n",
      " [0.33374217]]\n"
     ]
    }
   ],
   "source": [
    "output = network.forward(x_train)\n",
    "print(\"Predictions:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we obtain something very close to:\n",
    "\n",
    "- (0, 0) ‚Üí ~0\n",
    "- (0, 1) ‚Üí ~1\n",
    "- (1, 0) ‚Üí ~1\n",
    "- (1, 1) ‚Üí ~0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that our small network‚Äîwith a hidden layer and non-linear activation‚Äîcan indeed solve the XOR problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By starting with XOR, we confirm that our **forward propagation**, **backpropagation**, and **layer architecture** can learn a classic non-linear mapping. This same foundation allows us to tackle more complex tasks‚Äîsuch as image classification‚Äîsimply by adding more layers and larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_'></a>[Example 2: Solving MNIST](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While XOR demonstrates our network can learn a simple non-linear function, it‚Äôs another leap to solve a more ‚Äúreal-world‚Äù task like **recognizing handwritten digits**. The **MNIST** dataset is a classic benchmark, consisting of 28√ó28 grayscale images of digits from 0 to 9. Below, we‚Äôll use our from-scratch framework to classify these images‚Äîdespite not having implemented **Convolutional** layers, we can still do a decent job by flattening each 28√ó28 image into a single 784-dimensional vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mnist.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_'></a>[Dataset Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Ä¢ MNIST images are 28√ó28 pixels, giving 784 intensity values per image.  \n",
    "‚Ä¢ We have 10 possible digit classes (0 through 9).  \n",
    "‚Ä¢ Each data sample can be reshaped from (28, 28) to (784,).  \n",
    "‚Ä¢ For output, we often use a **one-hot** encoding: each label is a 10-dimensional vector with a 1 at the index of the correct class and 0 elsewhere.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your data source, you might load and preprocess MNIST in different ways. A simple approach involves using libraries like **scikit-learn** or **TensorFlow Keras** to fetch and reshape the dataset, or you can load it manually from a file (e.g., ‚Äútrain-images.idx3-ubyte‚Äù and ‚Äútrain-labels.idx1-ubyte‚Äù). After loading:\n",
    "\n",
    "1. **Flatten each image** into a 1D vector of length 784.  \n",
    "2. **Normalize** the pixel values to [0,1] or [-1,1].  \n",
    "3. **Convert labels to one-hot vectors** of length 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal example with scikit-learn might look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch MNIST via scikit-learn (this may be slow on first run)\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "X = mnist.data.values  # Shape: (70000, 784)\n",
    "y = mnist.target.values  # Shape: (70000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to int\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "num_classes = 10\n",
    "y_onehot = np.zeros((y.size, num_classes))\n",
    "y_onehot[np.arange(y.size), y] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split into training/validation sets as needed\n",
    "x_train = X[:60000]\n",
    "y_train = y_onehot[:60000]\n",
    "x_test = X[60000:]\n",
    "y_test = y_onehot[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digits(X, num_images=10, figsize=(15,3)):\n",
    "    # Create a figure with specified size\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot multiple images\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        # Reshape the image from 784 pixels to 28x28\n",
    "        img = X[i].reshape(28, 28)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize first 10 digits from training set\n",
    "plot_digits(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can also visualize specific digits\n",
    "def plot_digit(X, index):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    img = X[index].reshape(28, 28)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a specific digit (e.g., the first one)\n",
    "plot_digit(x_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To visualize random digits:\n",
    "random_indices = np.random.randint(0, len(x_train), 10)\n",
    "plot_digits(x_train[random_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if you load MNIST differently (e.g., direct from files), the result should be **two NumPy arrays**: one for training inputs (`x_train`) and one for training labels (`y_train`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_2_'></a>[Flattening Images for Input](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our **Fully Connected** layers, the input dimension needs to match the feature vector size. Thus, each MNIST image, originally 28√ó28, becomes a flat 784-element vector. Because we handled that with the above approach, each sample is already shape $(784,)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_3_'></a>[Network Configuration and Training](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse our previous architecture without modification. However, for MNIST, we might want deeper or wider layers:\n",
    "\n",
    "1. **Fully Connected Layer** mapping 784 inputs to, say, 64 or 128 hidden neurons.  \n",
    "2. **Activation Layer** (e.g., ReLU).  \n",
    "3. Another **Fully Connected Layer** down to 10 outputs (one for each digit class).  \n",
    "4. **Softmax** or **Sigmoid** activation for classification.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of how you might assemble the network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "network.add(FCLayer(784, 64))  # 784 -> 64\n",
    "network.add(ActivationLayer(relu, relu_prime))\n",
    "network.add(FCLayer(64, 10))   # 64 -> 10\n",
    "network.add(ActivationLayer(sigmoid, sigmoid_prime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **Sigmoid** on each of the 10 outputs could work, but a more typical approach is to use a **Softmax** activation with a cross-entropy loss for multi-class classification. For the sake of simplicity (and since we are building from scratch), we might rely on Sigmoid or ReLU-based final outputs. However, we must interpret the results carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical training loop may look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.092102\n",
      "epoch 2/1000   error=0.092066\n",
      "epoch 3/1000   error=0.092035\n",
      "epoch 4/1000   error=0.092007\n",
      "epoch 5/1000   error=0.091981\n",
      "epoch 6/1000   error=0.091959\n",
      "epoch 7/1000   error=0.091938\n",
      "epoch 8/1000   error=0.091919\n",
      "epoch 9/1000   error=0.091901\n",
      "epoch 10/1000   error=0.091885\n",
      "epoch 11/1000   error=0.091870\n",
      "epoch 12/1000   error=0.091855\n",
      "epoch 13/1000   error=0.091841\n",
      "epoch 14/1000   error=0.091828\n",
      "epoch 15/1000   error=0.091816\n",
      "epoch 16/1000   error=0.091803\n",
      "epoch 17/1000   error=0.091792\n",
      "epoch 18/1000   error=0.091780\n",
      "epoch 19/1000   error=0.091769\n",
      "epoch 20/1000   error=0.091758\n",
      "epoch 21/1000   error=0.091748\n",
      "epoch 22/1000   error=0.091738\n",
      "epoch 23/1000   error=0.091727\n",
      "epoch 24/1000   error=0.091717\n",
      "epoch 25/1000   error=0.091707\n",
      "epoch 26/1000   error=0.091698\n",
      "epoch 27/1000   error=0.091688\n",
      "epoch 28/1000   error=0.091678\n",
      "epoch 29/1000   error=0.091669\n",
      "epoch 30/1000   error=0.091659\n",
      "epoch 31/1000   error=0.091650\n",
      "epoch 32/1000   error=0.091640\n",
      "epoch 33/1000   error=0.091631\n",
      "epoch 34/1000   error=0.091621\n",
      "epoch 35/1000   error=0.091612\n",
      "epoch 36/1000   error=0.091602\n",
      "epoch 37/1000   error=0.091593\n",
      "epoch 38/1000   error=0.091583\n",
      "epoch 39/1000   error=0.091574\n",
      "epoch 40/1000   error=0.091564\n",
      "epoch 41/1000   error=0.091554\n",
      "epoch 42/1000   error=0.091545\n",
      "epoch 43/1000   error=0.091535\n",
      "epoch 44/1000   error=0.091525\n",
      "epoch 45/1000   error=0.091515\n",
      "epoch 46/1000   error=0.091505\n",
      "epoch 47/1000   error=0.091495\n",
      "epoch 48/1000   error=0.091485\n",
      "epoch 49/1000   error=0.091475\n",
      "epoch 50/1000   error=0.091465\n",
      "epoch 51/1000   error=0.091454\n",
      "epoch 52/1000   error=0.091444\n",
      "epoch 53/1000   error=0.091433\n",
      "epoch 54/1000   error=0.091423\n",
      "epoch 55/1000   error=0.091412\n",
      "epoch 56/1000   error=0.091401\n",
      "epoch 57/1000   error=0.091390\n",
      "epoch 58/1000   error=0.091379\n",
      "epoch 59/1000   error=0.091368\n",
      "epoch 60/1000   error=0.091356\n",
      "epoch 61/1000   error=0.091345\n",
      "epoch 62/1000   error=0.091333\n",
      "epoch 63/1000   error=0.091321\n",
      "epoch 64/1000   error=0.091309\n",
      "epoch 65/1000   error=0.091297\n",
      "epoch 66/1000   error=0.091285\n",
      "epoch 67/1000   error=0.091272\n",
      "epoch 68/1000   error=0.091259\n",
      "epoch 69/1000   error=0.091247\n",
      "epoch 70/1000   error=0.091234\n",
      "epoch 71/1000   error=0.091220\n",
      "epoch 72/1000   error=0.091207\n",
      "epoch 73/1000   error=0.091193\n",
      "epoch 74/1000   error=0.091180\n",
      "epoch 75/1000   error=0.091166\n",
      "epoch 76/1000   error=0.091151\n",
      "epoch 77/1000   error=0.091137\n",
      "epoch 78/1000   error=0.091122\n",
      "epoch 79/1000   error=0.091107\n",
      "epoch 80/1000   error=0.091092\n",
      "epoch 81/1000   error=0.091077\n",
      "epoch 82/1000   error=0.091061\n",
      "epoch 83/1000   error=0.091045\n",
      "epoch 84/1000   error=0.091029\n",
      "epoch 85/1000   error=0.091013\n",
      "epoch 86/1000   error=0.090996\n",
      "epoch 87/1000   error=0.090979\n",
      "epoch 88/1000   error=0.090962\n",
      "epoch 89/1000   error=0.090944\n",
      "epoch 90/1000   error=0.090926\n",
      "epoch 91/1000   error=0.090908\n",
      "epoch 92/1000   error=0.090889\n",
      "epoch 93/1000   error=0.090870\n",
      "epoch 94/1000   error=0.090851\n",
      "epoch 95/1000   error=0.090831\n",
      "epoch 96/1000   error=0.090811\n",
      "epoch 97/1000   error=0.090791\n",
      "epoch 98/1000   error=0.090770\n",
      "epoch 99/1000   error=0.090749\n",
      "epoch 100/1000   error=0.090727\n",
      "epoch 101/1000   error=0.090705\n",
      "epoch 102/1000   error=0.090682\n",
      "epoch 103/1000   error=0.090659\n",
      "epoch 104/1000   error=0.090636\n",
      "epoch 105/1000   error=0.090612\n",
      "epoch 106/1000   error=0.090588\n",
      "epoch 107/1000   error=0.090563\n",
      "epoch 108/1000   error=0.090537\n",
      "epoch 109/1000   error=0.090511\n",
      "epoch 110/1000   error=0.090485\n",
      "epoch 111/1000   error=0.090458\n",
      "epoch 112/1000   error=0.090430\n",
      "epoch 113/1000   error=0.090402\n",
      "epoch 114/1000   error=0.090373\n",
      "epoch 115/1000   error=0.090344\n",
      "epoch 116/1000   error=0.090314\n",
      "epoch 117/1000   error=0.090283\n",
      "epoch 118/1000   error=0.090251\n",
      "epoch 119/1000   error=0.090219\n",
      "epoch 120/1000   error=0.090186\n",
      "epoch 121/1000   error=0.090152\n",
      "epoch 122/1000   error=0.090118\n",
      "epoch 123/1000   error=0.090083\n",
      "epoch 124/1000   error=0.090046\n",
      "epoch 125/1000   error=0.090009\n",
      "epoch 126/1000   error=0.089972\n",
      "epoch 127/1000   error=0.089933\n",
      "epoch 128/1000   error=0.089893\n",
      "epoch 129/1000   error=0.089853\n",
      "epoch 130/1000   error=0.089811\n",
      "epoch 131/1000   error=0.089769\n",
      "epoch 132/1000   error=0.089725\n",
      "epoch 133/1000   error=0.089680\n",
      "epoch 134/1000   error=0.089634\n",
      "epoch 135/1000   error=0.089587\n",
      "epoch 136/1000   error=0.089539\n",
      "epoch 137/1000   error=0.089490\n",
      "epoch 138/1000   error=0.089439\n",
      "epoch 139/1000   error=0.089388\n",
      "epoch 140/1000   error=0.089334\n",
      "epoch 141/1000   error=0.089280\n",
      "epoch 142/1000   error=0.089224\n",
      "epoch 143/1000   error=0.089167\n",
      "epoch 144/1000   error=0.089108\n",
      "epoch 145/1000   error=0.089048\n",
      "epoch 146/1000   error=0.088986\n",
      "epoch 147/1000   error=0.088922\n",
      "epoch 148/1000   error=0.088857\n",
      "epoch 149/1000   error=0.088791\n",
      "epoch 150/1000   error=0.088722\n",
      "epoch 151/1000   error=0.088652\n",
      "epoch 152/1000   error=0.088580\n",
      "epoch 153/1000   error=0.088506\n",
      "epoch 154/1000   error=0.088430\n",
      "epoch 155/1000   error=0.088353\n",
      "epoch 156/1000   error=0.088273\n",
      "epoch 157/1000   error=0.088191\n",
      "epoch 158/1000   error=0.088108\n",
      "epoch 159/1000   error=0.088022\n",
      "epoch 160/1000   error=0.087934\n",
      "epoch 161/1000   error=0.087844\n",
      "epoch 162/1000   error=0.087751\n",
      "epoch 163/1000   error=0.087657\n",
      "epoch 164/1000   error=0.087560\n",
      "epoch 165/1000   error=0.087461\n",
      "epoch 166/1000   error=0.087359\n",
      "epoch 167/1000   error=0.087255\n",
      "epoch 168/1000   error=0.087149\n",
      "epoch 169/1000   error=0.087041\n",
      "epoch 170/1000   error=0.086930\n",
      "epoch 171/1000   error=0.086816\n",
      "epoch 172/1000   error=0.086701\n",
      "epoch 173/1000   error=0.086582\n",
      "epoch 174/1000   error=0.086462\n",
      "epoch 175/1000   error=0.086339\n",
      "epoch 176/1000   error=0.086213\n",
      "epoch 177/1000   error=0.086085\n",
      "epoch 178/1000   error=0.085955\n",
      "epoch 179/1000   error=0.085823\n",
      "epoch 180/1000   error=0.085688\n",
      "epoch 181/1000   error=0.085550\n",
      "epoch 182/1000   error=0.085411\n",
      "epoch 183/1000   error=0.085269\n",
      "epoch 184/1000   error=0.085125\n",
      "epoch 185/1000   error=0.084979\n",
      "epoch 186/1000   error=0.084830\n",
      "epoch 187/1000   error=0.084680\n",
      "epoch 188/1000   error=0.084528\n",
      "epoch 189/1000   error=0.084373\n",
      "epoch 190/1000   error=0.084217\n",
      "epoch 191/1000   error=0.084058\n",
      "epoch 192/1000   error=0.083898\n",
      "epoch 193/1000   error=0.083736\n",
      "epoch 194/1000   error=0.083572\n",
      "epoch 195/1000   error=0.083406\n",
      "epoch 196/1000   error=0.083239\n",
      "epoch 197/1000   error=0.083070\n",
      "epoch 198/1000   error=0.082899\n",
      "epoch 199/1000   error=0.082727\n",
      "epoch 200/1000   error=0.082553\n",
      "epoch 201/1000   error=0.082378\n",
      "epoch 202/1000   error=0.082202\n",
      "epoch 203/1000   error=0.082024\n",
      "epoch 204/1000   error=0.081845\n",
      "epoch 205/1000   error=0.081664\n",
      "epoch 206/1000   error=0.081483\n",
      "epoch 207/1000   error=0.081300\n",
      "epoch 208/1000   error=0.081116\n",
      "epoch 209/1000   error=0.080931\n",
      "epoch 210/1000   error=0.080746\n",
      "epoch 211/1000   error=0.080559\n",
      "epoch 212/1000   error=0.080372\n",
      "epoch 213/1000   error=0.080184\n",
      "epoch 214/1000   error=0.079996\n",
      "epoch 215/1000   error=0.079807\n",
      "epoch 216/1000   error=0.079618\n",
      "epoch 217/1000   error=0.079428\n",
      "epoch 218/1000   error=0.079238\n",
      "epoch 219/1000   error=0.079048\n",
      "epoch 220/1000   error=0.078858\n",
      "epoch 221/1000   error=0.078669\n",
      "epoch 222/1000   error=0.078479\n",
      "epoch 223/1000   error=0.078290\n",
      "epoch 224/1000   error=0.078101\n",
      "epoch 225/1000   error=0.077912\n",
      "epoch 226/1000   error=0.077724\n",
      "epoch 227/1000   error=0.077537\n",
      "epoch 228/1000   error=0.077350\n",
      "epoch 229/1000   error=0.077164\n",
      "epoch 230/1000   error=0.076979\n",
      "epoch 231/1000   error=0.076795\n",
      "epoch 232/1000   error=0.076612\n",
      "epoch 233/1000   error=0.076429\n",
      "epoch 234/1000   error=0.076248\n",
      "epoch 235/1000   error=0.076067\n",
      "epoch 236/1000   error=0.075888\n",
      "epoch 237/1000   error=0.075710\n",
      "epoch 238/1000   error=0.075533\n",
      "epoch 239/1000   error=0.075357\n",
      "epoch 240/1000   error=0.075182\n",
      "epoch 241/1000   error=0.075008\n",
      "epoch 242/1000   error=0.074836\n",
      "epoch 243/1000   error=0.074664\n",
      "epoch 244/1000   error=0.074494\n",
      "epoch 245/1000   error=0.074325\n",
      "epoch 246/1000   error=0.074157\n",
      "epoch 247/1000   error=0.073990\n",
      "epoch 248/1000   error=0.073824\n",
      "epoch 249/1000   error=0.073659\n",
      "epoch 250/1000   error=0.073494\n",
      "epoch 251/1000   error=0.073331\n",
      "epoch 252/1000   error=0.073169\n",
      "epoch 253/1000   error=0.073008\n",
      "epoch 254/1000   error=0.072847\n",
      "epoch 255/1000   error=0.072687\n",
      "epoch 256/1000   error=0.072528\n",
      "epoch 257/1000   error=0.072370\n",
      "epoch 258/1000   error=0.072212\n",
      "epoch 259/1000   error=0.072055\n",
      "epoch 260/1000   error=0.071898\n",
      "epoch 261/1000   error=0.071742\n",
      "epoch 262/1000   error=0.071587\n",
      "epoch 263/1000   error=0.071432\n",
      "epoch 264/1000   error=0.071277\n",
      "epoch 265/1000   error=0.071123\n",
      "epoch 266/1000   error=0.070969\n",
      "epoch 267/1000   error=0.070815\n",
      "epoch 268/1000   error=0.070662\n",
      "epoch 269/1000   error=0.070508\n",
      "epoch 270/1000   error=0.070355\n",
      "epoch 271/1000   error=0.070202\n",
      "epoch 272/1000   error=0.070049\n",
      "epoch 273/1000   error=0.069895\n",
      "epoch 274/1000   error=0.069742\n",
      "epoch 275/1000   error=0.069589\n",
      "epoch 276/1000   error=0.069436\n",
      "epoch 277/1000   error=0.069282\n",
      "epoch 278/1000   error=0.069129\n",
      "epoch 279/1000   error=0.068975\n",
      "epoch 280/1000   error=0.068821\n",
      "epoch 281/1000   error=0.068667\n",
      "epoch 282/1000   error=0.068512\n",
      "epoch 283/1000   error=0.068358\n",
      "epoch 284/1000   error=0.068203\n",
      "epoch 285/1000   error=0.068047\n",
      "epoch 286/1000   error=0.067892\n",
      "epoch 287/1000   error=0.067736\n",
      "epoch 288/1000   error=0.067579\n",
      "epoch 289/1000   error=0.067422\n",
      "epoch 290/1000   error=0.067265\n",
      "epoch 291/1000   error=0.067108\n",
      "epoch 292/1000   error=0.066950\n",
      "epoch 293/1000   error=0.066792\n",
      "epoch 294/1000   error=0.066633\n",
      "epoch 295/1000   error=0.066475\n",
      "epoch 296/1000   error=0.066315\n",
      "epoch 297/1000   error=0.066156\n",
      "epoch 298/1000   error=0.065996\n",
      "epoch 299/1000   error=0.065836\n",
      "epoch 300/1000   error=0.065676\n",
      "epoch 301/1000   error=0.065516\n",
      "epoch 302/1000   error=0.065355\n",
      "epoch 303/1000   error=0.065194\n",
      "epoch 304/1000   error=0.065033\n",
      "epoch 305/1000   error=0.064872\n",
      "epoch 306/1000   error=0.064711\n",
      "epoch 307/1000   error=0.064550\n",
      "epoch 308/1000   error=0.064389\n",
      "epoch 309/1000   error=0.064228\n",
      "epoch 310/1000   error=0.064067\n",
      "epoch 311/1000   error=0.063906\n",
      "epoch 312/1000   error=0.063745\n",
      "epoch 313/1000   error=0.063584\n",
      "epoch 314/1000   error=0.063424\n",
      "epoch 315/1000   error=0.063264\n",
      "epoch 316/1000   error=0.063103\n",
      "epoch 317/1000   error=0.062944\n",
      "epoch 318/1000   error=0.062784\n",
      "epoch 319/1000   error=0.062625\n",
      "epoch 320/1000   error=0.062466\n",
      "epoch 321/1000   error=0.062308\n",
      "epoch 322/1000   error=0.062150\n",
      "epoch 323/1000   error=0.061992\n",
      "epoch 324/1000   error=0.061835\n",
      "epoch 325/1000   error=0.061678\n",
      "epoch 326/1000   error=0.061522\n",
      "epoch 327/1000   error=0.061366\n",
      "epoch 328/1000   error=0.061211\n",
      "epoch 329/1000   error=0.061057\n",
      "epoch 330/1000   error=0.060903\n",
      "epoch 331/1000   error=0.060749\n",
      "epoch 332/1000   error=0.060597\n",
      "epoch 333/1000   error=0.060445\n",
      "epoch 334/1000   error=0.060293\n",
      "epoch 335/1000   error=0.060142\n",
      "epoch 336/1000   error=0.059992\n",
      "epoch 337/1000   error=0.059843\n",
      "epoch 338/1000   error=0.059694\n",
      "epoch 339/1000   error=0.059546\n",
      "epoch 340/1000   error=0.059399\n",
      "epoch 341/1000   error=0.059252\n",
      "epoch 342/1000   error=0.059106\n",
      "epoch 343/1000   error=0.058961\n",
      "epoch 344/1000   error=0.058817\n",
      "epoch 345/1000   error=0.058673\n",
      "epoch 346/1000   error=0.058530\n",
      "epoch 347/1000   error=0.058388\n",
      "epoch 348/1000   error=0.058247\n",
      "epoch 349/1000   error=0.058106\n",
      "epoch 350/1000   error=0.057966\n",
      "epoch 351/1000   error=0.057827\n",
      "epoch 352/1000   error=0.057689\n",
      "epoch 353/1000   error=0.057552\n",
      "epoch 354/1000   error=0.057415\n",
      "epoch 355/1000   error=0.057279\n",
      "epoch 356/1000   error=0.057144\n",
      "epoch 357/1000   error=0.057010\n",
      "epoch 358/1000   error=0.056876\n",
      "epoch 359/1000   error=0.056743\n",
      "epoch 360/1000   error=0.056611\n",
      "epoch 361/1000   error=0.056480\n",
      "epoch 362/1000   error=0.056350\n",
      "epoch 363/1000   error=0.056220\n",
      "epoch 364/1000   error=0.056091\n",
      "epoch 365/1000   error=0.055963\n",
      "epoch 366/1000   error=0.055835\n",
      "epoch 367/1000   error=0.055709\n",
      "epoch 368/1000   error=0.055583\n",
      "epoch 369/1000   error=0.055458\n",
      "epoch 370/1000   error=0.055333\n",
      "epoch 371/1000   error=0.055209\n",
      "epoch 372/1000   error=0.055086\n",
      "epoch 373/1000   error=0.054964\n",
      "epoch 374/1000   error=0.054843\n",
      "epoch 375/1000   error=0.054722\n",
      "epoch 376/1000   error=0.054602\n",
      "epoch 377/1000   error=0.054482\n",
      "epoch 378/1000   error=0.054363\n",
      "epoch 379/1000   error=0.054245\n",
      "epoch 380/1000   error=0.054128\n",
      "epoch 381/1000   error=0.054011\n",
      "epoch 382/1000   error=0.053895\n",
      "epoch 383/1000   error=0.053779\n",
      "epoch 384/1000   error=0.053664\n",
      "epoch 385/1000   error=0.053550\n",
      "epoch 386/1000   error=0.053436\n",
      "epoch 387/1000   error=0.053323\n",
      "epoch 388/1000   error=0.053210\n",
      "epoch 389/1000   error=0.053098\n",
      "epoch 390/1000   error=0.052986\n",
      "epoch 391/1000   error=0.052875\n",
      "epoch 392/1000   error=0.052765\n",
      "epoch 393/1000   error=0.052655\n",
      "epoch 394/1000   error=0.052546\n",
      "epoch 395/1000   error=0.052437\n",
      "epoch 396/1000   error=0.052328\n",
      "epoch 397/1000   error=0.052220\n",
      "epoch 398/1000   error=0.052113\n",
      "epoch 399/1000   error=0.052006\n",
      "epoch 400/1000   error=0.051899\n",
      "epoch 401/1000   error=0.051793\n",
      "epoch 402/1000   error=0.051687\n",
      "epoch 403/1000   error=0.051582\n",
      "epoch 404/1000   error=0.051477\n",
      "epoch 405/1000   error=0.051373\n",
      "epoch 406/1000   error=0.051268\n",
      "epoch 407/1000   error=0.051165\n",
      "epoch 408/1000   error=0.051061\n",
      "epoch 409/1000   error=0.050959\n",
      "epoch 410/1000   error=0.050856\n",
      "epoch 411/1000   error=0.050754\n",
      "epoch 412/1000   error=0.050652\n",
      "epoch 413/1000   error=0.050550\n",
      "epoch 414/1000   error=0.050449\n",
      "epoch 415/1000   error=0.050348\n",
      "epoch 416/1000   error=0.050248\n",
      "epoch 417/1000   error=0.050147\n",
      "epoch 418/1000   error=0.050048\n",
      "epoch 419/1000   error=0.049948\n",
      "epoch 420/1000   error=0.049849\n",
      "epoch 421/1000   error=0.049750\n",
      "epoch 422/1000   error=0.049651\n",
      "epoch 423/1000   error=0.049553\n",
      "epoch 424/1000   error=0.049455\n",
      "epoch 425/1000   error=0.049357\n",
      "epoch 426/1000   error=0.049260\n",
      "epoch 427/1000   error=0.049162\n",
      "epoch 428/1000   error=0.049066\n",
      "epoch 429/1000   error=0.048969\n",
      "epoch 430/1000   error=0.048873\n",
      "epoch 431/1000   error=0.048777\n",
      "epoch 432/1000   error=0.048681\n",
      "epoch 433/1000   error=0.048586\n",
      "epoch 434/1000   error=0.048491\n",
      "epoch 435/1000   error=0.048396\n",
      "epoch 436/1000   error=0.048301\n",
      "epoch 437/1000   error=0.048207\n",
      "epoch 438/1000   error=0.048114\n",
      "epoch 439/1000   error=0.048020\n",
      "epoch 440/1000   error=0.047927\n",
      "epoch 441/1000   error=0.047834\n",
      "epoch 442/1000   error=0.047741\n",
      "epoch 443/1000   error=0.047649\n",
      "epoch 444/1000   error=0.047557\n",
      "epoch 445/1000   error=0.047466\n",
      "epoch 446/1000   error=0.047375\n",
      "epoch 447/1000   error=0.047284\n",
      "epoch 448/1000   error=0.047193\n",
      "epoch 449/1000   error=0.047103\n",
      "epoch 450/1000   error=0.047013\n",
      "epoch 451/1000   error=0.046924\n",
      "epoch 452/1000   error=0.046835\n",
      "epoch 453/1000   error=0.046746\n",
      "epoch 454/1000   error=0.046657\n",
      "epoch 455/1000   error=0.046569\n",
      "epoch 456/1000   error=0.046482\n",
      "epoch 457/1000   error=0.046395\n",
      "epoch 458/1000   error=0.046308\n",
      "epoch 459/1000   error=0.046221\n",
      "epoch 460/1000   error=0.046135\n",
      "epoch 461/1000   error=0.046049\n",
      "epoch 462/1000   error=0.045964\n",
      "epoch 463/1000   error=0.045879\n",
      "epoch 464/1000   error=0.045795\n",
      "epoch 465/1000   error=0.045711\n",
      "epoch 466/1000   error=0.045627\n",
      "epoch 467/1000   error=0.045544\n",
      "epoch 468/1000   error=0.045461\n",
      "epoch 469/1000   error=0.045378\n",
      "epoch 470/1000   error=0.045296\n",
      "epoch 471/1000   error=0.045215\n",
      "epoch 472/1000   error=0.045134\n",
      "epoch 473/1000   error=0.045053\n",
      "epoch 474/1000   error=0.044973\n",
      "epoch 475/1000   error=0.044893\n",
      "epoch 476/1000   error=0.044813\n",
      "epoch 477/1000   error=0.044734\n",
      "epoch 478/1000   error=0.044656\n",
      "epoch 479/1000   error=0.044577\n",
      "epoch 480/1000   error=0.044500\n",
      "epoch 481/1000   error=0.044422\n",
      "epoch 482/1000   error=0.044346\n",
      "epoch 483/1000   error=0.044269\n",
      "epoch 484/1000   error=0.044193\n",
      "epoch 485/1000   error=0.044117\n",
      "epoch 486/1000   error=0.044042\n",
      "epoch 487/1000   error=0.043968\n",
      "epoch 488/1000   error=0.043893\n",
      "epoch 489/1000   error=0.043819\n",
      "epoch 490/1000   error=0.043746\n",
      "epoch 491/1000   error=0.043673\n",
      "epoch 492/1000   error=0.043600\n",
      "epoch 493/1000   error=0.043528\n",
      "epoch 494/1000   error=0.043456\n",
      "epoch 495/1000   error=0.043385\n",
      "epoch 496/1000   error=0.043314\n",
      "epoch 497/1000   error=0.043243\n",
      "epoch 498/1000   error=0.043173\n",
      "epoch 499/1000   error=0.043104\n",
      "epoch 500/1000   error=0.043034\n",
      "epoch 501/1000   error=0.042965\n",
      "epoch 502/1000   error=0.042897\n",
      "epoch 503/1000   error=0.042829\n",
      "epoch 504/1000   error=0.042761\n",
      "epoch 505/1000   error=0.042694\n",
      "epoch 506/1000   error=0.042627\n",
      "epoch 507/1000   error=0.042560\n",
      "epoch 508/1000   error=0.042494\n",
      "epoch 509/1000   error=0.042428\n",
      "epoch 510/1000   error=0.042362\n",
      "epoch 511/1000   error=0.042297\n",
      "epoch 512/1000   error=0.042233\n",
      "epoch 513/1000   error=0.042168\n",
      "epoch 514/1000   error=0.042104\n",
      "epoch 515/1000   error=0.042040\n",
      "epoch 516/1000   error=0.041977\n",
      "epoch 517/1000   error=0.041914\n",
      "epoch 518/1000   error=0.041851\n",
      "epoch 519/1000   error=0.041789\n",
      "epoch 520/1000   error=0.041727\n",
      "epoch 521/1000   error=0.041666\n",
      "epoch 522/1000   error=0.041604\n",
      "epoch 523/1000   error=0.041543\n",
      "epoch 524/1000   error=0.041483\n",
      "epoch 525/1000   error=0.041422\n",
      "epoch 526/1000   error=0.041362\n",
      "epoch 527/1000   error=0.041303\n",
      "epoch 528/1000   error=0.041243\n",
      "epoch 529/1000   error=0.041184\n",
      "epoch 530/1000   error=0.041126\n",
      "epoch 531/1000   error=0.041067\n",
      "epoch 532/1000   error=0.041009\n",
      "epoch 533/1000   error=0.040951\n",
      "epoch 534/1000   error=0.040893\n",
      "epoch 535/1000   error=0.040836\n",
      "epoch 536/1000   error=0.040779\n",
      "epoch 537/1000   error=0.040722\n",
      "epoch 538/1000   error=0.040666\n",
      "epoch 539/1000   error=0.040610\n",
      "epoch 540/1000   error=0.040554\n",
      "epoch 541/1000   error=0.040498\n",
      "epoch 542/1000   error=0.040443\n",
      "epoch 543/1000   error=0.040388\n",
      "epoch 544/1000   error=0.040333\n",
      "epoch 545/1000   error=0.040278\n",
      "epoch 546/1000   error=0.040224\n",
      "epoch 547/1000   error=0.040170\n",
      "epoch 548/1000   error=0.040116\n",
      "epoch 549/1000   error=0.040062\n",
      "epoch 550/1000   error=0.040009\n",
      "epoch 551/1000   error=0.039956\n",
      "epoch 552/1000   error=0.039903\n",
      "epoch 553/1000   error=0.039850\n",
      "epoch 554/1000   error=0.039798\n",
      "epoch 555/1000   error=0.039745\n",
      "epoch 556/1000   error=0.039693\n",
      "epoch 557/1000   error=0.039642\n",
      "epoch 558/1000   error=0.039590\n",
      "epoch 559/1000   error=0.039539\n",
      "epoch 560/1000   error=0.039488\n",
      "epoch 561/1000   error=0.039437\n",
      "epoch 562/1000   error=0.039386\n",
      "epoch 563/1000   error=0.039335\n",
      "epoch 564/1000   error=0.039285\n",
      "epoch 565/1000   error=0.039235\n",
      "epoch 566/1000   error=0.039185\n",
      "epoch 567/1000   error=0.039135\n",
      "epoch 568/1000   error=0.039086\n",
      "epoch 569/1000   error=0.039036\n",
      "epoch 570/1000   error=0.038987\n",
      "epoch 571/1000   error=0.038938\n",
      "epoch 572/1000   error=0.038890\n",
      "epoch 573/1000   error=0.038841\n",
      "epoch 574/1000   error=0.038792\n",
      "epoch 575/1000   error=0.038744\n",
      "epoch 576/1000   error=0.038696\n",
      "epoch 577/1000   error=0.038648\n",
      "epoch 578/1000   error=0.038600\n",
      "epoch 579/1000   error=0.038553\n",
      "epoch 580/1000   error=0.038505\n",
      "epoch 581/1000   error=0.038458\n",
      "epoch 582/1000   error=0.038411\n",
      "epoch 583/1000   error=0.038364\n",
      "epoch 584/1000   error=0.038317\n",
      "epoch 585/1000   error=0.038271\n",
      "epoch 586/1000   error=0.038224\n",
      "epoch 587/1000   error=0.038178\n",
      "epoch 588/1000   error=0.038132\n",
      "epoch 589/1000   error=0.038086\n",
      "epoch 590/1000   error=0.038040\n",
      "epoch 591/1000   error=0.037994\n",
      "epoch 592/1000   error=0.037949\n",
      "epoch 593/1000   error=0.037903\n",
      "epoch 594/1000   error=0.037858\n",
      "epoch 595/1000   error=0.037813\n",
      "epoch 596/1000   error=0.037768\n",
      "epoch 597/1000   error=0.037723\n",
      "epoch 598/1000   error=0.037678\n",
      "epoch 599/1000   error=0.037633\n",
      "epoch 600/1000   error=0.037589\n",
      "epoch 601/1000   error=0.037544\n",
      "epoch 602/1000   error=0.037500\n",
      "epoch 603/1000   error=0.037456\n",
      "epoch 604/1000   error=0.037412\n",
      "epoch 605/1000   error=0.037368\n",
      "epoch 606/1000   error=0.037324\n",
      "epoch 607/1000   error=0.037281\n",
      "epoch 608/1000   error=0.037237\n",
      "epoch 609/1000   error=0.037194\n",
      "epoch 610/1000   error=0.037151\n",
      "epoch 611/1000   error=0.037107\n",
      "epoch 612/1000   error=0.037064\n",
      "epoch 613/1000   error=0.037021\n",
      "epoch 614/1000   error=0.036978\n",
      "epoch 615/1000   error=0.036936\n",
      "epoch 616/1000   error=0.036893\n",
      "epoch 617/1000   error=0.036850\n",
      "epoch 618/1000   error=0.036808\n",
      "epoch 619/1000   error=0.036766\n",
      "epoch 620/1000   error=0.036723\n",
      "epoch 621/1000   error=0.036681\n",
      "epoch 622/1000   error=0.036639\n",
      "epoch 623/1000   error=0.036597\n",
      "epoch 624/1000   error=0.036555\n",
      "epoch 625/1000   error=0.036513\n",
      "epoch 626/1000   error=0.036472\n",
      "epoch 627/1000   error=0.036430\n",
      "epoch 628/1000   error=0.036388\n",
      "epoch 629/1000   error=0.036347\n",
      "epoch 630/1000   error=0.036306\n",
      "epoch 631/1000   error=0.036264\n",
      "epoch 632/1000   error=0.036223\n",
      "epoch 633/1000   error=0.036182\n",
      "epoch 634/1000   error=0.036141\n",
      "epoch 635/1000   error=0.036100\n",
      "epoch 636/1000   error=0.036059\n",
      "epoch 637/1000   error=0.036018\n",
      "epoch 638/1000   error=0.035977\n",
      "epoch 639/1000   error=0.035936\n",
      "epoch 640/1000   error=0.035896\n",
      "epoch 641/1000   error=0.035855\n",
      "epoch 642/1000   error=0.035815\n",
      "epoch 643/1000   error=0.035774\n",
      "epoch 644/1000   error=0.035734\n",
      "epoch 645/1000   error=0.035693\n",
      "epoch 646/1000   error=0.035653\n",
      "epoch 647/1000   error=0.035613\n",
      "epoch 648/1000   error=0.035573\n",
      "epoch 649/1000   error=0.035533\n",
      "epoch 650/1000   error=0.035492\n",
      "epoch 651/1000   error=0.035452\n",
      "epoch 652/1000   error=0.035412\n",
      "epoch 653/1000   error=0.035373\n",
      "epoch 654/1000   error=0.035333\n",
      "epoch 655/1000   error=0.035293\n",
      "epoch 656/1000   error=0.035253\n",
      "epoch 657/1000   error=0.035213\n",
      "epoch 658/1000   error=0.035173\n",
      "epoch 659/1000   error=0.035134\n",
      "epoch 660/1000   error=0.035094\n",
      "epoch 661/1000   error=0.035055\n",
      "epoch 662/1000   error=0.035015\n",
      "epoch 663/1000   error=0.034975\n",
      "epoch 664/1000   error=0.034936\n",
      "epoch 665/1000   error=0.034896\n",
      "epoch 666/1000   error=0.034857\n",
      "epoch 667/1000   error=0.034817\n",
      "epoch 668/1000   error=0.034778\n",
      "epoch 669/1000   error=0.034739\n",
      "epoch 670/1000   error=0.034699\n",
      "epoch 671/1000   error=0.034660\n",
      "epoch 672/1000   error=0.034621\n",
      "epoch 673/1000   error=0.034581\n",
      "epoch 674/1000   error=0.034542\n",
      "epoch 675/1000   error=0.034503\n",
      "epoch 676/1000   error=0.034463\n",
      "epoch 677/1000   error=0.034424\n",
      "epoch 678/1000   error=0.034385\n",
      "epoch 679/1000   error=0.034346\n",
      "epoch 680/1000   error=0.034306\n",
      "epoch 681/1000   error=0.034267\n",
      "epoch 682/1000   error=0.034228\n",
      "epoch 683/1000   error=0.034189\n",
      "epoch 684/1000   error=0.034149\n",
      "epoch 685/1000   error=0.034110\n",
      "epoch 686/1000   error=0.034071\n",
      "epoch 687/1000   error=0.034032\n",
      "epoch 688/1000   error=0.033993\n",
      "epoch 689/1000   error=0.033953\n",
      "epoch 690/1000   error=0.033914\n",
      "epoch 691/1000   error=0.033875\n",
      "epoch 692/1000   error=0.033836\n",
      "epoch 693/1000   error=0.033796\n",
      "epoch 694/1000   error=0.033757\n",
      "epoch 695/1000   error=0.033718\n",
      "epoch 696/1000   error=0.033678\n",
      "epoch 697/1000   error=0.033639\n",
      "epoch 698/1000   error=0.033600\n",
      "epoch 699/1000   error=0.033560\n",
      "epoch 700/1000   error=0.033521\n",
      "epoch 701/1000   error=0.033482\n",
      "epoch 702/1000   error=0.033442\n",
      "epoch 703/1000   error=0.033403\n",
      "epoch 704/1000   error=0.033363\n",
      "epoch 705/1000   error=0.033324\n",
      "epoch 706/1000   error=0.033284\n",
      "epoch 707/1000   error=0.033245\n",
      "epoch 708/1000   error=0.033205\n",
      "epoch 709/1000   error=0.033165\n",
      "epoch 710/1000   error=0.033126\n",
      "epoch 711/1000   error=0.033086\n",
      "epoch 712/1000   error=0.033047\n",
      "epoch 713/1000   error=0.033007\n",
      "epoch 714/1000   error=0.032967\n",
      "epoch 715/1000   error=0.032927\n",
      "epoch 716/1000   error=0.032887\n",
      "epoch 717/1000   error=0.032848\n",
      "epoch 718/1000   error=0.032808\n",
      "epoch 719/1000   error=0.032768\n",
      "epoch 720/1000   error=0.032728\n",
      "epoch 721/1000   error=0.032688\n",
      "epoch 722/1000   error=0.032648\n",
      "epoch 723/1000   error=0.032608\n",
      "epoch 724/1000   error=0.032568\n",
      "epoch 725/1000   error=0.032528\n",
      "epoch 726/1000   error=0.032487\n",
      "epoch 727/1000   error=0.032447\n",
      "epoch 728/1000   error=0.032407\n",
      "epoch 729/1000   error=0.032367\n",
      "epoch 730/1000   error=0.032327\n",
      "epoch 731/1000   error=0.032286\n",
      "epoch 732/1000   error=0.032246\n",
      "epoch 733/1000   error=0.032205\n",
      "epoch 734/1000   error=0.032165\n",
      "epoch 735/1000   error=0.032125\n",
      "epoch 736/1000   error=0.032084\n",
      "epoch 737/1000   error=0.032044\n",
      "epoch 738/1000   error=0.032003\n",
      "epoch 739/1000   error=0.031962\n",
      "epoch 740/1000   error=0.031922\n",
      "epoch 741/1000   error=0.031881\n",
      "epoch 742/1000   error=0.031840\n",
      "epoch 743/1000   error=0.031800\n",
      "epoch 744/1000   error=0.031759\n",
      "epoch 745/1000   error=0.031718\n",
      "epoch 746/1000   error=0.031678\n",
      "epoch 747/1000   error=0.031637\n",
      "epoch 748/1000   error=0.031596\n",
      "epoch 749/1000   error=0.031555\n",
      "epoch 750/1000   error=0.031514\n",
      "epoch 751/1000   error=0.031473\n",
      "epoch 752/1000   error=0.031433\n",
      "epoch 753/1000   error=0.031392\n",
      "epoch 754/1000   error=0.031351\n",
      "epoch 755/1000   error=0.031310\n",
      "epoch 756/1000   error=0.031269\n",
      "epoch 757/1000   error=0.031228\n",
      "epoch 758/1000   error=0.031187\n",
      "epoch 759/1000   error=0.031146\n",
      "epoch 760/1000   error=0.031105\n",
      "epoch 761/1000   error=0.031064\n",
      "epoch 762/1000   error=0.031023\n",
      "epoch 763/1000   error=0.030982\n",
      "epoch 764/1000   error=0.030941\n",
      "epoch 765/1000   error=0.030900\n",
      "epoch 766/1000   error=0.030860\n",
      "epoch 767/1000   error=0.030819\n",
      "epoch 768/1000   error=0.030778\n",
      "epoch 769/1000   error=0.030737\n",
      "epoch 770/1000   error=0.030696\n",
      "epoch 771/1000   error=0.030655\n",
      "epoch 772/1000   error=0.030615\n",
      "epoch 773/1000   error=0.030574\n",
      "epoch 774/1000   error=0.030533\n",
      "epoch 775/1000   error=0.030492\n",
      "epoch 776/1000   error=0.030452\n",
      "epoch 777/1000   error=0.030411\n",
      "epoch 778/1000   error=0.030371\n",
      "epoch 779/1000   error=0.030330\n",
      "epoch 780/1000   error=0.030289\n",
      "epoch 781/1000   error=0.030249\n",
      "epoch 782/1000   error=0.030209\n",
      "epoch 783/1000   error=0.030168\n",
      "epoch 784/1000   error=0.030128\n",
      "epoch 785/1000   error=0.030088\n",
      "epoch 786/1000   error=0.030047\n",
      "epoch 787/1000   error=0.030007\n",
      "epoch 788/1000   error=0.029967\n",
      "epoch 789/1000   error=0.029927\n",
      "epoch 790/1000   error=0.029887\n",
      "epoch 791/1000   error=0.029847\n",
      "epoch 792/1000   error=0.029807\n",
      "epoch 793/1000   error=0.029767\n",
      "epoch 794/1000   error=0.029727\n",
      "epoch 795/1000   error=0.029688\n",
      "epoch 796/1000   error=0.029648\n",
      "epoch 797/1000   error=0.029609\n",
      "epoch 798/1000   error=0.029569\n",
      "epoch 799/1000   error=0.029530\n",
      "epoch 800/1000   error=0.029490\n",
      "epoch 801/1000   error=0.029451\n",
      "epoch 802/1000   error=0.029412\n",
      "epoch 803/1000   error=0.029373\n",
      "epoch 804/1000   error=0.029334\n",
      "epoch 805/1000   error=0.029295\n",
      "epoch 806/1000   error=0.029256\n",
      "epoch 807/1000   error=0.029218\n",
      "epoch 808/1000   error=0.029179\n",
      "epoch 809/1000   error=0.029141\n",
      "epoch 810/1000   error=0.029102\n",
      "epoch 811/1000   error=0.029064\n",
      "epoch 812/1000   error=0.029026\n",
      "epoch 813/1000   error=0.028987\n",
      "epoch 814/1000   error=0.028949\n",
      "epoch 815/1000   error=0.028912\n",
      "epoch 816/1000   error=0.028874\n",
      "epoch 817/1000   error=0.028836\n",
      "epoch 818/1000   error=0.028798\n",
      "epoch 819/1000   error=0.028761\n",
      "epoch 820/1000   error=0.028723\n",
      "epoch 821/1000   error=0.028686\n",
      "epoch 822/1000   error=0.028649\n",
      "epoch 823/1000   error=0.028612\n",
      "epoch 824/1000   error=0.028575\n",
      "epoch 825/1000   error=0.028538\n",
      "epoch 826/1000   error=0.028501\n",
      "epoch 827/1000   error=0.028464\n",
      "epoch 828/1000   error=0.028428\n",
      "epoch 829/1000   error=0.028391\n",
      "epoch 830/1000   error=0.028355\n",
      "epoch 831/1000   error=0.028319\n",
      "epoch 832/1000   error=0.028283\n",
      "epoch 833/1000   error=0.028247\n",
      "epoch 834/1000   error=0.028211\n",
      "epoch 835/1000   error=0.028175\n",
      "epoch 836/1000   error=0.028140\n",
      "epoch 837/1000   error=0.028104\n",
      "epoch 838/1000   error=0.028069\n",
      "epoch 839/1000   error=0.028033\n",
      "epoch 840/1000   error=0.027998\n",
      "epoch 841/1000   error=0.027963\n",
      "epoch 842/1000   error=0.027928\n",
      "epoch 843/1000   error=0.027893\n",
      "epoch 844/1000   error=0.027859\n",
      "epoch 845/1000   error=0.027824\n",
      "epoch 846/1000   error=0.027790\n",
      "epoch 847/1000   error=0.027755\n",
      "epoch 848/1000   error=0.027721\n",
      "epoch 849/1000   error=0.027687\n",
      "epoch 850/1000   error=0.027653\n",
      "epoch 851/1000   error=0.027619\n",
      "epoch 852/1000   error=0.027585\n",
      "epoch 853/1000   error=0.027552\n",
      "epoch 854/1000   error=0.027518\n",
      "epoch 855/1000   error=0.027485\n",
      "epoch 856/1000   error=0.027452\n",
      "epoch 857/1000   error=0.027418\n",
      "epoch 858/1000   error=0.027385\n",
      "epoch 859/1000   error=0.027353\n",
      "epoch 860/1000   error=0.027320\n",
      "epoch 861/1000   error=0.027287\n",
      "epoch 862/1000   error=0.027255\n",
      "epoch 863/1000   error=0.027222\n",
      "epoch 864/1000   error=0.027190\n",
      "epoch 865/1000   error=0.027158\n",
      "epoch 866/1000   error=0.027126\n",
      "epoch 867/1000   error=0.027094\n",
      "epoch 868/1000   error=0.027062\n",
      "epoch 869/1000   error=0.027030\n",
      "epoch 870/1000   error=0.026998\n",
      "epoch 871/1000   error=0.026967\n",
      "epoch 872/1000   error=0.026936\n",
      "epoch 873/1000   error=0.026904\n",
      "epoch 874/1000   error=0.026873\n",
      "epoch 875/1000   error=0.026842\n",
      "epoch 876/1000   error=0.026811\n",
      "epoch 877/1000   error=0.026780\n",
      "epoch 878/1000   error=0.026750\n",
      "epoch 879/1000   error=0.026719\n",
      "epoch 880/1000   error=0.026689\n",
      "epoch 881/1000   error=0.026658\n",
      "epoch 882/1000   error=0.026628\n",
      "epoch 883/1000   error=0.026598\n",
      "epoch 884/1000   error=0.026568\n",
      "epoch 885/1000   error=0.026538\n",
      "epoch 886/1000   error=0.026508\n",
      "epoch 887/1000   error=0.026478\n",
      "epoch 888/1000   error=0.026449\n",
      "epoch 889/1000   error=0.026419\n",
      "epoch 890/1000   error=0.026390\n",
      "epoch 891/1000   error=0.026360\n",
      "epoch 892/1000   error=0.026331\n",
      "epoch 893/1000   error=0.026302\n",
      "epoch 894/1000   error=0.026273\n",
      "epoch 895/1000   error=0.026244\n",
      "epoch 896/1000   error=0.026216\n",
      "epoch 897/1000   error=0.026187\n",
      "epoch 898/1000   error=0.026159\n",
      "epoch 899/1000   error=0.026130\n",
      "epoch 900/1000   error=0.026102\n",
      "epoch 901/1000   error=0.026074\n",
      "epoch 902/1000   error=0.026046\n",
      "epoch 903/1000   error=0.026018\n",
      "epoch 904/1000   error=0.025990\n",
      "epoch 905/1000   error=0.025962\n",
      "epoch 906/1000   error=0.025934\n",
      "epoch 907/1000   error=0.025907\n",
      "epoch 908/1000   error=0.025879\n",
      "epoch 909/1000   error=0.025852\n",
      "epoch 910/1000   error=0.025825\n",
      "epoch 911/1000   error=0.025797\n",
      "epoch 912/1000   error=0.025770\n",
      "epoch 913/1000   error=0.025743\n",
      "epoch 914/1000   error=0.025717\n",
      "epoch 915/1000   error=0.025690\n",
      "epoch 916/1000   error=0.025663\n",
      "epoch 917/1000   error=0.025637\n",
      "epoch 918/1000   error=0.025610\n",
      "epoch 919/1000   error=0.025584\n",
      "epoch 920/1000   error=0.025558\n",
      "epoch 921/1000   error=0.025531\n",
      "epoch 922/1000   error=0.025505\n",
      "epoch 923/1000   error=0.025479\n",
      "epoch 924/1000   error=0.025454\n",
      "epoch 925/1000   error=0.025428\n",
      "epoch 926/1000   error=0.025402\n",
      "epoch 927/1000   error=0.025377\n",
      "epoch 928/1000   error=0.025351\n",
      "epoch 929/1000   error=0.025326\n",
      "epoch 930/1000   error=0.025300\n",
      "epoch 931/1000   error=0.025275\n",
      "epoch 932/1000   error=0.025250\n",
      "epoch 933/1000   error=0.025225\n",
      "epoch 934/1000   error=0.025200\n",
      "epoch 935/1000   error=0.025175\n",
      "epoch 936/1000   error=0.025151\n",
      "epoch 937/1000   error=0.025126\n",
      "epoch 938/1000   error=0.025102\n",
      "epoch 939/1000   error=0.025077\n",
      "epoch 940/1000   error=0.025053\n",
      "epoch 941/1000   error=0.025028\n",
      "epoch 942/1000   error=0.025004\n",
      "epoch 943/1000   error=0.024980\n",
      "epoch 944/1000   error=0.024956\n",
      "epoch 945/1000   error=0.024932\n",
      "epoch 946/1000   error=0.024908\n",
      "epoch 947/1000   error=0.024885\n",
      "epoch 948/1000   error=0.024861\n",
      "epoch 949/1000   error=0.024837\n",
      "epoch 950/1000   error=0.024814\n",
      "epoch 951/1000   error=0.024790\n",
      "epoch 952/1000   error=0.024767\n",
      "epoch 953/1000   error=0.024744\n",
      "epoch 954/1000   error=0.024721\n",
      "epoch 955/1000   error=0.024698\n",
      "epoch 956/1000   error=0.024675\n",
      "epoch 957/1000   error=0.024652\n",
      "epoch 958/1000   error=0.024629\n",
      "epoch 959/1000   error=0.024606\n",
      "epoch 960/1000   error=0.024583\n",
      "epoch 961/1000   error=0.024561\n",
      "epoch 962/1000   error=0.024538\n",
      "epoch 963/1000   error=0.024516\n",
      "epoch 964/1000   error=0.024493\n",
      "epoch 965/1000   error=0.024471\n",
      "epoch 966/1000   error=0.024449\n",
      "epoch 967/1000   error=0.024427\n",
      "epoch 968/1000   error=0.024405\n",
      "epoch 969/1000   error=0.024383\n",
      "epoch 970/1000   error=0.024361\n",
      "epoch 971/1000   error=0.024339\n",
      "epoch 972/1000   error=0.024317\n",
      "epoch 973/1000   error=0.024296\n",
      "epoch 974/1000   error=0.024274\n",
      "epoch 975/1000   error=0.024253\n",
      "epoch 976/1000   error=0.024231\n",
      "epoch 977/1000   error=0.024210\n",
      "epoch 978/1000   error=0.024189\n",
      "epoch 979/1000   error=0.024167\n",
      "epoch 980/1000   error=0.024146\n",
      "epoch 981/1000   error=0.024125\n",
      "epoch 982/1000   error=0.024104\n",
      "epoch 983/1000   error=0.024083\n",
      "epoch 984/1000   error=0.024062\n",
      "epoch 985/1000   error=0.024042\n",
      "epoch 986/1000   error=0.024021\n",
      "epoch 987/1000   error=0.024000\n",
      "epoch 988/1000   error=0.023980\n",
      "epoch 989/1000   error=0.023959\n",
      "epoch 990/1000   error=0.023939\n",
      "epoch 991/1000   error=0.023918\n",
      "epoch 992/1000   error=0.023898\n",
      "epoch 993/1000   error=0.023878\n",
      "epoch 994/1000   error=0.023858\n",
      "epoch 995/1000   error=0.023838\n",
      "epoch 996/1000   error=0.023818\n",
      "epoch 997/1000   error=0.023798\n",
      "epoch 998/1000   error=0.023778\n",
      "epoch 999/1000   error=0.023758\n",
      "epoch 1000/1000   error=0.023738\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 1\n",
    "\n",
    "# Train on a subset (e.g., 10k samples) for demonstration\n",
    "x_train_small = x_train[:10000]\n",
    "y_train_small = y_train[:10000]\n",
    "\n",
    "network.train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    mse,\n",
    "    mse_prime,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit ourselves to a subset for speed in a from-scratch implementation. During training, you‚Äôll see the loss steadily fall, although it may not reach performance on par with more sophisticated techniques or optimizers like Adam‚Äîstill, it can prove the network is learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_4_'></a>[Evaluating Results](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can test our model‚Äôs accuracy by running a forward pass on **test** or **validation** data and comparing the predicted labels to the ground truth. For example, we can do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "outputs = network.forward(x_test)\n",
    "# Convert continuous outputs to discrete predictions\n",
    "predictions = np.argmax(outputs, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1501898 , 0.16636064, 0.15094503, ..., 0.16296318, 0.14765909,\n",
       "        0.15593281],\n",
       "       [0.10787507, 0.12026068, 0.10853247, ..., 0.11432784, 0.10436826,\n",
       "        0.10935208],\n",
       "       [0.21963359, 0.2399546 , 0.22212019, ..., 0.23060707, 0.21601196,\n",
       "        0.22379426],\n",
       "       ...,\n",
       "       [0.05212741, 0.06276052, 0.05278043, ..., 0.05869016, 0.04996652,\n",
       "        0.05537663],\n",
       "       [0.08270211, 0.09534005, 0.08273872, ..., 0.09005367, 0.07925342,\n",
       "        0.0854659 ],\n",
       "       [0.04229477, 0.04920594, 0.04224324, ..., 0.04634889, 0.03979751,\n",
       "        0.0433413 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21963359, 0.2399546 , 0.22212019, 0.22401622, 0.22377155,\n",
       "       0.21656237, 0.2230862 , 0.23060707, 0.21601196, 0.22379426])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the exact accuracy will vary, you should see a reasonable performance (maybe above 80%) if your network and hyperparameters are well-chosen‚Äîeven without convolution or advanced optimizers. Of course, using **SGD with mini-batches**, **dropout**, **better activation functions**, or **regularization** can further enhance performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying our from-scratch **Fully Connected** architecture and **backpropagation** algorithm to MNIST, we validate that the same foundation used for **XOR** scales up to more practical tasks. Although performance may be less than that of specialized models or libraries like TensorFlow and PyTorch, this exercise cements an understanding of core neural network operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
